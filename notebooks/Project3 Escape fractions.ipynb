{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escape fractions <a class=\"tocSkip\">\n",
    "\n",
    "    \n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues.\n",
    "   \n",
    "This notebook useses multiple galaxies at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from astrotools.packages import *\n",
    "from astrotools.constants import tab10, single_column, two_column, thesis_width\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('pymuse')\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter(\"%(asctime)-15s %(message)s\",datefmt='%H:%M:%S')\n",
    "handler.setFormatter(fmt)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# first we need to specify the path to the raw data\n",
    "basedir = Path('..')\n",
    "data_ext = Path('a:')/'Archive' #basedir / 'data' / 'raw' \n",
    "\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])\n",
    "sample_table['power_index'] = 2.3\n",
    "sample_table['power_index'][sample_table['AO'].mask]=2.8\n",
    "sample_table['distance'] = Distance(distmod=sample_table['(m-M)'])\n",
    "\n",
    "sample_table_v1p6 = Table.read(basedir/'data'/'external'/'phangs_sample_table_v1p6.fits')\n",
    "sample_table_v1p6 = sample_table_v1p6[sample_table_v1p6['survey_muse_status']=='released'].copy()\n",
    "sample_table_v1p6['gal_name'] = [x.upper() for x in sample_table_v1p6['name']]\n",
    "sample_table_v1p6['dist_err'] = sample_table_v1p6['dist']*sample_table_v1p6['dist_unc']*np.log(10)\n",
    "sample_table_v1p6.add_index('gal_name')\n",
    "# we are interested in props_mstar, dist, dist_err, dist_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the association catalogue matched with the nebuale catalogue\n",
    "version = 'v1p2'\n",
    "for HSTband in ['nuv','v']:\n",
    "    for scalepc in [8,16,32,64]:\n",
    "        folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "        lst = []\n",
    "        for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "            gal_name = file.stem.split('_')[0]\n",
    "            tbl = Table(fits.getdata(file,ext=1))\n",
    "            tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "            lst.append(tbl)\n",
    "        assoc_tmp = vstack(lst)\n",
    "        # missing = set(sample_table['name']) - set(np.unique(assoc_tmp['gal_name']))\n",
    "        print(f'{HSTband:>3}, {scalepc:>2}pc: {len(lst)} galaxies, {len(assoc_tmp):>5} associations ({np.sum(assoc_tmp[\"1to1\"]):>4} 1to1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which version of the association catalogue to use\n",
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The nebulae catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original catalogue from Francesco\n",
    "# Table.read treats the units correctly\n",
    "nebulae = Table.read(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v3.fits')\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra'],nebulae['cen_dec'],frame='icrs')\n",
    "# we remove the old EW values\n",
    "del nebulae[['EW_HA','EW_HA_ERR']]\n",
    "nebulae.rename_columns(['cen_x','cen_y','cen_ra','cen_dec','region_area',\n",
    "                          'EBV','EBV_ERR','SkyCoord'],\n",
    "                         ['x_neb','y_neb','ra_neb','dec_neb','area_neb',\n",
    "                          'EBV_balmer','EBV_balmer_err','SkyCoord_neb'])\n",
    "    \n",
    "# some additional properties \n",
    "for filename in ['dig','FUV_bkg','EW','density_refit','refitNII','in_frame','CO']:\n",
    "    tbl = Table.read(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_{filename}.fits')\n",
    "    nebulae = join(nebulae,tbl,keys=['gal_name','region_ID'],metadata_conflicts='silent')\n",
    "nebulae['dig/hii'] = nebulae['dig_median'] / nebulae['hii_median']\n",
    "\n",
    "# this will rais a few errors that we just ignore\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX']/nebulae['FUV_FLUX']\n",
    "    nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['FUV_FLUX_ERR']/nebulae['FUV_FLUX'])**2+(nebulae['HA6562_FLUX_ERR']/nebulae['HA6562_FLUX'])**2)\n",
    "\n",
    "# calculate luminosity based on distance from sample table\n",
    "# we do not include the distance in the uncertainty as it should cancel out with the mass\n",
    "nebulae = join(nebulae,sample_table_v1p6['gal_name','dist','dist_err'],keys='gal_name')\n",
    "flux_to_lum = 1e-20 * 4*np.pi*(u.erg/u.cm**2/u.s * u.Mpc**2).to(u.erg/u.s)\n",
    "nebulae['HA6562_LUM'] = nebulae['HA6562_FLUX']*flux_to_lum*nebulae['dist']**2\n",
    "nebulae['HA6562_LUM_ERR'] = nebulae['HA6562_LUM'] * np.sqrt((nebulae['HA6562_FLUX_ERR']/nebulae['HA6562_FLUX'])**2)\n",
    "nebulae['HA6562_LUM_CORR'] = nebulae['HA6562_FLUX_CORR']*flux_to_lum*nebulae['dist']**2\n",
    "nebulae['HA6562_LUM_CORR_ERR'] = nebulae['HA6562_LUM_CORR'] * np.sqrt((nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2)\n",
    "\n",
    "# and the observed number of ionising photons\n",
    "nebulae['Qobserved']     = 7.31e11*nebulae['HA6562_LUM_CORR']\n",
    "nebulae['Qobserved_err'] = 7.31e11*nebulae['HA6562_LUM_CORR_ERR']\n",
    "nebulae['Qobserved_uncorr']     = 7.31e11*nebulae['HA6562_LUM']\n",
    "nebulae['Qobserved_uncorr_err'] = 7.31e11*nebulae['HA6562_LUM_ERR']\n",
    "\n",
    "# the nebulae catalogue with additional information\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*{scalepc}pc_nebulae.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    name=gal_name.lower()\n",
    "    lst.append(tbl)\n",
    "nebulae_tmp = vstack(lst)\n",
    "\n",
    "nebulae = join(nebulae,nebulae_tmp,keys=['gal_name','region_ID'],join_type='outer')\n",
    "nebulae = nebulae[nebulae['flag_star']==0]\n",
    "nebulae = nebulae[nebulae['flag_edge']==0]\n",
    "HIIregion_mask = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "\n",
    "print(f'{len(nebulae)} nebulae in initial catalogue (all galaxies)')\n",
    "nebulae = nebulae[HIIregion_mask]\n",
    "print(f'we use {len(lst)} galaxies with {np.sum(~nebulae[\"overlap_neb\"].mask & (nebulae[\"in_frame\"]))} HII regions')\n",
    "# only use HII regions and only the galaxies with associations\n",
    "#nebulae = nebulae[HIIregion_mask & ~nebulae[\"overlap_neb\"].mask]\n",
    "print(f'{np.sum((nebulae[\"overlap_neb\"]>0) & (nebulae[\"in_frame\"]))} HII regions ({100*np.sum((nebulae[\"overlap_neb\"]>0) & (nebulae[\"in_frame\"]))/np.sum(nebulae[\"in_frame\"]):.1f}%) overlap with association')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those files hold the merged association catalogues\n",
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "    associations = Table(hdul[1].data)\n",
    "    \n",
    "associations['SkyCoord'] = SkyCoord(associations['reg_ra']*u.degree,associations['reg_dec']*u.degree)\n",
    "associations.rename_columns(['reg_ra','reg_dec','reg_x','reg_y',\n",
    "                             'reg_dolflux_Age_MinChiSq','reg_dolflux_Mass_MinChiSq','reg_dolflux_Ebv_MinChiSq',\n",
    "                             'reg_dolflux_Age_MinChiSq_err','reg_dolflux_Mass_MinChiSq_err','reg_dolflux_Ebv_MinChiSq_err',\n",
    "                             'SkyCoord'],\n",
    "                            ['ra_asc','dec_asc','x_asc','y_asc','age','mass',\n",
    "                             'EBV_stars','age_err','mass_err','EBV_stars_err','SkyCoord_asc'])\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}_in_frame.fits') as hdul:\n",
    "    in_frame = Table(hdul[1].data)\n",
    "associations = join(associations,in_frame,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "associations_SLUG = Table.read(basedir/'data'/'interim'/'associations_SLUG.fits')\n",
    "associations = join(associations,associations_SLUG,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "# many associations have error=0. we set it to 1\n",
    "associations['age_err'][associations['age_err']<1] = 1\n",
    "\n",
    "# the association catalogue matched with the nebuale catalogue\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    lst.append(tbl)\n",
    "assoc_tmp = vstack(lst)\n",
    "# combine both catalogues\n",
    "associations = join(associations,assoc_tmp,keys=['gal_name','assoc_ID'])\n",
    "print(f'{len(lst)} galaxies in sample ({np.sum(associations[\"in_frame\"])} associations)')\n",
    "print(f'{np.sum((associations[\"overlap_asc\"]>0) & (associations[\"in_frame\"]))} associations ({100*np.sum((associations[\"overlap_asc\"]>0) & (associations[\"in_frame\"]))/np.sum(associations[\"in_frame\"]):.1f} %) overlap with an HII region')\n",
    "\n",
    "#criteria = np.abs(associations['age']-associations['age_16'])>associations['age_err']\n",
    "#criteria |= np.abs(associations['age']-associations['age_64'])>associations['age_err']\n",
    "#associations['uniform_age'] = ~criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The joint catalogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1to1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also recreate the table from the association an nebulae catalogues\n",
    "catalogue = join(nebulae[~nebulae['assoc_ID'].mask],associations,keys=['gal_name','assoc_ID','region_ID'])\n",
    "\n",
    "print(f'{len(catalogue)} objects in matched catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The extended sample\n",
    "\n",
    "some associations overlap with multiple HII regions. However we can only deal with those that overlap with only one HII regions (unless they are in an HII region complex, but this catalogue comes later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations['one_to_multi'] = False\n",
    "\n",
    "gal_name = None\n",
    "for row in associations:\n",
    "    if row['gal_name'] != gal_name:\n",
    "        gal_name = row['gal_name']\n",
    "        \n",
    "        with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "            associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "\n",
    "        with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_nebulae.yml') as f:\n",
    "            nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "\n",
    "        \n",
    "    if len(associations_dict[row['assoc_ID']]) == 1:\n",
    "        for assoc_ID in nebulae_dict[row['region_ID']]:\n",
    "            if len(associations_dict[assoc_ID])!=1:\n",
    "                break\n",
    "        else:\n",
    "            row['one_to_multi'] = True\n",
    "    \n",
    "extended = associations[associations['one_to_multi'] & ~associations['1to1']]\n",
    "\n",
    "print(f'{len(extended)} associations in {len(np.unique(extended[\"gal_name\",\"region_ID\"]))} nebulae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the first version of the code, assocations that fall inside a nebulae with multiple assocations did not get assigned a `region_ID`. The following code fixes that (you only need to run it if you use a new resolution/band)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gal_name in sample_table_v1p6['name']:\n",
    "    print(gal_name)\n",
    "    with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "        associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "        \n",
    "    filename = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_associations.fits'\n",
    "    assoc_tmp = Table.read(filename)\n",
    "    print(f\"{np.sum(~np.isnan(assoc_tmp['region_ID']))} region IDs in initial catalogue\")\n",
    "    \n",
    "    assoc_tmp['region_ID'][assoc_tmp['Nnebulae']==1] = [associations_dict[k][0] for k in assoc_tmp[assoc_tmp['Nnebulae']==1]['assoc_ID']]\n",
    "    print(f\"{np.sum(~np.isnan(assoc_tmp['region_ID']))} region IDs in final catalogue\")\n",
    "\n",
    "    hdu = fits.BinTableHDU(assoc_tmp,name='joined catalogue')\n",
    "    hdu.writeto(filename,overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HII region complexes\n",
    "\n",
    "The heavy lifting is done by the script `HII_region_complexes.py`. Here we just read in the six output files created by this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir/'data'/'interim'/f'nebulae_neighbors.yml') as f:\n",
    "    nebulae_neighbors = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "with open(basedir/'data'/'interim'/f'complexes_nebulae.yml') as f:\n",
    "    complexes_neb_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "with open(basedir/'data'/'interim'/f'complexes_associations_{HSTband}_ws{scalepc}pc_{version}.yml') as f:\n",
    "    complexes_assoc_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    \n",
    "complexes = Table.read(basedir/'data'/'interim'/'complexes.fits')\n",
    "complexes['SkyCoord'] = SkyCoord(complexes['ra']*u.deg,complexes['dec']*u.deg,frame='icrs')\n",
    "# split string with region_IDs and assoc_IDs into lists\n",
    "complexes['region_IDs'] = [list(map(int,row['region_IDs'].split(','))) for row in complexes]\n",
    "complexes['assoc_IDs'] = [list(map(int,row['assoc_IDs'].split(','))) if row['assoc_IDs'] else [] for row in complexes]\n",
    "complexes.add_column([len(row['region_IDs']) for row in complexes],index=4,name='N_neb')\n",
    "complexes.add_column([len(row['assoc_IDs']) for row in complexes],index=5,name='N_assoc')\n",
    "\n",
    "complexes_nebulae = Table.read(basedir/'data'/'interim'/'complexes_nebulae.fits')\n",
    "complexes_associations = Table.read(basedir/'data'/'interim'/f'complexes_associations_{HSTband}_ws{scalepc}pc_{version}.fits')\n",
    "\n",
    "print(f\"{np.sum([len(x) for x in complexes['region_IDs']])} nebulae in {len(complexes)} complexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we filter the catalogues to only include clean HII region complexes and their associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexes = complexes[complexes['isHII'] & complexes['multi_to_multi'] & (complexes['N_assoc']>0)]\n",
    "complexes_associations = join(complexes_associations[~complexes_associations['complex_ID'].mask],associations,keys=['gal_name','assoc_ID'])\n",
    "complexes_nebulae = join(complexes_nebulae[~complexes_nebulae['complex_ID'].mask],nebulae,keys=['gal_name','region_ID'])\n",
    "\n",
    "print(f\"{np.sum([len(x) for x in complexes['region_IDs']])} HII regions in {len(complexes)} complexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cluster catalogue\n",
    "\n",
    "read the existing Catalogue. Most HII regions overlap with multiple clusters. Therefore the matching between the two catalogues is done at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_clusters = Table.read(basedir/'data'/'interim'/f'compact_clusters_in_HIIregions.fits')\n",
    "# we remove clusters that do not overlap\n",
    "compact_clusters = compact_clusters[(compact_clusters['region_ID']>-1)]\n",
    "compact_clusters['age_err'][compact_clusters['age_err']<1] = 1\n",
    "\n",
    "print(f'{len(compact_clusters)} objects in cluster catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Code to match clusters to HII regions\n",
    "\n",
    "if this file does not exist yet, create it with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astrotools.regions import find_sky_region\n",
    "\n",
    "def get_value(matrix, index, default_value=np.nan):\n",
    "    '''\n",
    "    The `to_pixel` method returns the x,y coordinates. However in the \n",
    "    image they correspond to img[y,x]\n",
    "    '''\n",
    "    result = np.zeros(len(index))+default_value\n",
    "    mask = (index[:,1] < matrix.shape[0]) & (index[:,0] < matrix.shape[1])\n",
    "    mask &= (index[:,1] >= 0) & (index[:,0] >=0)\n",
    "\n",
    "    valid = index[mask]\n",
    "    result[mask] = matrix[valid[:,1], valid[:,0]]\n",
    "    return result\n",
    "\n",
    "compact_clusters_list = []\n",
    "for gal_name in np.unique(sample_table['name']):\n",
    "\n",
    "    filename = data_ext / 'MUSE' / 'DR2.1' / 'copt' / 'MUSEDAP'\n",
    "    filename = [x for x in filename.iterdir() if x.stem.startswith(gal_name)][0]\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))    \n",
    "\n",
    "    reg_muse_pix, reg_muse_sky = find_sky_region(Halpha.mask.astype(int),wcs=Halpha.wcs)\n",
    "    \n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))    \n",
    "    \n",
    "    # read the compact cluster catalogues\n",
    "    filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "    if not filename.is_file():\n",
    "        print(f'no compact clusters for {gal_name}')\n",
    "    else:     \n",
    "        compact_clusters = Table.read(filename)\n",
    "        compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "        compact_clusters.add_index('ID_PHANGS_CLUSTERS')\n",
    "        compact_clusters['in_frame'] = reg_muse_sky.contains(compact_clusters['SkyCoord'],Halpha.wcs)\n",
    "        compact_clusters['region_ID'] = get_value(nebulae_mask.data,np.array(compact_clusters['SkyCoord'].to_pixel(nebulae_mask.wcs)).T.astype(int))\n",
    "\n",
    "        # we save all objects that fall inside one of our empty HII regions\n",
    "        compact_clusters.add_column(gal_name,index=0,name='gal_name')\n",
    "        del compact_clusters['SkyCoord']\n",
    "        compact_clusters_list.append(compact_clusters)\n",
    "\n",
    "compact_clusters = vstack(compact_clusters_list)\n",
    "compact_clusters.rename_columns(['ID_PHANGS_CLUSTERS','PHANGS_CLUSTER_CLASS_HUMAN','PHANGS_CLUSTER_CLASS_ML_VGG','PHANGS_AGE_MINCHISQ','PHANGS_AGE_MINCHISQ_ERR','PHANGS_MASS_MINCHISQ','PHANGS_MASS_MINCHISQ_ERR','PHANGS_EBV_MINCHISQ','PHANGS_EBV_MINCHISQ_ERR'],\n",
    "                                ['cluster_ID','class_human','class_ml','age','age_err','mass','mass_err','EBV_stellar','EBV_stellar_err'])\n",
    "print(f'{len(compact_clusters)} compact clusters in final catalogue')\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(compact_clusters)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto(basedir/'data'/'interim'/f'compact_clusters_in_HIIregions.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s = 0\n",
    "for gal_name in np.unique(compact_clusters['gal_name']):\n",
    "    sub = compact_clusters[compact_clusters['gal_name']==gal_name]\n",
    "    s+=len(np.unique(sub['region_ID']))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# how many clusters and HII regions have a 1to1 relation\n",
    "one_to_one = 0\n",
    "for gal_name in sample_table_v1p6['name']:\n",
    "    _,counts = np.unique(compact_clusters[compact_clusters['gal_name']==gal_name]['region_ID'],return_counts=True)\n",
    "    one_to_one += np.sum(counts==1)\n",
    "print(f'{one_to_one} clusters with 1to1 relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Table to showcase the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'PHANGS sample',\n",
    "'preamble': '\\\\centering',\n",
    "'col_align':'lrlrrr',\n",
    "'tablefoot': f'\\\\label{{tbl:sample}}'\n",
    "            }\n",
    "\n",
    "sample = sample_table_v1p6[['name','dist','dist_unc']].copy()\n",
    "\n",
    "Nasc = []\n",
    "Nneb = []\n",
    "Nclu = []\n",
    "\n",
    "for gal_name in sample['name']:\n",
    "\n",
    "    Nneb.append(np.sum((nebulae[\"gal_name\"]==gal_name) & (nebulae['in_frame'])))    \n",
    "    Nasc.append(np.sum((associations[\"gal_name\"]==gal_name) & (associations['in_frame'])))\n",
    "    Nclu.append(np.sum((compact_clusters[\"gal_name\"]==gal_name) & (compact_clusters['in_frame'])))\n",
    "\n",
    "sample['Nneb'] = Nneb\n",
    "sample['Nasc'] = Nasc\n",
    "sample['Nclu'] = Nclu\n",
    "\n",
    "sample['dist_unc'].info.format = '%.2f'\n",
    "\n",
    "ascii.write(sample,sys.stdout, Writer = ascii.Latex,latexdict=latexdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "astrosat_sample = np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name'])\n",
    "muse_sample     = sample_table['name']\n",
    "hst_sample      = associations['gal_name']\n",
    "sitelle_sample  = ['NGC0628','NGC2835','NGC3351','NGC3627','NGC4535']\n",
    "\n",
    "t = Table({\n",
    "    'name':muse_sample,\n",
    "    'MUSE':np.isin(muse_sample,muse_sample),\n",
    "    'HST':np.isin(muse_sample,hst_sample),\n",
    "    'Astrosat':np.isin(muse_sample,astrosat_sample),\n",
    "    'Sitelle':np.isin(muse_sample,sitelle_sample)}\n",
    "     )\n",
    "\n",
    "for col in t.columns[1:]:\n",
    "    t[col] = ['\\checkmark' if x else '' for x in t[col] ]\n",
    "ascii.write(t,sys.stdout, Writer = ascii.Latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sample table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table*',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Galaxy sample',\n",
    "'units': {'R.A.':'(J2000)','Dec.':'(J2000)','$i$':'deg','PA':'deg','Distance':'$\\si{\\mega\\parsec}$',\n",
    "          'r25':'arcmin'},\n",
    "'preamble': '\\\\centering',\n",
    "'tablefoot': f'\\\\label{{tbl:sample}}'\n",
    "            }\n",
    "\n",
    "tbl = sample_table[['name','Type','R.A.','Dec.','Inclination','posang','r25','mass','SFR','PSF','(m-M)']]\n",
    "\n",
    "tbl['R.A.'] = [row.replace('h','x').replace('m','y').replace('.','z').replace('s','') for row in tbl['R.A.']]\n",
    "tbl['R.A.'] = [row.replace('x','$^\\mathrm{h}$').replace('y','$^\\mathrm{m}$').replace('z','$^\\mathrm{s}\\kern -3pt.$') for row in tbl['R.A.']]\n",
    "\n",
    "tbl['Dec.'] = [row.replace('d','x').replace('m','y').replace('.','z').replace('s','') for row in tbl['Dec.']]\n",
    "tbl['Dec.'] = [row.replace('-','$-$').replace('+','$+$').replace('x','$^\\mathrm{d}$').replace('y','$^\\mathrm{m}$').replace('z','$^\\mathrm{s}\\kern -3pt.$') for row in tbl['Dec.']]\n",
    "tbl.add_column(Distance(distmod=tbl['(m-M)']).to(u.Mpc),index=4,name='Distance')\n",
    "tbl['Distance'].info.format = '%.2f' \n",
    "tbl['Nneb'] = [np.sum((nebulae['gal_name']==name) & (nebulae['in_frame'])) for name in tbl['name']]\n",
    "tbl['Nasc'] = [np.sum((associations['gal_name']==name) & (associations['in_frame'])) for name in tbl['name']]\n",
    "\n",
    "#tbl['name'] = [f'\\\\galaxyname{{{row[\"name\"][:-4]}}}{{{row[\"name\"][-4:]}}}' for row in tbl]\n",
    "tbl.rename_columns(['Inclination','posang','r25'],['$i$','PA','$r_{25}$'])\n",
    "#tbl = join(tbl,t,keys='name')\n",
    "\n",
    "#with open(basedir / 'data' / 'interim' /'sample.tex','w',newline='\\n') as f:\n",
    "#    ascii.write(tbl,f,Writer=ascii.Latex, latexdict=latexdict,overwrite=True,exclude_names=['(m-M)','Sitelle'])\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict,exclude_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot the sample (cutouts)\n",
    "\n",
    "plot all objects in the merged catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from cluster.plot import single_cutout\n",
    "\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria]\n",
    "\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "size=10*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_{HSTband}_{scalepc}pc'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                # nebulae mask\n",
    "                filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            ax = single_cutout(ax,\n",
    "                             position = row['SkyCoord_neb'],\n",
    "                             image = F275,\n",
    "                             mask1 = nebulae_mask,\n",
    "                             mask2 = associations_mask,\n",
    "                             label = f\"{row['gal_name']}: {row['region_ID']:.0f}/{row['assoc_ID']:.0f}\",\n",
    "                             size  = 4*u.arcsecond)\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "three color composit with CO emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "from pnlf.plot import create_RGB\n",
    "\n",
    "\n",
    "criteria = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_asc'] == 1)\n",
    "criteria &= (catalogue['age'] < 10)\n",
    "#tmp = catalogue[criteria]\n",
    "\n",
    "print(f'{len(tmp)} objects')\n",
    "\n",
    "size=5*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_rgb_{HSTband}_{scalepc}pc'\n",
    "\n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for j in range(Npages):\n",
    "        print(f'working on page {j+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[j*Npage:(j+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                print(f'reading files for {gal_name}')\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                filename = data_ext/'MUSE'/'DR2.1'/'MUSEDAP'/ f'{gal_name}_MAPS.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                                    meta=hdul['HA6562_FLUX'].header,\n",
    "                                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    \n",
    "                # nebulae mask\n",
    "                filename = data_ext/'Products'/ 'Nebulae catalogue'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'HST'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "            \n",
    "                with fits.open(data_ext/'ALMA'/'v4p0'/f'{gal_name.lower()}_12m+7m+tp_co21_broad_tpeak.fits') as hdul:\n",
    "                    CO = NDData(data=hdul[0].data,\n",
    "                                meta=hdul[0].header,\n",
    "                                wcs=WCS(hdul[0].header))\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            \n",
    "            position = row['SkyCoord_neb']\n",
    "            \n",
    "            label = f\"{row['gal_name']}: {row['region_ID']:.0f}/{row['assoc_ID']:.0f}\"\n",
    "\n",
    "            cutout_F275 = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "            norm = simple_norm(cutout_F275.data,stretch='linear',clip=False,percent=99.9)\n",
    "\n",
    "            cutout_CO, _  = reproject_interp(CO,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "            cutout_Halpha, _  = reproject_interp(Halpha,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "\n",
    "            rgb = create_RGB(cutout_CO,cutout_Halpha,cutout_F275.data,\n",
    "                             percentile=[98,98,99.8],weights=[0.7,0.6,1])\n",
    "            #ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "            ax.imshow(rgb,origin='lower')\n",
    "\n",
    "            # plot the nebulae catalogue\n",
    "            cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape,order='nearest-neighbor')    \n",
    "            region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_mask)\n",
    "                blank_mask[cutout_mask==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=0.8,label='HII-region')\n",
    "\n",
    "            # 32 pc\n",
    "            cutout_32 = Cutout2D(associations_mask.data,position,size=size,wcs=associations_mask.wcs)\n",
    "            region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_32.data)\n",
    "                blank_mask[cutout_32.data==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='blue',lw=0.8,label='32pc assoc.')\n",
    "            t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "            t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if j == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute number of ionising photons\n",
    "\n",
    "the available models are GENEVASTD, GENEVAHIGH, PADOVASTD, PADOVAAGB, GENEVAv00, GENEVAv40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv00',metallicity=0.014)\n",
    "\n",
    "# model age in Myr and flux in s-1 per Msun\n",
    "model_age  = cluster.quanta['Time'].value/1e6    \n",
    "model_flux = cluster.quanta['HI_rate'].value / cluster.mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each association, calculate the predicted number of ionizing photons based on the mass and the age of the association.\n",
    "\n",
    "The observed number of ionizing photos is calulcated based on the conversion factor from Niederhofer+2016 (or Kennicutt+98)\n",
    "\n",
    "$$\n",
    "Q(\\mathrm{H}^0) = 7.31\\cdot 10^{11} L(\\mathrm{H}\\alpha)\n",
    "$$\n",
    "\n",
    "Starburst99 uses a factor of 7.354+-0.008 (no Idea why it changes over time) to convert between quanta (number of ionizing photons) and ewidth (Halpha luminosity).\n",
    "\n",
    "we only use a subsample (high mass, contained, speparated etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Age to photon flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import plot_ionising_photon_flux\n",
    "\n",
    "HI_rate = cluster.quanta['HI_rate'].value\n",
    "time = cluster.quanta['Time'].value/1e6\n",
    "    \n",
    "plot_ionising_photon_flux(2,1,time,HI_rate,sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "# model age in Myr and flux in s-1 per Msun\n",
    "model_age  = cluster.quanta['Time'].value/1e6    \n",
    "model_flux = cluster.quanta['HI_rate'].value / cluster.mass\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(thesis_width,thesis_width/1.618))\n",
    "\n",
    "for age_err in [1,2,4,8]:\n",
    "    \n",
    "    flux, flux_err = ionising_photon_flux(\n",
    "                                     age = np.arange(1,11),\n",
    "                                     age_err = age_err,\n",
    "                                     model_age = model_age,\n",
    "                                     model_flux = model_flux,\n",
    "                                     sample_size=1000\n",
    "                                     )\n",
    "    ax.plot(np.arange(1,11),flux/flux_err,label=f'$\\Delta \\mathrm{{age}} = {age_err}$')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel='age / Myr',yscale='linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "# model age in Myr and flux in s-1 per Msun\n",
    "model_age  = cluster.quanta['Time'].value/1e6    \n",
    "model_flux = cluster.quanta['HI_rate'].value / cluster.mass\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(thesis_width,thesis_width/1.618))\n",
    "\n",
    "\n",
    "flux, flux_err = ionising_photon_flux(\n",
    "                                 age = np.arange(1,11,0.1),\n",
    "                                 age_err = np.arange(1,11,0.1),\n",
    "                                 model_age = model_age,\n",
    "                                 model_flux = model_flux,\n",
    "                                 sample_size=1000\n",
    "                                 )\n",
    "ax.plot(np.arange(1,11,0.1),flux/flux_err,label=f'$\\Delta \\mathrm{{age}} = {age_err}$')\n",
    "\n",
    "ax.set(xlabel='age / Myr',ylabel='S/N flux',yscale='linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the ages are often integers (and so are the errors). Therefore we compute the flux and uncertainties multiple times. To save time, we can compute them for all unique combinations of age and error and then just look up the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = extended[extended['mass']>1e4]\n",
    "unique_age = np.unique(sample[['age','age_err']].as_array(),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the 1to1 sample\n",
    "\n",
    "we compute the ionising photon flux and the escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "flux, flux_err = ionising_photon_flux(\n",
    "                                     age = catalogue['age'],\n",
    "                                     age_err = catalogue['age_err'],\n",
    "                                     model_age = model_age,\n",
    "                                     model_flux = model_flux,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "Qpredicted     = flux*catalogue['mass']\n",
    "Qpredicted_err = Qpredicted * np.sqrt((flux_err/flux)**2+(catalogue['mass_err']/catalogue['mass'])**2)\n",
    "\n",
    "catalogue['Qpredicted']     = Qpredicted\n",
    "catalogue['Qpredicted_err'] = Qpredicted_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import escape_fraction\n",
    "\n",
    "\n",
    "fesc, fesc_err = escape_fraction(catalogue['Qpredicted'],catalogue['Qpredicted_err'],catalogue['Qobserved'],catalogue['Qobserved_err'],stats=True)\n",
    "\n",
    "catalogue['fesc'] = fesc\n",
    "catalogue['fesc_err'] = fesc_err\n",
    "catalogue['robust'] = (catalogue['mass']>1e4) & (catalogue['overlap']=='contained') & (catalogue['age']<=8)\n",
    "\n",
    "print('for the robust sample')\n",
    "tmp = catalogue[catalogue['robust']]\n",
    "fesc, fesc_err = escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],stats=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at different sub-samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import plot_escape_fraction\n",
    "\n",
    "print('contained')\n",
    "tmp = catalogue[(catalogue['overlap']=='contained') & (catalogue['mass']>1e4) & (catalogue['age']<=8)]\n",
    "fesc, fesc_err = escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],stats=True)\n",
    "plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'])\n",
    "plt.show()\n",
    "\n",
    "print('partial')\n",
    "tmp = catalogue[(catalogue['overlap']=='partial') & (catalogue['mass']>1e4) & (catalogue['age']<=8)]\n",
    "fesc, fesc_err = escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],stats=True)\n",
    "plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,0.8*single_column))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "Qpredicted_line = np.logspace(48,52)\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "for i,f in enumerate([0.0,0.5,0.9,0.99]):\n",
    "    Qobserved_line = Qpredicted_line*(1-f)\n",
    "    ax.plot(np.log10(Qpredicted_line),np.log10(Qobserved_line),ls=lines[i],c='k',label=f'$f_\\mathrm{{esc}}={f}$',zorder=1)\n",
    "ax.fill_between(np.log10(Qpredicted_line),4*np.log10(Qpredicted_line),np.log10(Qpredicted_line),color='0.7',alpha=0.1)\n",
    "\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    \n",
    "    tmp = catalogue[catalogue['robust'] & (catalogue['gal_name']==gal_name)]\n",
    "    color = cmap(norm(np.log10(sample_table_v1p6.loc[gal_name]['props_mstar'])))\n",
    "    \n",
    "    sc=ax.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),s=5,\n",
    "                  color=color,cmap=cmap,zorder=1,\n",
    "                  rasterized=True)\n",
    "    \n",
    "    tmp = tmp[(tmp['Qpredicted']+tmp['Qpredicted_err']>tmp['Qobserved']) & (tmp['age']<=2)]\n",
    "    ax.errorbar(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "             xerr=tmp['Qpredicted_err']/tmp['Qpredicted']/np.log(10),\n",
    "             yerr=tmp['Qobserved_err']/tmp['Qobserved']/np.log(10),\n",
    "             color=color,fmt='o',ms=0.1,zorder=0)\n",
    "\n",
    "    \n",
    "ax.set(xlabel=r'$\\log_{10} (Q (\\mathrm{H}^0)\\,/\\,\\mathrm{s}^{-1})$ predicted',\n",
    "       ylabel=r'$\\log_{10} (Q_{\\mathrm{H}\\,\\alpha}\\,/\\,\\mathrm{s}^{-1})$ observed',\n",
    "       xlim=[49,52],ylim=[49,52])\n",
    "\n",
    "ax.set_xticks([49,50,51,52])\n",
    "ax.set_yticks([49,50,51,52])\n",
    "\n",
    "\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.8))\n",
    "\n",
    "\n",
    "ax.legend(handlelength=2,loc=2)\n",
    "ax.set_title(r'\\texttt{robust sample}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Extended sample\n",
    "\n",
    "thus far we ignored HII regions with multiple associations. Here we also include them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "flux, flux_err = ionising_photon_flux(\n",
    "                                     age = extended['age'],\n",
    "                                     age_err = extended['age_err'],\n",
    "                                     model_age = model_age,\n",
    "                                     model_flux = model_flux,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "Qpredicted     = flux*extended['mass']\n",
    "Qpredicted_err = Qpredicted * np.sqrt((flux_err/flux)**2+(extended['mass_err']/extended['mass'])**2)\n",
    "extended['Qpredicted']     = Qpredicted\n",
    "extended['Qpredicted_err'] = Qpredicted_err\n",
    "\n",
    "# a few additional flags we need to clean the sample\n",
    "extended['robust'] = (extended['mass']>1e4) & (extended['overlap']=='contained') & (extended['age']<=8)\n",
    "extended['young_and_massive'] = (extended['mass']>1e4) & (extended['age']<=8)\n",
    "extended['young_and_massive_contained'] = extended['young_and_massive'] & (extended['overlap']=='contained')\n",
    "extended['young_and_massive_partial'] = extended['young_and_massive'] & (extended['overlap']=='partial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the `group_by` function to combine multiple associations inside on HII region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_prop(array):\n",
    "    return np.sqrt(np.sum(array**2))\n",
    "def mass_cut(array):\n",
    "    return np.all(array>1e4)\n",
    "def age_cut(array):\n",
    "    return np.all(array<=8)\n",
    "def overlap_cut(array):\n",
    "    return np.all(array=='contained')\n",
    "\n",
    "groups = extended.group_by(['gal_name','region_ID'])\n",
    "temp = groups.groups.keys\n",
    "\n",
    "# sum up predicted ionising photon flux\n",
    "temp['Qpredicted'] = groups['Qpredicted'].groups.aggregate(np.sum)\n",
    "temp['Qpredicted_err'] = groups['Qpredicted_err'].groups.aggregate(error_prop)\n",
    "\n",
    "# a few additional properties\n",
    "temp['Ncluster'] = groups['gal_name'].groups.aggregate(len)\n",
    "temp['mass'] = groups['mass'].groups.aggregate(np.sum)\n",
    "temp['mass_cut'] = groups['mass'].groups.aggregate(mass_cut)\n",
    "temp['overlap_cut'] = groups['overlap'].groups.aggregate(overlap_cut)\n",
    "temp['age_cut'] = groups['age'].groups.aggregate(age_cut)\n",
    "temp['ymc'] = groups['young_and_massive_contained'].groups.aggregate(np.any)\n",
    "temp['ymp'] = groups['young_and_massive_partial'].groups.aggregate(np.any)\n",
    "\n",
    "# finally we merge with the nebulae catalogue\n",
    "nebulae_extended = join(nebulae,temp,keys=['gal_name','region_ID'])\n",
    "nebulae_extended['robust'] = nebulae_extended['ymc'] & ~nebulae_extended['ymp']\n",
    "\n",
    "print(f'{len(nebulae_extended)} HII regions in extended catalogue')\n",
    "N_extended = np.sum(nebulae_extended['robust']) \n",
    "print(f'{N_extended} objects in robust extended catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import plot_escape_fraction\n",
    "\n",
    "fesc, fesc_err = escape_fraction(nebulae_extended['Qpredicted'],nebulae_extended['Qpredicted_err'],nebulae_extended['Qobserved'],nebulae_extended['Qobserved_err'],stats=True)\n",
    "plot_escape_fraction(nebulae_extended['Qpredicted'],nebulae_extended['Qpredicted_err'],nebulae_extended['Qobserved'],nebulae_extended['Qobserved_err'])\n",
    "\n",
    "nebulae_extended['fesc'] = fesc\n",
    "nebulae_extended['fesc_err'] = fesc_err\n",
    "\n",
    "print('for the robust sample')\n",
    "tmp = nebulae_extended[nebulae_extended['robust']]\n",
    "fesc, fesc_err = escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],stats=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the HII region complexes\n",
    "\n",
    "for the complexes things are a lot more difficult. \n",
    "\n",
    "1. We first compute the ionising photon flux of the individual associations\n",
    "2. We then sum up all HII regions inside each complex\n",
    "3. We match the associations to their complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "flux, flux_err = ionising_photon_flux(\n",
    "                                     age = complexes_associations['age'],\n",
    "                                     age_err = complexes_associations['age_err'],\n",
    "                                     model_age = model_age,\n",
    "                                     model_flux = model_flux,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "Qpredicted     = flux*complexes_associations['mass']\n",
    "Qpredicted_err = Qpredicted * np.sqrt((flux_err/flux)**2+(complexes_associations['mass_err']/complexes_associations['mass'])**2)\n",
    "complexes_associations['Qpredicted']     = Qpredicted\n",
    "complexes_associations['Qpredicted_err'] = Qpredicted_err\n",
    "complexes_associations['young_and_massive'] = (complexes_associations['mass']>1e4) & (complexes_associations['age']<=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we group the nebulae by their `complex_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_prop(array):\n",
    "    return np.sqrt(np.sum(array**2))\n",
    "def most_common_value(array):\n",
    "    v,count = np.unique(array,return_counts=True)\n",
    "    return v[np.argmax(count)]\n",
    "\n",
    "groups = complexes_nebulae.group_by(['gal_name','complex_ID'])\n",
    "temp = groups.groups.keys\n",
    "temp['N_HII']   = groups['gal_name'].groups.aggregate(len)\n",
    "temp['env']   = groups['env_neb'].groups.aggregate(most_common_value)\n",
    "\n",
    "for col in ['HA6562_LUM','HA6562_LUM_CORR','Qobserved','Qobserved_uncorr']:\n",
    "    temp[col] = groups[col].groups.aggregate(np.sum) \n",
    "for col in ['HA6562_LUM_ERR','HA6562_LUM_CORR_ERR','Qobserved_err','Qobserved_uncorr_err']:\n",
    "    temp[col] = groups[col].groups.aggregate(error_prop)    \n",
    "\n",
    "complexes_sample = join(complexes,temp,keys=['gal_name','complex_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and next the associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexes_associations['young_and_massive'] = (complexes_associations['mass']>1e4) & (complexes_associations['age']<=8)\n",
    "complexes_associations['young_and_massive_contained'] = complexes_associations['young_and_massive'] & (complexes_associations['overlap']=='contained')\n",
    "complexes_associations['young_and_massive_partial'] = complexes_associations['young_and_massive'] & (complexes_associations['overlap']=='partial')\n",
    "\n",
    "groups = complexes_associations.group_by(['gal_name','complex_ID'])\n",
    "temp = groups.groups.keys\n",
    "temp['N_assoc']   = groups['gal_name'].groups.aggregate(len)\n",
    "temp['ymc'] = groups['young_and_massive_contained'].groups.aggregate(np.any)\n",
    "temp['ymp'] = groups['young_and_massive_partial'].groups.aggregate(np.any)\n",
    "\n",
    "temp['Qpredicted'] = groups['Qpredicted'].groups.aggregate(np.sum)\n",
    "temp['Qpredicted_err'] = groups['Qpredicted_err'].groups.aggregate(error_prop)\n",
    " \n",
    "complexes_sample = join(complexes_sample,temp,keys=['gal_name','complex_ID'])\n",
    "complexes_sample['robust'] = complexes_sample['ymc'] & ~complexes_sample['ymp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import escape_fraction\n",
    "\n",
    "fesc, fesc_err = escape_fraction(complexes_sample['Qpredicted'],complexes_sample['Qpredicted_err'],complexes_sample['Qobserved'],complexes_sample['Qobserved_err'],stats=True)\n",
    "\n",
    "complexes_sample['fesc'] = fesc\n",
    "complexes_sample['fesc_err'] = fesc_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in np.unique(complexes_sample['env']):\n",
    "    print(f\"{env}: {np.sum(complexes_sample['env']==env)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the compact clusters\n",
    "\n",
    "for each cluster that overlaps with an HII region, we compute the ionising photon flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "flux, flux_err = ionising_photon_flux(\n",
    "                                     age = compact_clusters['age'],\n",
    "                                     age_err = compact_clusters['age_err'],\n",
    "                                     model_age = model_age,\n",
    "                                     model_flux = model_flux,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "Qpredicted     = flux*compact_clusters['mass']\n",
    "Qpredicted_err = Qpredicted * np.sqrt((flux_err/flux)**2+(compact_clusters['mass_err']/compact_clusters['mass'])**2)\n",
    "compact_clusters['Qpredicted']     = Qpredicted\n",
    "compact_clusters['Qpredicted_err'] = Qpredicted_err\n",
    "compact_clusters['young_and_massive'] = (compact_clusters['mass']>1e4) & (compact_clusters['age']<=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we use `group_by` to sum the flux of all clusters inside the same HII region and join this catalogue with the HII region catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_prop(array):\n",
    "    return np.sqrt(np.sum(array**2))\n",
    "def mass_cut(array):\n",
    "    return np.all(array>1e4)\n",
    "def age_cut(array):\n",
    "    return np.all(array<=8)\n",
    "\n",
    "groups = compact_clusters.group_by(['gal_name','region_ID'])\n",
    "temp = groups.groups.keys\n",
    "\n",
    "temp['Qpredicted'] = groups['Qpredicted'].groups.aggregate(np.sum)\n",
    "temp['Qpredicted_err'] = groups['Qpredicted_err'].groups.aggregate(error_prop)\n",
    "temp['Ncluster'] = groups['gal_name'].groups.aggregate(len)\n",
    "temp['mass'] = groups['mass'].groups.aggregate(np.sum)\n",
    "temp['mass_cut'] = groups['mass'].groups.aggregate(mass_cut)\n",
    "temp['age_cut'] = groups['age'].groups.aggregate(age_cut)\n",
    "temp['young_and_massive'] = groups['young_and_massive'].groups.aggregate(np.any)\n",
    "\n",
    "nebulae_cluster = join(nebulae,temp,keys=['gal_name','region_ID'])\n",
    "nebulae_cluster['robust'] = nebulae_cluster['young_and_massive']\n",
    "\n",
    "print(f'{len(nebulae_cluster)} HII regions in cluster catalogue')\n",
    "N_cluster = np.sum(nebulae_cluster['robust']) \n",
    "print(f'{N_cluster} objects in robust cluster catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we can compute the escape fraction in this new catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fesc, fesc_err = escape_fraction(nebulae_cluster['Qpredicted'],nebulae_cluster['Qpredicted_err'],nebulae_cluster['Qobserved'],nebulae_cluster['Qobserved_err'],stats=True)\n",
    "\n",
    "nebulae_cluster['fesc'] = fesc\n",
    "nebulae_cluster['fesc_err'] = fesc_err\n",
    "\n",
    "print('for the robust sample')\n",
    "tmp = nebulae_cluster[nebulae_cluster['robust']]\n",
    "fesc, fesc_err = escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ionized photon flux\n",
    "\n",
    "### For different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import compare_ionising_photons\n",
    "\n",
    "fig,(ax1,ax2,ax4,ax3) = plt.subplots(figsize=(1.1*two_column,two_column/3.),ncols=4,nrows=1,sharex=True,sharey=True)\n",
    "\n",
    "tmp = catalogue[~catalogue['robust']]\n",
    "ax1 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax1,color='0.7',s=0.5,zorder=0)\n",
    "tmp = catalogue[catalogue['robust']]\n",
    "ax1 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax1,plot_lines=False,color=tab10[0],s=0.5)\n",
    "ax1.legend(handlelength=2,loc=2,fontsize=6)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "ax1.set_title(r'\\texttt{1to1 sample}')\n",
    "\n",
    "tmp = nebulae_extended[~nebulae_extended['robust']]\n",
    "ax2 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax2,color='0.7',s=0.5,zorder=0)\n",
    "tmp = nebulae_extended[nebulae_extended['robust']]\n",
    "ax2 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax2,plot_lines=False,color=tab10[0],s=0.5)\n",
    "ax2.set(ylabel='')\n",
    "ax2.set_aspect('equal', adjustable='box')\n",
    "ax2.set_title(r'\\texttt{extended sample}')\n",
    "\n",
    "tmp = complexes_sample[~complexes_sample['robust']]\n",
    "ax4 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax4,color='0.7',s=0.5,zorder=0)\n",
    "tmp = complexes_sample[complexes_sample['robust']]\n",
    "ax4 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax4,plot_lines=False,color=tab10[0],s=0.5)\n",
    "ax4.set(ylabel='')\n",
    "ax4.set_aspect('equal', adjustable='box')\n",
    "ax4.set_title(r'\\texttt{complexes sample}')\n",
    "\n",
    "\n",
    "tmp = nebulae_cluster[~nebulae_cluster['robust']]\n",
    "ax3 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax3,color='0.7',s=0.5,zorder=0)\n",
    "tmp = nebulae_cluster[nebulae_cluster['robust']]\n",
    "ax3 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],ax=ax3,plot_lines=False,color=tab10[0],s=0.5)\n",
    "ax3.set(ylabel='')\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "ax3.set_title(r'\\texttt{cluster sample}')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'ionising_photons_samples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For different Population synthesis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "from cluster.io import read_bpass\n",
    "\n",
    "GENEVAv00 = Cluster(stellar_model='GENEVAv00',metallicity=0.014)\n",
    "GENEVAv40 = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "bpass = read_bpass(basedir/'..'/'BPASS',metallicity='z020')\n",
    "bc03 = Table.read(basedir/'data'/'external'/'bc03_q.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "sample = catalogue\n",
    "\n",
    "flux_v00, flux_err_v00 = ionising_photon_flux(\n",
    "                                     age = sample['age'],\n",
    "                                     age_err = sample['age_err'],\n",
    "                                     model_age = GENEVAv00.quanta['Time'].value/1e6   ,\n",
    "                                     model_flux = GENEVAv00.quanta['HI_rate'].value / GENEVAv00.mass,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "flux_v40, flux_err_v40 = ionising_photon_flux(\n",
    "                                     age = sample['age'],\n",
    "                                     age_err = sample['age_err'],\n",
    "                                     model_age = GENEVAv40.quanta['Time'].value/1e6   ,\n",
    "                                     model_flux = GENEVAv40.quanta['HI_rate'].value / GENEVAv40.mass,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "flux_BPASS, flux_err_BPASS = ionising_photon_flux(\n",
    "                                     age = sample['age'],\n",
    "                                     age_err = sample['age_err'],\n",
    "                                     model_age = bpass['age'].value/1e6,\n",
    "                                     model_flux = bpass['Q'].value / 1e6, # the BPASS cluster start with 1e6 Msun\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "flux_bc03, flux_err_bc03 = ionising_photon_flux(\n",
    "                                     age = sample['age'],\n",
    "                                     age_err = sample['age_err'],\n",
    "                                     model_age = bc03['sfh.age'],\n",
    "                                     model_flux = bc03['stellar.n_ly_young']/bc03['stellar.m_star'],\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "Qpredicted_v00     = flux_v00*sample['mass']\n",
    "Qpredicted_v00_err = flux_err_v00*sample['mass']\n",
    "Qpredicted_v40     = flux_v40*sample['mass']\n",
    "Qpredicted_v40_err = flux_err_v40*sample['mass']\n",
    "Qpredicted_BPASS     = flux_BPASS*sample['mass']\n",
    "Qpredicted_BPASS_err = flux_err_BPASS*sample['mass']\n",
    "Qpredicted_bc03     = flux_bc03*sample['mass']\n",
    "Qpredicted_bc03_err = flux_err_bc03*sample['mass']\n",
    "Qobserved = 7.31e11*sample['HA6562_LUM_CORR']\n",
    "Qobserved_err = 7.31e11*sample['HA6562_LUM_CORR_ERR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(GENEVAv00.quanta['Time'].value/1e6,GENEVAv00.quanta['HI_rate'].value / GENEVAv00.mass,label='SB99')\n",
    "ax.plot(bpass['age'].value/1e6,bpass['Q'].value / 1e6,label='BPASS')\n",
    "ax.plot(bc03['sfh.age'],bc03['stellar.n_ly_young']/bc03['stellar.m_star'],label='CIGALE')\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import compare_ionising_photons\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(figsize=(two_column,two_column/2.7),ncols=3,sharey=True)\n",
    "\n",
    "ax1 = compare_ionising_photons(Qpredicted_v00,Qobserved,ax=ax1,color='0.7',s=0.5,zorder=0)\n",
    "ax1.set_title(r'\\texttt{GENEVAv00}')\n",
    "\n",
    "ax2 = compare_ionising_photons(Qpredicted_v40,Qobserved,ax=ax2,color='0.7',s=0.5,zorder=0)\n",
    "ax2.set(ylabel='')\n",
    "ax2.set_title(r'\\texttt{GENEVAv40}')\n",
    "\n",
    "ax3 = compare_ionising_photons(Qpredicted_BPASS,Qobserved,ax=ax3,color='0.7',s=0.5,zorder=0)\n",
    "ax3.set(ylabel='')\n",
    "ax3.set_title(r'\\texttt{BPASS}')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'escape_fraction_samples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = Table({'v00':Qpredicted_v00,'v00_err':Qpredicted_v00_err,\n",
    "             'v40':Qpredicted_v40,'v40_err':Qpredicted_v40_err,\n",
    "             'BPASS':Qpredicted_BPASS,'BPASS_err':Qpredicted_BPASS_err,\n",
    "             'CIGALE':Qpredicted_bc03,'CIGALE_err':Qpredicted_bc03_err,\n",
    "             'SLUG':catalogue['q_SLUG'],'SLUG_err':catalogue['q_SLUG_err_plus'],\n",
    "            })\n",
    "\n",
    "for col in ['v00','v40','BPASS','CIGALE','SLUG']:\n",
    "    tbl[col][tbl[col]/tbl[f'{col}_err']<1] = np.nan\n",
    "\n",
    "tbl['mass'] = catalogue['mass']\n",
    "tbl['age'] = catalogue['age']\n",
    "\n",
    "#tbl['SLUG'][tbl['SLUG']/tbl['SLUG_err']<1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import fix_aspect_ratio\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "def corner(table,columns,mask,labels={},filename=None,figsize=10,aspect_ratio=1,**kwargs):\n",
    "        \n",
    "    lim = [1e46,1e52]\n",
    "    # create a figure with the correct proportions\n",
    "    nrows, ncols = len(columns)-1, len(columns)-1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(figsize,figsize*aspect_ratio*ncols/nrows))\n",
    "    for i,row in enumerate(columns[1:]):\n",
    "        for j,col in enumerate(columns[:-1]):\n",
    "            ax=axes[i,j]\n",
    "            if j>i:\n",
    "                ax.remove()\n",
    "            else:\n",
    "                ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim)\n",
    "                ax.set_xticks([1e47,1e49,1e51])\n",
    "                ax.set_yticks([1e47,1e49,1e51])\n",
    "\n",
    "                if j==0:\n",
    "                    ax.set_ylabel(labels.get(row,row.replace(\"_\",\"\")))\n",
    "                else:\n",
    "                    ax.set_yticklabels([])\n",
    "                if i==len(columns)-2:\n",
    "                    ax.set_xlabel(labels.get(col,col.replace(\"_\",\"\")))\n",
    "                else:\n",
    "                    ax.set_xticklabels([])\n",
    "\n",
    "                x,y = table[col][mask],table[row][mask]\n",
    "                ax.scatter(x,y,s=0.5,color='0.7',rasterized=True,**kwargs)\n",
    "                \n",
    "                x,y = table[col][~mask],table[row][~mask]    \n",
    "                ax.scatter(x,y,s=0.5,color=tab10[0],rasterized=True,**kwargs)\n",
    "                #hist, x_e, y_e = np.histogram2d(x,y,bins=np.logspace(45,52,10),density=True)\n",
    "                #z = interpn((0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ),hist,np.vstack([x,y]).T,method=\"linear\",bounds_error=False)\n",
    "                #vmin,vmax=np.nanpercentile(z,[15,95])\n",
    "                #sc=ax.scatter(x,y,c=z,vmin=vmin,vmax=vmax,s=0.5,cmap=plt.cm.Reds)\n",
    "                ax.plot(lim,lim,color='black')\n",
    "                \n",
    "            fix_aspect_ratio(ax,aspect_ratio=aspect_ratio)\n",
    "    \n",
    "    ax = fig.add_axes([0.6, 0.6, 0.2, 0.2])\n",
    "    ax.axis('off')\n",
    "    ax.text(0.,0.,r'$Q (\\mathrm{H}^0)\\,/\\,\\mathrm{s}^{-1}$')\n",
    "    plt.subplots_adjust(wspace=0.12, hspace=0.12)\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename,dpi=600)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "columns  = ['v00','v40','BPASS','CIGALE','SLUG']\n",
    "labels = {'v00': 'SB99 v00', 'v40': 'SB99 v40', 'BPASS': 'BPASS'}\n",
    "filename = basedir/'reports'/f'corner_ionising_photons.pdf'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tbl,columns,labels=labels,mask=(tbl['mass']<0),\n",
    "           filename=filename,figsize=1.2*single_column)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(len(associations),dtype=bool)\n",
    "\n",
    "for filt in ['NUV','U','B','I','V']:\n",
    "    mask |= (associations[f'{filt}_dolmag_vega']==-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations[mask & (associations['q_SLUG']>associations['q_SLUG_err_plus'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.hist(catalogue['age'],bins=np.linspace(0.5,40.5,40),histtype='step',label='cigale')\n",
    "ax.hist(catalogue['age_SLUG'],bins=np.linspace(0.5,40.5,40),histtype='step',label='SLUG')\n",
    "ax.legend()\n",
    "ax.set(xlabel='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(figsize=(two_column,two_column/3),ncols=3)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    \n",
    "    tmp = catalogue[(catalogue['gal_name']==gal_name)]\n",
    "    \n",
    "    color=cmap(norm(np.log10(sample_table_v1p6.loc[gal_name]['props_mstar'])))\n",
    "    \n",
    "    #ax1.errorbar(tmp['mass'],tmp['mass_SLUG'],xerr=tmp['mass_err'],yerr=tmp['mass_SLUG_err_plus'],fmt='o',color=color,ms=0,alpha=0.3)\n",
    "    ax1.scatter(tmp['mass'],tmp['mass_SLUG'],color=color,s=0.5,alpha=0.5)\n",
    "\n",
    "    #ax2.errorbar(tmp['age'],tmp['age_SLUG'],xerr=tmp['age_err'],yerr=tmp['age_SLUG_err_plus'],fmt='o',color=color,ms=0,alpha=0.3)\n",
    "    ax2.scatter(tmp['age'],tmp['age_SLUG'],color=color,s=0.5,alpha=0.5)\n",
    "\n",
    "    #ax3.errorbar(tmp['EBV_balmer'],tmp['av_SLUG'],xerr=tmp['EBV_balmer_err'],yerr=tmp['av_SLUG_err_plus'],fmt='o',color=color,ms=0,alpha=0.3)\n",
    "    ax3.scatter(tmp['EBV_balmer'],tmp['av_SLUG'],color=color,s=0.5,alpha=0.5)\n",
    "\n",
    "    \n",
    "mean, edges, _ = binned_statistic(catalogue['mass'],catalogue['mass_SLUG'],statistic='median',bins=np.logspace(2,6,8))\n",
    "x = (edges[1:]+edges[:-1])/2\n",
    "ax1.plot(x,mean,color='0.2',label='SLUG median')\n",
    "    \n",
    "lim = [1e2,1e6]\n",
    "ax1.plot(lim,lim,color='black')\n",
    "ax1.set(xlim=lim,ylim=lim,xscale='log',yscale='log',xlabel='mass cigale / Msun',ylabel='mass SLUG / Msun')\n",
    "\n",
    "mean, edges, _ = binned_statistic(catalogue['age'],catalogue['age_SLUG'],statistic='median',bins=np.linspace(0,50,10))\n",
    "x = (edges[1:]+edges[:-1])/2\n",
    "ax2.plot(x,mean,color='0.2',label='SLUG median')\n",
    "\n",
    "lim = [0,50]\n",
    "ax2.plot(lim,lim,color='black')\n",
    "ax2.set(xlim=lim,ylim=lim,xlabel='age cigale / Myr',ylabel='age SLUG / Myr')\n",
    "\n",
    "mean, edges, _ = binned_statistic(catalogue['EBV_balmer'],catalogue['av_SLUG'],statistic='median',bins=np.linspace(0,1,8))\n",
    "x = (edges[1:]+edges[:-1])/2\n",
    "ax3.plot(x,mean,color='0.2',label='SLUG median')\n",
    "\n",
    "ax3.plot([0,1],[0,3.1],color='black')\n",
    "ax3.set(xlim=[0,1],ylim=[0,10],xlabel='E(B-V) Balmer',ylabel='Av')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.05,right=0.92)\n",
    "cbar_ax = fig.add_axes([0.95, 0.18, 0.02, 0.75])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='vertical',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "    \n",
    "plt.savefig(basedir/'reports'/'SLUG_vs_CIGALE.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### impact of models on the escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(figsize=(two_column,two_column/2.7),ncols=3,sharey=True)\n",
    "\n",
    "bins = np.linspace(0,1,20)\n",
    "\n",
    "fesc_v00 = (Qpredicted_v00-Qobserved)/Qpredicted_v00\n",
    "#fesc_v00_err = Qobserved/nebulae_cluster['Qpredicted']*np.sqrt((Qobserved_err/nebulae_cluster['Qobserved'])**2+(nebulae_cluster['Qpredicted_err']/nebulae_cluster['Qpredicted'])**2)\n",
    "\n",
    "ax1.hist(fesc_v00,bins=bins)\n",
    "label = f'$f_\\mathrm{{esc}}={np.nanmedian(fesc_v00):.2f}$'\n",
    "ax1.text(0.05,0.85,label, transform=ax1.transAxes,color='black',fontsize=7)\n",
    "ax1.set_title(r'\\texttt{GENEVAv00}')\n",
    "\n",
    "fesc_v40 = (Qpredicted_v40-Qobserved)/Qpredicted_v40\n",
    "ax2.hist(fesc_v40,bins=bins)\n",
    "label = f'$f_\\mathrm{{esc}}={np.nanmedian(fesc_v40):.2f}$'\n",
    "ax2.text(0.05,0.85,label, transform=ax2.transAxes,color='black',fontsize=7)\n",
    "ax2.set(ylabel='')\n",
    "ax2.set_title(r'\\texttt{GENEVAv40}')\n",
    "\n",
    "fesc_BPASS = (Qpredicted_BPASS-Qobserved)/Qpredicted_BPASS\n",
    "ax3.hist(fesc_BPASS,bins=bins)\n",
    "label = f'$f_\\mathrm{{esc}}={np.nanmedian(fesc_BPASS):.2f}$'\n",
    "#ax3.text(0.05,0.85,label, transform=ax3.transAxes,color='black',fontsize=10)\n",
    "ax3.set(ylabel='')\n",
    "ax3.set_title(r'\\texttt{BPASS}')\n",
    "\n",
    "for ax in (ax1,ax2,ax3):\n",
    "    ax.set(xlim=[0,1],xlabel=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'escape_fraction_samples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import plot_escape_fraction\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(figsize=(two_column,two_column/2.7),ncols=3)\n",
    "\n",
    "ax1 = plot_escape_fraction(Qpredicted_v00,Qpredicted_v00_err,Qobserved,Qobserved_err,ax=ax1)\n",
    "ax1.set_title(r'\\texttt{GENEVAv00}')\n",
    "\n",
    "ax2 = plot_escape_fraction(Qpredicted_v40,Qpredicted_v40_err,Qobserved,Qobserved_err,ax=ax2)\n",
    "ax2.set(ylabel='')\n",
    "ax2.set_title(r'\\texttt{GENEVAv40}')\n",
    "\n",
    "ax3 = plot_escape_fraction(Qpredicted_BPASS,Qpredicted_BPASS_err,Qobserved,Qobserved_err,ax=ax3)\n",
    "ax3.set(ylabel='')\n",
    "ax3.set_title(r'\\texttt{BPASS}')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'escape_fraction_samples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import plot_escape_fraction\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(figsize=(single_column,single_column*4/1.618),nrows=4,sharex=True,sharey=True)\n",
    "\n",
    "tmp = catalogue[catalogue['robust']]\n",
    "ax1 = plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],ax=ax1,type='other')\n",
    "ax1.set(xlabel='')\n",
    "ax1.set_title(r'\\texttt{1to1 sample}')\n",
    "#ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "tmp = nebulae_extended[nebulae_extended['robust']]\n",
    "ax2 = plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],ax=ax2,type='other')\n",
    "ax2.set(xlabel='')\n",
    "ax2.set_title(r'\\texttt{extended sample}')\n",
    "#ax2.set_aspect('equal', adjustable='box')\n",
    "\n",
    "tmp = complexes_sample[complexes_sample['robust']]\n",
    "ax3 = plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],ax=ax3,type='other')\n",
    "ax3.set(xlabel='')\n",
    "ax3.set_title(r'\\texttt{complexes sample}')\n",
    "#ax3.set_aspect('equal', adjustable='box')\n",
    "\n",
    "tmp = nebulae_cluster[nebulae_cluster['robust']]\n",
    "ax4 = plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],ax=ax4,type='other')\n",
    "ax4.set_title(r'\\texttt{cluster sample}')\n",
    "#ax4.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/f'escape_fraction_samples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "escape fraction over the entire galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    Qobserved = nebulae[nebulae['in_frame'] & (nebulae['gal_name']==gal_name)]['Qobserved']\n",
    "    Qpredicted = associations[associations['in_frame'] & (associations['gal_name']==gal_name)]['Qpredicted']\n",
    "    print(f'{gal_name}: {100*(np.nansum(Qpredicted)-np.nansum(Qobserved))/np.nansum(Qpredicted):.1f}')\n",
    "\n",
    "Qobserved = nebulae[nebulae['in_frame']]['Qobserved']\n",
    "Qpredicted = associations[associations['in_frame']]['Qpredicted']\n",
    "print(f'total: {100*(np.nansum(Qpredicted)-np.nansum(Qobserved))/np.nansum(Qpredicted):.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    Qobserved = catalogue[(catalogue['gal_name']==gal_name)]['Qobserved']\n",
    "    Qpredicted = catalogue[(catalogue['gal_name']==gal_name)]['Qpredicted']\n",
    "    print(f'{gal_name}: {100*(np.nansum(Qpredicted)-np.nansum(Qobserved))/np.nansum(Qpredicted):.1f}')\n",
    "\n",
    "Qobserved = catalogue['Qobserved']\n",
    "Qpredicted = catalogue['Qpredicted']\n",
    "print(f'total: {100*(np.nansum(Qpredicted)-np.nansum(Qobserved))/np.nansum(Qpredicted):.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import compare_ionising_photons\n",
    "\n",
    "tmp = catalogue[catalogue['robust'] & (catalogue['Qobserved']+catalogue['Qobserved_err']>catalogue['Qpredicted']-catalogue['Qpredicted_err'])]\n",
    "tmp = tmp[tmp['age']<=2]\n",
    "fesc, fesc_err = escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],stats=True)\n",
    "\n",
    "ax1 = compare_ionising_photons(tmp['Qpredicted'],tmp['Qobserved'],color=tab10[0],s=0.5)\n",
    "ax1.errorbar(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "             xerr=tmp['Qpredicted_err']/tmp['Qpredicted']/np.log(10),\n",
    "             yerr=tmp['Qobserved_err']/tmp['Qobserved']/np.log(10),\n",
    "             color=tab10[0],fmt='o',ms=0.1,zorder=0)\n",
    "\n",
    "\n",
    "ax1.legend(handlelength=2,loc=2)\n",
    "ax1.set_title(r'\\texttt{robust sample}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = join(catalogue,temp,keys=['gal_name','region_ID'])\n",
    "#compare = compare[compare['mass1']>1e4]\n",
    "print(len(compare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(thesis_width,thesis_width))\n",
    "\n",
    "lim=[48,53]\n",
    "ax.scatter(np.log10(compare['Qpredicted_1']),np.log10(compare['Qpredicted_2']))\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.set(xlim=lim,ylim=lim,xlabel='Q form stellar associations',ylabel='Q from compact cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import plot_escape_fraction\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "tmp = catalogue[(catalogue['robust']) & (catalogue['age']<=2)]\n",
    "ax = plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],ax=ax,type='other',label='young')\n",
    "\n",
    "tmp = catalogue[(catalogue['robust']) & (catalogue['age']>2)]\n",
    "ax = plot_escape_fraction(tmp['Qpredicted'],tmp['Qpredicted_err'],tmp['Qobserved'],tmp['Qobserved_err'],ax=ax,type='other',label='old')\n",
    "\n",
    "ax.legend(loc=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of fesc\n",
    "\n",
    "Many regions have invalid escape fractions (<0). It is difficult to decide how to include them in the median and histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_median(values, weights):\n",
    "    i = np.argsort(values)\n",
    "    c = np.cumsum(weights[i])\n",
    "    return values[i[np.searchsorted(c, 0.5 * c[-1])]]\n",
    "\n",
    "fesc = catalogue[catalogue['robust']]['fesc']\n",
    "fesc_err = catalogue[catalogue['robust']]['fesc_err']\n",
    "\n",
    "weighted_median(fesc[fesc>0],weights=1/fesc_err[fesc>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def hist_with_uncertainty(mu,std,bins=np.linspace(0,100,21)):\n",
    "    '''Plot hisogram with uncertaintes\n",
    "    \n",
    "    For each bin in the histogram, the\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n = np.zeros(len(bins)-1)\n",
    "    for loc,scale in zip(mu,std):\n",
    "        cdf = norm.cdf(bins,100*loc,100*scale)\n",
    "        if not np.any(np.isnan(cdf)):\n",
    "            n+=cdf[1:]-cdf[:-1]\n",
    "    # normalize the result\n",
    "    n = n/np.sum(n)\n",
    "    \n",
    "    edges = (bins[1:]+bins[:-1])/2\n",
    "    \n",
    "    fig,ax=plt.subplots()\n",
    "    \n",
    "    ax.step(bins,np.append(n,0),where='post')\n",
    "    ax.set(xlim=[-2,102],xlabel=r'$f_\\mathrm{esc}$ / per\\,cent')\n",
    "    plt.show()\n",
    "    \n",
    "tmp = catalogue[catalogue['robust']].copy()\n",
    "#tmp = complexes_sample[complexes_sample['robust']]\n",
    "hist_with_uncertainty(tmp['fesc'],tmp['fesc_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "fesc = tmp['fesc']\n",
    "fesc_err = tmp['fesc_err']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "# first the normal histogram\n",
    "ax.hist(100*fesc,bins=np.linspace(0,100,21),label='hist',histtype='step')\n",
    "\n",
    "# next the histogram with the uncertainties\n",
    "bins=np.linspace(0,100,21)\n",
    "n = np.zeros(len(bins)-1)\n",
    "for loc,scale in zip(fesc,fesc_err):\n",
    "    cdf = norm.cdf(bins,100*loc,100*scale)\n",
    "    if not np.any(np.isnan(cdf)):\n",
    "        n+=cdf[1:]-cdf[:-1]\n",
    "n = n/np.sum(n)*np.sum(fesc>0)\n",
    "ax.step(bins,np.append(n,0),where='post',label='hist inc. error')\n",
    "\n",
    "# and finally the gaussian plot\n",
    "x = np.linspace(0,100,1000)\n",
    "y = np.zeros_like(x)\n",
    "for loc,scale in zip(fesc,fesc_err):\n",
    "    y += norm.pdf(x,loc=100*loc,scale=100*scale)\n",
    "ax.plot(x,y*np.max(n)/np.max(y),label='summed Gaussian')\n",
    "\n",
    "# or gaussian KDE\n",
    "kde = gaussian_kde(100*fesc,weights=1/(100*fesc_err))\n",
    "y_kde = kde(x)\n",
    "ax.plot(x,y_kde*np.max(n)/np.max(y_kde),label='Gaussian KDE')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[-2,102],xlabel=r'$f_\\mathrm{esc}$ / per\\,cent')\n",
    "plt.savefig(basedir/'reports'/'fesc_hist.pdf')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_negative = 0\n",
    "Ntotal = 0\n",
    "for loc,scale in zip(fesc,fesc_err):\n",
    "    N_negative += norm.cdf(0,100*loc,100*scale)\n",
    "    Ntotal += (norm.cdf(100,100*loc,100*scale) - norm.cdf(0,100*loc,100*scale))\n",
    "print(f'{Ntotal/len(fesc)*100:.2f} % have valid escape fractions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depending on what domain we compute the main, we get different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,1000)\n",
    "y = np.zeros_like(x)\n",
    "for loc,scale in zip(fesc,fesc_err):\n",
    "    y += norm.pdf(x,loc=loc,scale=scale)\n",
    "\n",
    "mid = np.argmin(np.abs(np.cumsum(y)/np.sum(y)-0.5))\n",
    "high = np.argmin(np.abs(np.cumsum(y)/np.sum(y)-0.8415))\n",
    "low = np.argmin(np.abs(np.cumsum(y)/np.sum(y)-0.1585))\n",
    "\n",
    "median, errp,errm = x[mid], x[high]-x[mid], x[mid]-x[low]\n",
    "\n",
    "print(f'fesc = {median:.2f} + {errp:.2f} - {errm:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "tmp = catalogue[catalogue['robust']].copy()\n",
    "tmp['fesc'][(tmp['fesc']<0) & (tmp['fesc']+tmp['fesc_err']>0)] = 0\n",
    "#tmp = tmp[tmp['fesc']<0.5]\n",
    "\n",
    "def combine_measurments(mu,std,lim=[-0.5,1.5]):\n",
    "    '''\n",
    "    \n",
    "    Assuming that individual measurments are normal distributed, \n",
    "    we combine them by summing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    mu : array\n",
    "        the measured value\n",
    "    std : array\n",
    "        the standard deviation\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    x = np.linspace(*lim,1000)\n",
    "    y = np.zeros_like(x)\n",
    "    for loc,scale in zip(mu,std):\n",
    "        y += norm.pdf(x,loc=loc,scale=scale)\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "x,y = combine_measurments(tmp['fesc'],tmp['fesc_err'])\n",
    "\n",
    "mid = np.argmin(np.abs(np.cumsum(y)/np.sum(y)-0.5))\n",
    "high = np.argmin(np.abs(np.cumsum(y)/np.sum(y)-0.8415))\n",
    "low = np.argmin(np.abs(np.cumsum(y)/np.sum(y)-0.1585))\n",
    "\n",
    "median, errp,errm = x[mid], x[high]-x[mid], x[mid]-x[low]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(nrows=2,figsize=(single_column,2*single_column/1.618),sharex=True)\n",
    "\n",
    "n,bins,_ = ax1.hist(tmp['fesc'],bins=np.linspace(0,1,20),histtype='step')\n",
    "ax1.plot(x,y*np.max(n[bins[1:]>0.5])/np.max(y))\n",
    "\n",
    "#for row in tmp:\n",
    "#    ax1.plot(x,0.002*norm.pdf(x,loc=row['fesc'],scale=row['fesc_err']),color='gray',zorder=0)\n",
    "    \n",
    "ax1.axvline(median,ls='--',c='k',lw=0.5)\n",
    "ax1.axvspan(1,1.5,color='gray',alpha=0.3,zorder=0,ec=None)\n",
    "ax1.axvspan(-0.5,0,color='gray',alpha=0.3,zorder=0,ec=None)\n",
    "ax1.set(xlim=[-0.5,1.5],ylabel='PDF')\n",
    "label = f'$f_\\mathrm{{esc}}={median:.2f}^{{+{errp:.2f}}}_{{-{errm:.2f}}}$'\n",
    "ax1.text(0.3,0.85,label, transform=ax1.transAxes,color='black',fontsize=8)\n",
    "\n",
    "ax2.plot(x,np.cumsum(y)/np.sum(y))\n",
    "\n",
    "ax2.axhline(0.5,ls='--',c='k',lw=0.5)\n",
    "ax2.axhline(0.5+0.683/2,ls='--',c='k',lw=0.5)\n",
    "ax2.axhline(0.5-0.683/2,ls='--',c='k',lw=0.5)\n",
    "ax2.axvline(median,ls='--',c='k',lw=0.5)\n",
    "ax2.axvline(median-errm,ls='--',c='k',lw=0.5)\n",
    "ax2.axvline(median+errp,ls='--',c='k',lw=0.5)\n",
    "ax2.axvspan(1,1.5,color='gray',alpha=0.3,zorder=0,ec=None)\n",
    "ax2.axvspan(-0.5,0,color='gray',alpha=0.3,zorder=0,ec=None)\n",
    "\n",
    "ax2.set(xlim=[-0.5,1.5],ylim=[0,1],xlabel=r'$f_\\mathrm{esc}$',ylabel='CDF')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/'fesc_pdf.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = np.argmin(np.abs(np.cumsum(y1)/np.sum(y1)-0.5))\n",
    "high = np.argmin(np.abs(np.cumsum(y1)/np.sum(y1)-0.8415))\n",
    "low = np.argmin(np.abs(np.cumsum(y1)/np.sum(y1)-0.1585))\n",
    "median, errp,errm = x1[mid], x1[high]-x1[mid], x1[mid]-x1[low]\n",
    "\n",
    "median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "x1 = np.linspace(-0.5,1.5,1000)\n",
    "\n",
    "kde = gaussian_kde(fesc,weights=1/fesc_err)\n",
    "y1 = kde(x1)\n",
    "\n",
    "mid = np.argmin(np.abs(np.cumsum(y1)/np.sum(y1)-0.5))\n",
    "high = np.argmin(np.abs(np.cumsum(y1)/np.sum(y1)-0.8415))\n",
    "low = np.argmin(np.abs(np.cumsum(y1)/np.sum(y1)-0.1585))\n",
    "median, errp,errm = x1[mid], x1[high]-x1[mid], x1[mid]-x1[low]\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(x1,y1,label='kde')\n",
    "ax.axvline(median,label='kde',color='black',ls=':')\n",
    "\n",
    "x2,y2 = combine_measurments(tmp['fesc'],tmp['fesc_err'])\n",
    "mid = np.argmin(np.abs(np.cumsum(y2)/np.sum(y2)-0.5))\n",
    "high = np.argmin(np.abs(np.cumsum(y2)/np.sum(y2)-0.8415))\n",
    "low = np.argmin(np.abs(np.cumsum(y2)/np.sum(y2)-0.1585))\n",
    "median, errp,errm = x2[mid], x2[high]-x2[mid], x2[mid]-x2[low]\n",
    "\n",
    "ax.plot(x2,y2*np.max(y1)/np.max(y2),label='summed gaussians')\n",
    "ax.axvline(median,label='summed gaussians',color='black',ls='-')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,0.9*single_column))\n",
    "\n",
    "Qpredicted = np.logspace(48,52)\n",
    "cmap = plt.cm.get_cmap('cool',6)\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "for i,f in enumerate([0.0,0.5,0.9,0.99]):\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    ax.plot(np.log10(Qpredicted),np.log10(Qobserved),ls=lines[i],c='k',label=f'$f_\\mathrm{{esc}}={f}$',zorder=1)\n",
    "Qobserved = Qpredicted*(1-0.76)\n",
    "ax.plot(np.log10(Qpredicted),np.log10(Qobserved),ls='-',color='gray',label=f'$f_\\mathrm{{esc}}={0.76}$',zorder=1)\n",
    "\n",
    "#ax.errorbar(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "#            xerr=tmp['Qpredicted_err']/tmp['Qpredicted'],yerr=tmp['Qobserved_err']/tmp['Qobserved'],fmt='',color='gray',zorder=0)\n",
    "sc=ax.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['density'],cmap=cmap,vmin=0.,vmax=60,s=2,zorder=2)\n",
    "ax.legend(handlelength=2)\n",
    "fig.colorbar(sc,label='density / cm$^{-3}$')\n",
    "\n",
    "ax.set(xlabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ predicted (SB99)',\n",
    "       ylabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ observed (MUSE)',\n",
    "       xlim=[49,52],ylim=[49,52])\n",
    "\n",
    "ax.set_xticks([49,50,51,52])\n",
    "ax.set_yticks([49,50,51,52])\n",
    "#ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4)) \n",
    "#ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4)) \n",
    "\n",
    "plt.savefig(basedir/'reports'/f'escape_fraction.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['bar', 'bar ends', 'center', 'interarm', 'interbar', 'spiral arms','others']\n",
    "l_to_n = {l:i for i,l in zip(range(len(labels)),labels)}\n",
    "\n",
    "#fig,((ax1,ax2),(ax3,ax4),(ax5,ax6)) =plt.subplots(ncols=2,nrows=3,figsize=(two_column,1.2*two_column))\n",
    "fig,((ax1,ax2,ax3),(ax4,ax5,ax6)) =plt.subplots(ncols=3,nrows=2,figsize=(1.2*two_column,two_column/1.5))\n",
    "\n",
    "\n",
    "criteria  = (catalogue['overlap_asc']==1) \n",
    "criteria  &= (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['age']<=10) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "print(f'{len(tmp)} objects with fesc={np.mean(tmp[tmp[\"fesc\"]>0][\"fesc\"]):.2f}')\n",
    "\n",
    "Qpredicted = np.logspace(47,52)\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "for i,f in enumerate([0.0,0.5,0.9,0.99]):\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    for ax in (ax1,ax2,ax3,ax4,ax5,ax6):\n",
    "        ax.plot(np.log10(Qpredicted),np.log10(Qobserved),ls=lines[i],c='k',label=f'$f_\\mathrm{{esc}}={f}$',zorder=1)\n",
    "\n",
    "cmap = plt.cm.get_cmap('plasma',5)\n",
    "sc1=ax1.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['logq_D91'],cmap=cmap,vmin=6,vmax=8,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('cividis',5)\n",
    "sc2=ax2.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['dig/hii'],cmap=cmap,vmin=0,vmax=1,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('viridis',4)\n",
    "sc3=ax3.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['EBV_balmer'],cmap=cmap,vmin=0,vmax=0.8,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('copper',5)\n",
    "sc4=ax4.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "                c=tmp['age'],cmap=cmap,vmin=0,vmax=10,s=2,zorder=2)\n",
    "#cmap = plt.cm.get_cmap('ocean',5)\n",
    "#https://stackoverflow.com/questions/18926031/how-to-extract-a-subset-of-a-colormap-as-a-new-colormap-in-matplotlib\n",
    "#cmap = mpl.colors.LinearSegmentedColormap.from_list('new_ocean',cmap(np.linspace(0, 0.79, 4)),8)\n",
    "#sc5=ax5.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "#                c=np.log10(tmp['HA6562_LUM_CORR']),cmap=cmap,vmin=37,vmax=39,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('cool',6)\n",
    "sc5=ax5.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "                c=tmp['density'],cmap=cmap,vmin=0.,vmax=60,s=2,zorder=2)\n",
    "\n",
    "tmp.sort('env_neb',reverse=True)\n",
    "cmap = plt.cm.get_cmap('Spectral',6)\n",
    "env = [l_to_n.get(x,6) for x in tmp['env_neb']]\n",
    "sc6=ax6.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=env,cmap=cmap,vmin=-0.5,vmax=5.5,s=1,zorder=2)\n",
    "\n",
    "''' # check if the colorbar is correctly labeld\n",
    "for col in labels:\n",
    "    sub = tmp[tmp['env_neb']==col]\n",
    "    env = [l_to_n.get(x,6) for x in sub['env_neb']]\n",
    "    sc6=ax6.scatter(np.log10(sub['Qpredicted']),np.log10(sub['Qobserved']),\n",
    "                  c=env,cmap=cmap,vmin=-0.5,vmax=5.5,s=2,zorder=2,label=col)\n",
    "ax6.legend()\n",
    "'''\n",
    "\n",
    "ax1.legend(handlelength=2)\n",
    "fig.colorbar(sc1,ax=ax1,label=r'$\\log q$ (from Diaz 91)',ticks=[6,6.4,6.8,7.2,7.6,8])\n",
    "fig.colorbar(sc2,ax=ax2,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "fig.colorbar(sc3,ax=ax3,label=r'$E(B-V)_\\mathrm{Balmer}$',ticks=[0,0.2,0.4,0.6,0.8])\n",
    "fig.colorbar(sc4,ax=ax4,label=r'age / Myr')\n",
    "fig.colorbar(sc5,ax=ax5,label=r'density / cm$^{-3}$')\n",
    "cbar=fig.colorbar(sc6,ax=ax6,ticks=np.arange(0.,5.5,1))\n",
    "cbar.ax.set_yticklabels(labels[:-1])  # vertically oriented colorbar\n",
    "\n",
    "for ax in (ax4,ax5,ax6):\n",
    "    ax.set(xlabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ predicted')\n",
    "for ax in (ax1,ax4):\n",
    "    ax.set(ylabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ observed')\n",
    "\n",
    "for ax in (ax1,ax2,ax3,ax4,ax5,ax6):\n",
    "    ax.set(xlim=[49,52],ylim=[49,52])\n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(3)) \n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(3)) \n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'escape_fraction_{HSTband}_{scalepc}pc.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "\n",
    "for env in ['interarm','interbar','spiral arms']:\n",
    "    sub = tmp[tmp['env_neb']==env]\n",
    "    print(f'{env}: {100*np.nanmean(sub[\"fesc\"]):.1f}+-{100*np.nanstd(sub[\"fesc\"]):.1f} % ({len(sub)})')\n",
    "    \n",
    "    ax.hist(100*sub['fesc'],bins=np.linspace(0,100,20),\n",
    "            histtype='step',alpha=0.9,label=env)\n",
    "    \n",
    "ax.legend(loc=2)\n",
    "ax.set(xlabel=r'$f_\\mathrm{esc}$ / percent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "fig,((ax1,ax2,ax3),(ax4,ax5,ax6)) =plt.subplots(ncols=3,nrows=2,figsize=(two_column,two_column/1.5))\n",
    "\n",
    "\n",
    "for col in ['logq_D91','dig/hii','EBV_balmer','age','mass','density']:\n",
    "    rho,p = spearmanr(tmp[col],tmp['fesc'],nan_policy='omit')\n",
    "\n",
    "    print(f'{col:>10}: rho={rho:>5.2f}, p-value={p:>7.2g}')\n",
    "\n",
    "ax1.scatter(tmp['logq_D91'],tmp['fesc'])\n",
    "ax1.set(xlabel=r'$\\log q$ (from Diaz 91)')\n",
    "\n",
    "ax2.scatter(tmp['dig/hii'],tmp['fesc'])\n",
    "ax2.set(xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "\n",
    "ax3.scatter(tmp['EBV_balmer'],tmp['fesc'])\n",
    "ax3.set(xlabel=r'$E(B-V)_\\mathrm{stars}$')\n",
    "\n",
    "ax4.scatter(tmp['age'],tmp['fesc'])\n",
    "ax4.set(xlabel=r'age / Myr',xlim=[0,10])\n",
    "\n",
    "ax5.scatter(tmp['mass'],tmp['fesc'])\n",
    "ax5.set(xlabel=r'$\\log_{10} M/\\mathrm{M}_\\odot$',xscale='log')\n",
    "\n",
    "ax6.scatter(tmp['density'],tmp['fesc'])\n",
    "ax6.set(xlabel=r'density')\n",
    "\n",
    "\n",
    "\n",
    "for ax in [ax1,ax2,ax3,ax4,ax5,ax6]:\n",
    "    ax.set(ylim=[0,1.1])\n",
    "for ax in [ax1,ax4]:\n",
    "    ax.set(ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External sources\n",
    "\n",
    "we need to ionizing photon flux of most associations, so we compute them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir/'data'/'interim'/f'neighboring_associations.yml') as f:\n",
    "    neighboring_associations = yaml.load(f,Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.escape_fractions import ionising_photon_flux\n",
    "\n",
    "flux, flux_err = ionising_photon_flux(\n",
    "                                     age = associations['age'],\n",
    "                                     age_err = associations['age_err'],\n",
    "                                     model_age = model_age,\n",
    "                                     model_flux = model_flux,\n",
    "                                     sample_size=100\n",
    "                                     )\n",
    "\n",
    "Qpredicted     = flux*associations['mass']\n",
    "Qpredicted_err = Qpredicted * np.sqrt((flux_err/flux)**2+(associations['mass_err']/associations['mass'])**2)\n",
    "\n",
    "associations['Qpredicted']     = Qpredicted\n",
    "associations['Qpredicted_err'] = Qpredicted_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "subsample = catalogue.copy()\n",
    "subsample['Qexternal'] = 0.0\n",
    "subsample['Nexternal'] = 0\n",
    "subsample['d_max'] = 0.\n",
    "\n",
    "gal_name = None \n",
    "for row in tqdm(subsample):\n",
    "    \n",
    "    if row['gal_name']!=gal_name:\n",
    "        gal_name = row['gal_name']\n",
    "        assoc_sub = associations[associations['gal_name']==gal_name]\n",
    "        assoc_sub['distance'] = np.nan\n",
    "        assoc_sub.add_index('assoc_ID')\n",
    "        \n",
    "        # these properties also change with gal_name\n",
    "        distance = row['dist']\n",
    "        arcsec_to_parsec = distance*1e6*u.arcsec.to(u.rad)\n",
    "    \n",
    "    dic = neighboring_associations[gal_name][row['region_ID']]\n",
    "    assoc_IDs = np.atleast_1d(list(dic.keys()))\n",
    "    distances = np.atleast_1d(list(dic.values())) \n",
    "    \n",
    "    distances = distances[np.isin(assoc_IDs,assoc_sub['assoc_ID'])]\n",
    "    assoc_IDs = assoc_IDs[np.isin(assoc_IDs,assoc_sub['assoc_ID'])]\n",
    "    \n",
    "    row['d_max'] = np.max(distances) * arcsec_to_parsec\n",
    "    row['Nexternal'] = len(distances)\n",
    "    \n",
    "    # area in arcsec2\n",
    "    area_neb = row['area_neb'] / 25\n",
    "        \n",
    "    if len(dic)>1:\n",
    "        tmp = assoc_sub.loc[assoc_IDs]\n",
    "        tmp['distance'] = distances\n",
    "        tmp = tmp[tmp['distance']>0]\n",
    "\n",
    "        Qexternal = tmp['Qpredicted'] / (4*np.pi*tmp['distance']**2) * area_neb \n",
    "\n",
    "        # we exlude the associations that overlap and those that are contained within another \n",
    "        # nebula. only associations within 500 pc are considered\n",
    "        row['Qexternal'] += np.sum(Qexternal[(tmp['distance']>0) & (tmp['overlap_asc']<1) & (tmp['distance']*arcsec_to_parsec<=1000)])\n",
    "        #row['Nexternal'] += 1\n",
    "    \n",
    "    else:\n",
    "        tmp = assoc_sub.loc[list(dic.keys())]\n",
    "        tmp['distance'] = np.atleast_1d(list(dic.values()))\n",
    "        if (tmp['distance']==0) or (tmp['overlap_asc']==1) or (tmp['distance']*arcsec_to_parsec>1000):\n",
    "            continue\n",
    "\n",
    "        Qexternal = tmp['Qpredicted'] / (4*np.pi*tmp['distance']**2) * area_neb \n",
    "        row['Qexternal'] += np.sum(Qexternal)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=5,vmax=20)\n",
    "bins = np.linspace(0,500)\n",
    "\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    tmp = subsample[subsample['gal_name']==gal_name]\n",
    "    dist = sample_table_v1p6.loc[gal_name]['dist']\n",
    "    ax.hist(tmp['d_max'],bins=bins,color=cmap(norm(dist)),alpha=0.6)\n",
    "    \n",
    "ax.set(xlim=[10,100],xlabel=r'$r_\\mathrm{max}$ / pc')\n",
    "plt.subplots_adjust(right=0.92)\n",
    "cbar_ax = fig.add_axes([0.95, 0.12, 0.02, 0.75])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,\n",
    "             label='distance / Mpc',ticks=np.arange(5,20,6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=5,vmax=20)\n",
    "bins = np.linspace(0,100)\n",
    "\n",
    "nebulae['r_pc'] = np.sqrt(nebulae['area_neb']/np.pi)/5 * nebulae['dist']*1e6*u.arcsec.to(u.rad)\n",
    "\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    tmp = nebulae[nebulae['gal_name']==gal_name]\n",
    "    dist = sample_table_v1p6.loc[gal_name]['dist']\n",
    "    ax.hist(tmp['r_pc'],bins=bins,color=cmap(norm(dist)),alpha=0.6)\n",
    "    \n",
    "ax.set(xlim=[10,100],xlabel=r'$r_\\mathrm{nebula}$ / pc')\n",
    "plt.subplots_adjust(right=0.92)\n",
    "cbar_ax = fig.add_axes([0.95, 0.12, 0.02, 0.75])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,\n",
    "             label='distance / Mpc',ticks=np.arange(5,20,6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in sample_table_v1p6:\n",
    "    distance = row['dist']\n",
    "    arcsec_to_parsec = distance*1e6*u.arcsec.to(u.rad)\n",
    "    print(f'{row[\"gal_name\"]}: {arcsec_to_parsec*20:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(-3,3,21)\n",
    "\n",
    "tmp = subsample[(subsample['Qpredicted']>subsample['Qobserved'])]\n",
    "ax.hist(tmp['Qexternal']/tmp['Qpredicted'],bins=bins,label=r'positive $f_\\mathrm{esc}$',alpha=0.5)\n",
    "tmp = tmp[np.isfinite(tmp['Qexternal'])]\n",
    "print(f\"fesc>0: Qe/Qp={np.nanmedian(tmp['Qexternal']/tmp['Qpredicted']):.5f}\")\n",
    "\n",
    "tmp = subsample[(subsample['Qpredicted']<subsample['Qobserved'])]\n",
    "ax.hist(tmp['Qexternal']/tmp['Qpredicted'],bins=bins,label=r'negative $f_\\mathrm{esc}$',alpha=0.5)\n",
    "tmp = tmp[np.isfinite(tmp['Qexternal'])]\n",
    "print(f\"fesc<0: Qe/Qp={np.nanmedian(tmp['Qexternal']/tmp['Qpredicted']):.5f}\")\n",
    "\n",
    "ax.axvline(1,color='gray')\n",
    "ax.legend()\n",
    "ax.set(xlim=[1e-3,1e3],xscale='log',xlabel=r'$Q (\\mathrm{H}^0)_\\mathrm{external}\\,/\\,Q (\\mathrm{H}^0)_\\mathrm{internal}$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'external_ionising_sources.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(-3,3,21)\n",
    "\n",
    "tmp = subsample[(subsample['Qpredicted']>subsample['Qobserved'])]\n",
    "ax.scatter(tmp['Qexternal']/tmp['Qpredicted'],tmp['fesc'],s=1)\n",
    "ax.axvline(1,color='gray')\n",
    "\n",
    "ax.set(ylim=[0,1],xlim=[1e-3,1e3],xscale='log',ylabel=r'$f_\\mathrm{esc}$',\n",
    "       xlabel=r'$Q (\\mathrm{H}^0)_\\mathrm{external}\\,/\\,Q (\\mathrm{H}^0)_\\mathrm{internal}$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'external_ionising_sources.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends with Fesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define limits and labels for corner plots\n",
    "limits   = {\n",
    "            'Delta_met_scal':(-0.1,0.1),\n",
    "            'density':(0,100),\n",
    "            'dig/hii':(0.2,0.9),\n",
    "            'fesc':(0,1),\n",
    "            'HA/FUV_corr':(0.8,60),\n",
    "            'HA6562_LUM_CORR':(5e35,5e39),\n",
    "            'EBV_balmer':(0,1),\n",
    "            'logq_D91':(5.7,8),\n",
    "            'EW_HA':(6,600),\n",
    "            'EW_HA_CORR':(60,6000),\n",
    "            'temperature':(6000,8e3),\n",
    "            'age' : (0,8),\n",
    "            'mass' : (1e4,2e5)\n",
    "            }\n",
    "\n",
    "\n",
    "labels   = {\n",
    "            'Delta_met_scal':r'$\\Delta$(O/H)',\n",
    "            'density':r'density / cm$^{-3}$','temperature':'T / K',\n",
    "            'dig/hii':r'$I_\\mathrm{DIG}\\,/\\,I_{\\mathrm{H}\\,\\tiny{\\textsc{ii}}}$',\n",
    "            'fesc':r'$f_\\mathrm{esc}$',\n",
    "            'HA/FUV_corr':r'H$\\alpha$ / FUV',\n",
    "            'HA6562_LUM_CORR':r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "            'EBV_balmer':r'$E(B-V)_\\mathrm{Balmer}$',\n",
    "            'EW_HA' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$',\n",
    "            'EW_HA_CORR' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)_{\\mathrm{corr}}/\\mathrm{\\AA}$',\n",
    "            'logq_D91':r'$\\log q$',\n",
    "            'age' : 'age / Myr',\n",
    "            'mass' : r'mass / $\\mathrm{M}_\\odot$'\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with fesc on x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile, \\\n",
    "                                   corner_scatter\n",
    "from scipy.stats import spearmanr,binned_statistic\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "columns = ['logq_D91','density','EBV_balmer','dig/hii']\n",
    "tmp = catalogue[catalogue['robust']]\n",
    "\n",
    "def plot_function(x,y,group_by,ax,bins,**kwargs):\n",
    "\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "    \n",
    "    for gal_name in np.unique(group_by):\n",
    "        x_sub,y_sub=x[group_by==gal_name],y[group_by==gal_name]\n",
    "        corner_scatter(x_sub,y_sub,ax,s=4,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        \n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$'\n",
    "    t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "\n",
    "fig,axes=plt.subplots(nrows=len(columns),figsize=(single_column,single_column*len(columns)/2),sharex='col')\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "for ax,col in zip(axes,columns):\n",
    "    \n",
    "    bins = np.linspace(0,1,11)\n",
    "    plot_function(tmp['fesc'],tmp[col],tmp['gal_name'],ax,bins=bins)\n",
    "    ax.set(ylabel=labels[col],ylim=limits[col])\n",
    "    \n",
    "    if col == 'HA6562_LUM_CORR':\n",
    "        ax.set(yscale='log')\n",
    "    \n",
    "    if col == 'logq_D91':\n",
    "        ymin,ymax=limits['logq_D91']\n",
    "        ax_right = ax.twinx()\n",
    "        ax_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "        ax_right.set_yticklabels([])\n",
    "        ax_right.set_ylabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "        ax_right.set_yticks([0.1,1])\n",
    "        ax_right.set_yticklabels(['0.1','1'])\n",
    "    \n",
    "ax.set(xlim=[0,1],xscale='linear',xlabel=r'$f_\\mathrm{esc}$ / per\\,cent')       \n",
    "#axes[0].set(ylabel=labels['fesc'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.05,top=0.92)\n",
    "cbar_ax = fig.add_axes([0.17, 0.95, 0.67, 0.02])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='horizontal',ticklocation='top',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'trends_with_fesc.pdf',dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or with fesc on y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile, \\\n",
    "                                   corner_scatter\n",
    "from scipy.stats import spearmanr,binned_statistic\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "tmp = catalogue[catalogue['robust']]\n",
    "\n",
    "def plot_function(x,y,group_by,ax,bins,**kwargs):\n",
    "\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "    \n",
    "    for gal_name in np.unique(group_by):\n",
    "        x_sub,y_sub=x[group_by==gal_name],y[group_by==gal_name]\n",
    "        corner_scatter(x_sub,y_sub,ax,s=4,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        \n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$'\n",
    "    #t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    #t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=len(columns),figsize=(two_column,two_column/len(columns)*1.2),sharey='row')\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "    \n",
    "bins = np.linspace(0.5,8.5,9)\n",
    "plot_function(tmp['age'],tmp['fesc'],tmp['gal_name'],ax1,bins=bins)\n",
    "ax1.set(xlabel=labels['age'],xlim=limits['age'])\n",
    "\n",
    "bins = np.logspace(4,5.3,6)\n",
    "plot_function(tmp['mass'],tmp['fesc'],tmp['gal_name'],ax2,bins=bins)\n",
    "ax2.set(xlabel=labels['mass'],xlim=limits['mass'],xscale='log')\n",
    "\n",
    "bins = np.linspace(0,100,6)\n",
    "plot_function(tmp['density'],tmp['fesc'],tmp['gal_name'],ax3,bins=bins)\n",
    "ax3.set(xlabel=labels['density'],xlim=limits['density'])\n",
    "\n",
    "bins = np.linspace(0,1,6)\n",
    "plot_function(tmp['EBV_balmer'],tmp['fesc'],tmp['gal_name'],ax4,bins=bins)\n",
    "ax4.set(xlabel=labels['EBV_balmer'],xlim=limits['EBV_balmer'])\n",
    "\n",
    "\n",
    "if col == 'logq_D91':\n",
    "    xmin,xmax=limits['logq_D91']\n",
    "    ax_top = ax.twiny()\n",
    "    ax_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "    ax_top.set_xticklabels([])\n",
    "    ax_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "    ax_top.set_xticks([0.1,1])\n",
    "    ax_top.set_xticklabels(['0.1','1'])\n",
    "        \n",
    "ax1.set(ylabel=labels['fesc'],ylim=[0,1],yscale='linear')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.12,right=0.95)\n",
    "cbar_ax = fig.add_axes([0.97, 0.22, 0.02, 0.7])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='vertical',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'trends_with_fesc.pdf',dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ha luminosity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(figsize=(two_column,two_column/3.5),ncols=4,sharey=True)\n",
    "\n",
    "tmp = catalogue\n",
    "ax1.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='1to1',zorder=0,histtype='step')\n",
    "ax1.set_title(r'\\texttt{1to1 sample}')\n",
    "#ax1.set_aspect('equal', adjustable='box')\n",
    "print(f\"1to1: {np.nanmedian(tmp['HA6562_LUM_CORR']):.2g} erg s-1\")\n",
    "\n",
    "tmp = nebulae_extended\n",
    "ax2.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='1to1',zorder=0,histtype='step')\n",
    "ax2.set(ylabel='')\n",
    "ax2.set_title(r'\\texttt{extended sample}')\n",
    "#ax2.set_aspect('equal', adjustable='box')\n",
    "print(f\"Extended: {np.nanmedian(tmp['HA6562_LUM_CORR']):.2g} erg s-1\")\n",
    "\n",
    "tmp = complexes_sample\n",
    "ax3.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='1to1',zorder=0,histtype='step')\n",
    "ax3.set(ylabel='')\n",
    "ax3.set_title(r'\\texttt{complexes sample}')\n",
    "#ax3.set_aspect('equal', adjustable='box')\n",
    "print(f\"Complexes: {np.nanmedian(tmp['HA6562_LUM_CORR']):.2g} erg s-1\")\n",
    "\n",
    "tmp = nebulae_cluster\n",
    "ax4.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='1to1',zorder=0,histtype='step')\n",
    "ax4.set(ylabel='')\n",
    "ax4.set_title(r'\\texttt{cluster sample}')\n",
    "#ax4.set_aspect('equal', adjustable='box')\n",
    "print(f\"Cluster: {np.nanmedian(tmp['HA6562_LUM_CORR']):.2g} erg s-1\")\n",
    "\n",
    "for ax in [ax1,ax2,ax3,ax4]:\n",
    "    ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='linear')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'Ha_luminosity_subsamples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "fig,ax= plt.subplots(figsize=(thesis_width,thesis_width/1.618),ncols=1)\n",
    "\n",
    "tmp = catalogue\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label=r'\\texttt{1to1 sample}',zorder=0,histtype='step')\n",
    "\n",
    "tmp = nebulae_extended\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label=r'\\texttt{extended sample}',zorder=0,histtype='step')\n",
    "\n",
    "tmp = complexes_sample\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label=r'\\texttt{complexes sample}',zorder=0,histtype='step')\n",
    "\n",
    "tmp = nebulae_cluster\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label=r'\\texttt{cluster sample}',zorder=0,histtype='step')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "   yscale='linear')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'Ha_luminosity_subsamples.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mass vs observed Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Ha on a grid of different ages/masses with Starburst99\n",
    "age_grid = np.array([0,6,12,18])*u.Myr/2\n",
    "mass_grid = np.logspace(2,7,10)\n",
    "ionizing_photons = np.zeros((len(age_grid),len(mass_grid)))\n",
    "for i,mass in enumerate(mass_grid):\n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    for j,age in enumerate(age_grid):\n",
    "        idx = np.argmin(np.abs(scaled_cluster.quanta['Time']-age))\n",
    "        ionizing_photons[j,i] = scaled_cluster.quanta['HI_rate'][idx].value\n",
    "distance = 10*u.Mpc\n",
    "LHa = ionizing_photons / 7.31e11 * u.erg \n",
    "FHa = LHa / (1e-20*u.erg/u.s/u.cm**2 *4*np.pi*distance**2).to(u.erg/u.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 4,5\n",
    "\n",
    "aspect_ratio = 0.9\n",
    "cmap = plt.cm.get_cmap('copper',6)\n",
    "fig=plt.figure(figsize=(two_column,aspect_ratio*nrows/ncols*two_column))\n",
    "axes = []\n",
    "\n",
    "criteria  = (catalogue['overlap_asc']==1) \n",
    "criteria  &= (catalogue['mass']>1e2) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "#criteria &= fesc>0\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "\n",
    "sample = list(np.unique(tmp['gal_name']))\n",
    "#sample.remove('IC5332')\n",
    "\n",
    "for i,gal_name in enumerate(sample):\n",
    "    ax = fig.add_subplot(nrows,ncols,i+1)\n",
    "    axes.append(ax)\n",
    "    sub = tmp[tmp['gal_name']==gal_name]\n",
    "    \n",
    "    distance = sub['distance'][0]*u.Mpc\n",
    "    LHa = ionizing_photons / 7.31e11 * u.erg \n",
    "    FHa = LHa / (u.erg/u.s/u.cm**2 *4*np.pi*distance**2).to(u.erg/u.s)\n",
    "    \n",
    "    sc=ax.scatter(sub['mass'],1e-20*sub['HA6562_FLUX_CORR'],s=3,\n",
    "                  c=sub['age'],cmap=cmap,vmin=0,vmax=9,zorder=2)\n",
    "    \n",
    "    # plot the theoretical liens\n",
    "    for j,age in enumerate(age_grid):\n",
    "        color = cmap(age/9/u.Myr)\n",
    "        ax.plot(mass_grid,FHa[j,:],label=age,color=color)\n",
    "    \n",
    "    ax.set(xlim=[2e2,2e5],ylim=[1e-17,1e-12],xscale='log',yscale='log')\n",
    "    ax.text(0.05,0.85,f'{gal_name}', transform=ax.transAxes,fontsize=7)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1e4))\n",
    "    #ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(1e5))\n",
    "    \n",
    "    if i%ncols==0:\n",
    "        #ax.set(ylabel=r'$F(\\mathrm{H}\\alpha)$ / erg s$^{-1}$ cm$^{-2}$')\n",
    "        pass\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    if i//ncols==nrows-1:\n",
    "        #ax.set(xlabel=r'mass / M$_\\odot$')\n",
    "        pass\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "    #print(np.diff(ax.get_xlim())/np.diff(ax.get_ylim()))\n",
    "    #ax.set_aspect(0.5)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.05,hspace=0.05,right=0.9,left=0.1,bottom=0.1)\n",
    "fig.text(0.5, 0.04, r'mass / M$_\\odot$', ha='center')\n",
    "fig.text(0.0, 0.5, r'$F(\\mathrm{H}\\alpha)$ / erg s$^{-1}$ cm$^{-2}$', va='center', rotation='vertical')\n",
    "    \n",
    "# Create the legend\n",
    "h,l = axes[0].get_legend_handles_labels()\n",
    "fig.legend(h,l,\n",
    "           ncol=5,\n",
    "           #loc=\"upper center\",   # Position of legend\n",
    "           #borderaxespad=-0.1,    # Small spacing around legend box\n",
    "           bbox_to_anchor=(0.25, 0.9, 0.5, 0.05) \n",
    "          )\n",
    "\n",
    "cbar_ax = fig.add_axes([0.93, 0.08, 0.02, 0.8])\n",
    "fig.colorbar(sc,cax=cbar_ax,label='age / Myr',ticks=age_grid)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'mass_vs_Halpha_{HSTband}_{scalepc}pc.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Inspect sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "tmp = catalogue\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='1to1',zorder=0)\n",
    "print(f\"1to1: {np.mean(tmp['HA6562_LUM_CORR']):.2g}\")\n",
    "\n",
    "tmp = nebulae_extended\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='extened',zorder=0)\n",
    "print(f\"extened: {np.mean(tmp['HA6562_LUM_CORR']):.2g}\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='linear',ylim=[1,800],ylabel='N')\n",
    "#plt.savefig(basedir/'reports'/'Halpha_luminosity_function.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### For the stellar associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub = extended \n",
    "groups = sub.group_by(['gal_name','region_ID'])\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "k=0\n",
    "for group in groups.groups:\n",
    "    if np.all(group['mass']>1e4):\n",
    "        i+=1\n",
    "    if np.all(group['overlap']=='contained'):\n",
    "        j+=1\n",
    "    if np.all(group['mass']>1e4) & np.all(group['overlap']=='contained') & np.all(group['age']<=8):\n",
    "        k+=1\n",
    "        \n",
    "print(f'in {i} of {len(groups.groups)} groups, all associations are massive')\n",
    "print(f'in {j} of {len(groups.groups)} groups, all associations are contained')\n",
    "print(f'in {k} fullfill both criteria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Photon flux of individual objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub = extended \n",
    "groups = sub.group_by(['gal_name','region_ID'])\n",
    "\n",
    "# aggregate additional properties to the group\n",
    "temp = groups.groups.keys\n",
    "temp['Qpredicted'] = groups['Qpredicted'].groups.aggregate(np.sum)\n",
    "temp['mass'] = groups['mass'].groups.aggregate(np.sum)\n",
    "temp.sort('mass',reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/3))\n",
    "cmap = plt.cm.get_cmap('viridis', 10)\n",
    "norm = mpl.colors.Normalize(vmin=0.5,vmax=10.5)\n",
    "\n",
    "y1,y2=5,50\n",
    "x1,x2=46,52\n",
    "a = (y2-y1) / (x2-x1)\n",
    "b = y1-a*x1\n",
    "\n",
    "i=0\n",
    "#for row in temp[~np.isnan(temp['mass'])][200::16]:\n",
    "for row in temp[temp['gal_name']=='NGC2835']:\n",
    "    mask  = (extended['region_ID'] == row['region_ID']) & (extended['gal_name']==row['gal_name'])\n",
    "    group = extended[mask]\n",
    "    \n",
    "    i+=1\n",
    "    sub = group[group['overlap']=='contained']\n",
    "    s = a*np.log10(sub['Qpredicted'])+b\n",
    "    s[s<5]  = 5\n",
    "    s[s>50] = 50\n",
    "    sc= ax.scatter(len(sub)*[i],sub['mass'],c=sub['age'],s=s,marker='o',vmin=0,vmax=10,cmap=cmap)\n",
    "    sub = group[group['overlap']=='partial']\n",
    "    s = a*np.log10(sub['Qpredicted'])+b\n",
    "    s[s<5]  = 5\n",
    "    s[s>50] = 50\n",
    "    sc= ax.scatter(len(sub)*[i],sub['mass'],c=sub['age'],s=s,marker='o',vmin=0.5,vmax=10.5,cmap=cmap)\n",
    "    sc= ax.scatter(len(sub)*[i],sub['mass'],c='white',s=s/8,marker='o',vmin=0.5,vmax=10.5,cmap=cmap)\n",
    "\n",
    "    # we want at least one young and massive association that is fully contained\n",
    "    # and no youg and massive objects with partial overlap\n",
    "    if not np.any((group['mass']>1e4) & (group['age']<=8) & (group['overlap']=='contained')) or np.any((group['mass']>1e4) & (group['overlap']=='partial') & (group['age']<=8)):\n",
    "        ax.axvspan(i-0.4,i+0.4,color='gray',alpha=0.3,zorder=0,ec=None)\n",
    "    \n",
    "# adjustments to the figure\n",
    "for s in [52,49,46]:\n",
    "    ax.scatter([-1],[-1],s=a*s+b,color='gray',label=f'$10^{{{s}}}$')\n",
    "ax.legend(title=r'$Q(\\mathrm{H}^0)\\,/\\,\\mathrm{s}^{-1}$',title_fontsize='xx-small',loc=2)\n",
    "\n",
    "ax.set(xlim=[0.5,i+0.5],yscale='log',ylim=[2e2,3e5],\n",
    "       xlabel=r'H\\,\\textsc{ii} regions',ylabel=r'mass / $\\mathrm{M}_\\odot$')\n",
    "ax.axhline(1e4,color='gray',lw=0.5,zorder=0)\n",
    "ax.set_xticks([])\n",
    "#ax.grid()\n",
    "\n",
    "fig.subplots_adjust(right=0.95,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.96, 0.126, 0.02, 0.75])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,label=r'age / Myr')\n",
    "plt.savefig(basedir/'reports'/'association_groups.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "same plot but mass and Q are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/3))\n",
    "cmap = plt.cm.get_cmap('viridis', 10)\n",
    "norm = mpl.colors.Normalize(vmin=0.5,vmax=10.5)\n",
    "\n",
    "y1,y2=5,50\n",
    "x1,x2=2,6\n",
    "a = (y2-y1) / (x2-x1)\n",
    "b = y1-a*x1\n",
    "\n",
    "i=0\n",
    "#for row in temp[~np.isnan(temp['mass'])][200::16]:\n",
    "for row in temp[temp['gal_name']=='NGC2835']:\n",
    "    mask  = (extended['region_ID'] == row['region_ID']) & (extended['gal_name']==row['gal_name'])\n",
    "    group = extended[mask]\n",
    "    \n",
    "    i+=1\n",
    "    sub = group[group['overlap']=='contained']\n",
    "    s = a*np.log10(sub['mass'])+b\n",
    "    s[s<5]  = 5\n",
    "    s[s>50] = 50\n",
    "    sc= ax.scatter(len(sub)*[i],sub['Qpredicted'],c=sub['age'],s=s,marker='o',vmin=0,vmax=10,cmap=cmap)\n",
    "    sub = group[group['overlap']=='partial']\n",
    "    s = a*np.log10(sub['mass'])+b\n",
    "    s[s<5]  = 5\n",
    "    s[s>50] = 50\n",
    "    sc= ax.scatter(len(sub)*[i],sub['Qpredicted'],c=sub['age'],s=s,marker='o',vmin=0.5,vmax=10.5,cmap=cmap)\n",
    "    sc= ax.scatter(len(sub)*[i],sub['Qpredicted'],c='white',s=s/8,marker='o',vmin=0.5,vmax=10.5,cmap=cmap)\n",
    "\n",
    "    # we want at least one young and massive association that is fully contained\n",
    "    # and no youg and massive objects with partial overlap\n",
    "    if not np.any((group['mass']>1e4) & (group['age']<=8) & (group['overlap']=='contained')) or np.any((group['mass']>1e4) & (group['overlap']=='partial') & (group['age']<=8)):\n",
    "        ax.axvspan(i-0.4,i+0.4,color='gray',alpha=0.3,zorder=0,ec=None)\n",
    "    \n",
    "# adjustments to the figure\n",
    "for s in [6,4,2]:\n",
    "    ax.scatter([-1],[-1],s=a*s+b,color='gray',label=f'$10^{{{s}}}$')\n",
    "ax.legend(title=r'mass / M$_\\odot$',title_fontsize='xx-small',loc=2)\n",
    "\n",
    "ax.set(xlim=[0.5,i+0.5],yscale='log',ylim=[1e47,1e53],\n",
    "       xlabel=r'H\\,\\textsc{ii} regions',ylabel=r'$Q(\\mathrm{H}^0)\\,/\\,\\mathrm{s}^{-1}$')\n",
    "#ax.axhline(1e4,color='gray',lw=0.5,zorder=0)\n",
    "ax.set_xticks([])\n",
    "#ax.grid()\n",
    "\n",
    "fig.subplots_adjust(right=0.95,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.96, 0.126, 0.02, 0.75])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,label=r'age / Myr')\n",
    "plt.savefig(basedir/'reports'/'association_groups_Q.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plot cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from cluster.io import read_associations\n",
    "from cluster.plot import single_cutout\n",
    "\n",
    "#tmp = catalogue[catalogue['Qpredicted']+3*catalogue['Qpredicted_err']<catalogue['Qobserved']-3*catalogue['Qpredicted_err']]\n",
    "#tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['age']<=8) & (catalogue['overlap']=='contained')]\n",
    "#tmp = nebulae_extended[nebulae_extended['mass']>1e4]\n",
    "#tmp = nebulae_extended[nebulae_extended['robust']]\n",
    "#tmp = nebulae[(nebulae['gal_name']=='NGC2835') & (nebulae['Nassoc']>=1) & (nebulae['neighbors']==0)]\n",
    "#tmp = catalogue[(catalogue['gal_name']=='NGC2835') & (catalogue['overlap']=='partial')]\n",
    "tmp = catalogue[catalogue['robust'] & (catalogue['fesc']<0.) ]\n",
    "tmp = catalogue[catalogue['robust'] & (catalogue['fesc']+catalogue['fesc_err']<0.) ]\n",
    "#tmp = tmp[:95]\n",
    "\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "size=8*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/'cutouts'/f'cutouts_negative_fesc_error'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                \n",
    "                filename = data_ext / 'MUSE' / 'DR2.1' / 'MUSEDAP' / f'{gal_name}_MAPS.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                                    meta=hdul['HA6562_FLUX'].header,\n",
    "                                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "            \n",
    "                # nebulae mask\n",
    "                filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "                \n",
    "                # finally the compact clusters\n",
    "                filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "                if not filename.is_file():\n",
    "                    print(f'no compact clusters for {gal_name}')\n",
    "                    clusters = None\n",
    "                else:     \n",
    "                    clusters = Table.read(filename)\n",
    "                    clusters['SkyCoord'] = SkyCoord(clusters['PHANGS_RA']*u.deg,clusters['PHANGS_DEC']*u.deg)\n",
    "                    clusters.add_index('ID_PHANGS_CLUSTERS')\n",
    " \n",
    "            \n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            ax = single_cutout(ax,\n",
    "                             position = row['SkyCoord_neb'],\n",
    "                             image = F275,\n",
    "                             mask1 = nebulae_mask,\n",
    "                             mask2 = associations_mask,\n",
    "                             points = clusters,\n",
    "                             label = f\"{row['gal_name']}: {row['region_ID']:.0f}\",\n",
    "                             size  = size)\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        #h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        #ax.legend(h[::len(h)-3],l[::(len(l)-3)],fontsize=7,loc='center',frameon=False)\n",
    "        h1 = mlines.Line2D([], [], color=tab10[0],label='HII region')\n",
    "        h2 = mlines.Line2D([], [], color=tab10[1],label='association')\n",
    "        h3 = mlines.Line2D([], [], lw=0,mfc='white',mec=tab10[4],marker='o',label='compact cluster')\n",
    "        ax.legend(handles=[h1,h2,h3],fontsize=7,loc='center',frameon=False)        \n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The HII region complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from cluster.io import read_associations\n",
    "from cluster.plot import single_cutout_complex\n",
    "\n",
    "tmp = complexes[complexes['gal_name']=='NGC2835']\n",
    "\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "size=16*u.arcsec\n",
    "nrows=4\n",
    "ncols=5\n",
    "filename = basedir/'reports'/'cutouts'/f'HII_region_complexes'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                \n",
    "                filename = data_ext / 'MUSE' / 'DR2.1' / 'MUSEDAP' / f'{gal_name}_MAPS.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                                    meta=hdul['HA6562_FLUX'].header,\n",
    "                                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "            \n",
    "                # nebulae mask\n",
    "                filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "                \n",
    "                # finally the compact clusters\n",
    "                filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "                if not filename.is_file():\n",
    "                    print(f'no compact clusters for {gal_name}')\n",
    "                    clusters = None\n",
    "                else:     \n",
    "                    clusters = Table.read(filename)\n",
    "                    clusters['SkyCoord'] = SkyCoord(clusters['PHANGS_RA']*u.deg,clusters['PHANGS_DEC']*u.deg)\n",
    "                    clusters.add_index('ID_PHANGS_CLUSTERS')\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            ax = single_cutout_complex(ax,\n",
    "                             position = row['SkyCoord'],\n",
    "                             image = F275,\n",
    "                             nebulae_mask = nebulae_mask,\n",
    "                             associations_mask = associations_mask,\n",
    "                             points = clusters,\n",
    "                             region_IDs = row['region_IDs'],\n",
    "                             label = f\"{row['gal_name']}: {row['complex_ID']:.0f}\",\n",
    "                             size  = size)\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        #h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        #ax.legend(h[::len(h)-3],l[::(len(l)-3)],fontsize=7,loc='center',frameon=False)\n",
    "        h1 = mlines.Line2D([], [], color=tab10[0],label='HII region')\n",
    "        h2 = mlines.Line2D([], [], color=tab10[1],label='association')\n",
    "        h3 = mlines.Line2D([], [], lw=0,mfc='white',mec=tab10[4],marker='o',label='compact cluster')\n",
    "        ax.legend(handles=[h1,h2,h3],fontsize=7,loc='center',frameon=False)        \n",
    "        t = ax.text(0.07,0.87,'name: complex ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### the nice plot for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations, ReadHST\n",
    "\n",
    "gal_name = 'NGC2835'\n",
    "\n",
    "filename = data_ext / 'MUSE' / 'DR2.1' / 'MUSEDAP' / f'{gal_name}_MAPS.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "# nebulae mask\n",
    "filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "hst_images = ReadHST(gal_name,data_ext / 'HST' / 'filterImages' )\n",
    "hst_images.Halpha = Halpha\n",
    "\n",
    "# association mask\n",
    "associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                      target=gal_name.lower(),\n",
    "                                      scalepc=scalepc,\n",
    "                                      data='mask')\n",
    "\n",
    "# finally the compact clusters\n",
    "filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'    \n",
    "clusters = Table.read(filename)\n",
    "clusters['SkyCoord'] = SkyCoord(clusters['PHANGS_RA']*u.deg,clusters['PHANGS_DEC']*u.deg)\n",
    "clusters.add_index('ID_PHANGS_CLUSTERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.plot.cutouts import multi_cutout_hst\n",
    "\n",
    "# also not bad: 318, 301\n",
    "# 27 an HII region with compact cluster but no association\n",
    "positions = nebulae[(nebulae['gal_name']=='NGC2835') & np.isin(nebulae['region_ID'],[47,53,102,375])]['SkyCoord_neb']\n",
    "\n",
    "zipped_pairs = zip([2,1,4,3,6], positions)\n",
    "positions = [x for _, x in sorted(zipped_pairs)]\n",
    "# [83, 723]\n",
    "positions += list(complexes[(complexes['gal_name']==gal_name) & np.isin(complexes['complex_ID'],[114])]['SkyCoord'])\n",
    "\n",
    "filename = basedir/'reports'/'ionising_sources.pdf'\n",
    "\n",
    "multi_cutout_hst(positions = positions,\n",
    "             images=hst_images,\n",
    "             nebulae_mask = nebulae_mask,\n",
    "             associations_mask = associations_mask,\n",
    "             points= clusters,\n",
    "             complexes=[[],[],[],[],[521, 114, 846]],\n",
    "             labels= list(map(str,range(1,len(positions)+1))),\n",
    "             scalebar=('100 pc',0.2109),\n",
    "             size = 8*u.arcsecond,\n",
    "             width = single_column,\n",
    "             filename=filename,\n",
    "             ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate the length of the scalebar\n",
    "cutout_size = 8*u.arcsec\n",
    "distance = 12.22*u.Mpc\n",
    "scalebar_length = 100 # in parsec\n",
    "scalebar_length / (distance*(cutout_size).to(u.rad).value).to(u.pc).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Synthesis models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_bpass\n",
    "\n",
    "#GENEVASTD = Cluster(stellar_model='GENEVASTD',metallicity=0.014)\n",
    "#GENEVAHIGH = Cluster(stellar_model='GENEVAHIGH',metallicity=0.014)\n",
    "#PADOVASTD = Cluster(stellar_model='PADOVASTD',metallicity=0.014)\n",
    "#PADOVAAGB = Cluster(stellar_model='PADOVAAGB',metallicity=0.014)\n",
    "GENEVAv00 = Cluster(stellar_model='GENEVAv00',metallicity=0.014)\n",
    "GENEVAv40 = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "\n",
    "bc03 = Table.read(basedir/'data'/'external'/'bc03_q.fits')\n",
    "bpass = read_bpass(metallicity='z020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENEVAv00_04  = Cluster(stellar_model='GENEVAv00',metallicity=0.04)\n",
    "GENEVAv00_014 = Cluster(stellar_model='GENEVAv00',metallicity=0.014)\n",
    "GENEVAv00_008 = Cluster(stellar_model='GENEVAv00',metallicity=0.008)\n",
    "GENEVAv00_002 = Cluster(stellar_model='GENEVAv00',metallicity=0.002)\n",
    "GENEVAv00_001 = Cluster(stellar_model='GENEVAv00',metallicity=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(figsize=(single_column,single_column*2/1.618),nrows=2,sharex=True)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('cividis_r',5)\n",
    "bins = np.array([0,0.0015,0.005,0.011,0.032,0.048])\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=bins,ncolors=5)\n",
    "\n",
    "#ax1.plot(GENEVASTD.quanta['Time'].value/1e6,GENEVASTD.quanta['HI_rate'].value,label='GENEVASTD')\n",
    "#ax1.plot(GENEVAHIGH.quanta['Time'].value/1e6,GENEVAHIGH.quanta['HI_rate'].value,label='GENEVAHIGH')\n",
    "#ax1.plot(PADOVASTD.quanta['Time'].value/1e6,PADOVASTD.quanta['HI_rate'].value,label='PADOVASTD')\n",
    "#ax1.plot(PADOVAAGB.quanta['Time'].value/1e6,PADOVAAGB.quanta['HI_rate'].value,label='PADOVAAGB')#ax.plot(GENEVAv00.quanta['Time'].value/1e6,GENEVAv00.quanta['HI_rate'].value,label='GENEVAv00')\n",
    "ax1.plot(GENEVAv00.quanta['Time'].value/1e6,GENEVAv00.quanta['HI_rate'].value,label='GENEVAv00',ls='-',color=cmap(norm((0.014))))\n",
    "ax1.plot(GENEVAv40.quanta['Time'].value/1e6,GENEVAv40.quanta['HI_rate'].value,label='GENEVAv40',ls='--',color=cmap(norm((0.014))))\n",
    "ax1.plot(bpass['age'].value/1e6,bpass['Q'].value,label='BPASS',ls=':',color=cmap(norm((0.014))))\n",
    "ax1.plot(bc03['sfh.age'],bc03['stellar.n_ly_young']*1e6,label='CIGALE',ls='-.',color=cmap(norm((0.014))))\n",
    "\n",
    "ax1.legend(handlelength=2)\n",
    "ax1.set(xlim=[0.5,10],xscale='linear',\n",
    "       ylim=[1e50,5e53],yscale='log',ylabel='$Q(\\mathrm{H}^0)$ / s$^{-1}$')\n",
    " \n",
    "ax2.plot(GENEVAv00_04.quanta['Time'].value/1e6,GENEVAv00_04.quanta['HI_rate'].value,color=cmap(norm((0.04))))\n",
    "ax2.plot(GENEVAv00_014.quanta['Time'].value/1e6,GENEVAv00_014.quanta['HI_rate'].value,color=cmap(norm((0.014))))\n",
    "ax2.plot(GENEVAv00_008.quanta['Time'].value/1e6,GENEVAv00_008.quanta['HI_rate'].value,color=cmap(norm((0.008))))\n",
    "ax2.plot(GENEVAv00_002.quanta['Time'].value/1e6,GENEVAv00_002.quanta['HI_rate'].value,color=cmap(norm((0.002))))\n",
    "ax2.plot(GENEVAv00_001.quanta['Time'].value/1e6,GENEVAv00_001.quanta['HI_rate'].value,color=cmap(norm((0.001))))\n",
    "\n",
    "\n",
    "ax2.set(xlim=[0.5,10.5],xscale='linear',ylim=[1e50,5e53],yscale='log',\n",
    "        ylabel='$Q(\\mathrm{H}^0)$ / s$^{-1}$',xlabel=r'age / Myr')\n",
    "    \n",
    "plt.subplots_adjust(hspace=0.05)\n",
    "    \n",
    "fig.subplots_adjust(top=0.85)\n",
    "cbar_ax = fig.add_axes([0.13, 0.88, 0.76, 0.02])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),spacing='uniform',\n",
    "                    cax=cbar_ax,orientation='horizontal',ticklocation='top',label=r'$Z$',\n",
    "                    ticks=(bins[1:]+bins[:-1])/2)\n",
    "cbar.ax.set_xticklabels(np.array([0.001,0.002,0.008,0.014,0.04]))    \n",
    "\n",
    "#plt.savefig(basedir/'reports'/'thesis'/'impact_of_models.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(figsize=(two_column,two_column/2),ncols=2,sharey=True)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('cividis_r',5)\n",
    "bins = np.array([0,0.0015,0.005,0.011,0.032,0.048])\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=bins,ncolors=5)\n",
    "\n",
    "#ax1.plot(GENEVASTD.quanta['Time'].value/1e6,GENEVASTD.quanta['HI_rate'].value,label='GENEVASTD')\n",
    "#ax1.plot(GENEVAHIGH.quanta['Time'].value/1e6,GENEVAHIGH.quanta['HI_rate'].value,label='GENEVAHIGH')\n",
    "#ax1.plot(PADOVASTD.quanta['Time'].value/1e6,PADOVASTD.quanta['HI_rate'].value,label='PADOVASTD')\n",
    "#ax1.plot(PADOVAAGB.quanta['Time'].value/1e6,PADOVAAGB.quanta['HI_rate'].value,label='PADOVAAGB')#ax.plot(GENEVAv00.quanta['Time'].value/1e6,GENEVAv00.quanta['HI_rate'].value,label='GENEVAv00')\n",
    "ax1.plot(GENEVAv00.quanta['Time'].value/1e6,GENEVAv00.quanta['HI_rate'].value,label='SB99 v00',ls='-',color=cmap(norm((0.014))))\n",
    "ax1.plot(GENEVAv40.quanta['Time'].value/1e6,GENEVAv40.quanta['HI_rate'].value,label='SB99 v40',ls='--',color=cmap(norm((0.014))))\n",
    "ax1.plot(bpass['age'].value/1e6,bpass['Q'].value,label='BPASS',ls=':',color=cmap(norm((0.014))))\n",
    "ax1.plot(bc03['sfh.age'],bc03['stellar.n_ly_young']*1e6,label='CIGALE',ls='-.',color=cmap(norm((0.014))))\n",
    "\n",
    "ax1.legend(handlelength=2)\n",
    "ax1.set(xlim=[0.5,10],xscale='linear',xlabel=r'age / Myr',\n",
    "       ylim=[1e50,5e53],yscale='log',ylabel='$Q(\\mathrm{H}^0)$ / s$^{-1}$')\n",
    " \n",
    "ax2.plot(GENEVAv00_04.quanta['Time'].value/1e6,GENEVAv00_04.quanta['HI_rate'].value,color=cmap(norm((0.04))))\n",
    "ax2.plot(GENEVAv00_014.quanta['Time'].value/1e6,GENEVAv00_014.quanta['HI_rate'].value,color=cmap(norm((0.014))))\n",
    "ax2.plot(GENEVAv00_008.quanta['Time'].value/1e6,GENEVAv00_008.quanta['HI_rate'].value,color=cmap(norm((0.008))))\n",
    "ax2.plot(GENEVAv00_002.quanta['Time'].value/1e6,GENEVAv00_002.quanta['HI_rate'].value,color=cmap(norm((0.002))))\n",
    "ax2.plot(GENEVAv00_001.quanta['Time'].value/1e6,GENEVAv00_001.quanta['HI_rate'].value,color=cmap(norm((0.001))))\n",
    "\n",
    "\n",
    "ax2.set(xlim=[0.5,10.5],xscale='linear',ylim=[1e50,5e53],yscale='log',xlabel=r'age / Myr')\n",
    "    \n",
    "fig.subplots_adjust(wspace=0.05,right=0.9)\n",
    "cbar_ax = fig.add_axes([0.92, 0.13, 0.02, 0.75])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),spacing='uniform',\n",
    "                    cax=cbar_ax,orientation='vertical',label=r'$Z$',\n",
    "                    ticks=(bins[1:]+bins[:-1])/2)\n",
    "cbar.ax.set_yticklabels(np.array([0.001,0.002,0.008,0.014,0.04]))    \n",
    "\n",
    "plt.savefig(basedir/'reports'/'slides'/'impact_of_models.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPASS\n",
    "\n",
    "https://flexiblelearning.auckland.ac.nz/bpass/8/files/bpassv2_1_manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_bpass\n",
    "\n",
    "BPASS_folder = basedir/'..'/'BPASS'\n",
    "\n",
    "bpass = read_bpass(BPASS_folder,metallicity='z020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(bpass['age'],bpass['Halpha']/bpass['FUV'])\n",
    "ax.set(xlim=[1e6,1e7],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "HI_rate = bpass['Q'].value\n",
    "time = bpass['age']\n",
    "for row in tqdm(catalogue):\n",
    "    idx = np.argmin(np.abs(time-row['age']*u.Myr))\n",
    "    row['Qpredicted'] = HI_rate[idx] * row['mass'] / 1e6\n",
    "    \n",
    "catalogue['distance'] = np.nan\n",
    "for gal_name in catalogue['gal_name']:\n",
    "    distance = Distance(distmod=sample_table.loc[gal_name]['(m-M)'])\n",
    "    catalogue['distance'][catalogue['gal_name']==gal_name] = distance\n",
    "    \n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(catalogue['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for objects with negative fesc we redo the analysis with age-age_err\n",
    "Qpredict_new = []\n",
    "for row in tqdm(catalogue):\n",
    "    if row['fesc']<0:\n",
    "        idx = np.argmin(np.abs(time-(row['age']-3*row['age_err'])*u.Myr))\n",
    "        row['Qpredicted'] = ( HI_rate[idx] * row['mass'] / 1e6 )\n",
    "\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= fesc>0\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "print(f'fesc={np.nanmean(tmp[tmp[\"fesc\"]>0][\"fesc\"]):.2f} (from {np.sum(criteria)} objects)')\n",
    "print(f\"{np.sum(tmp['fesc']<0)} of {len(tmp)} ({np.sum(tmp['fesc']<0)/len(tmp)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLUG\n",
    "\n",
    "The SLUG output contains the full PDF of each property. Here we compute the median and uncertainty and save it a new and smaller file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the raw association catalogue that Jia Wei used in SLUG\n",
    "#with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "#    associations = Table(hdul[1].data)\n",
    "    \n",
    "mtab = np.genfromtxt(basedir/'data'/'external'/'SLUG'/'mtab_PHANGS_FS.tab', skip_header = 1)\n",
    "atab = np.genfromtxt(basedir/'data'/'external'/'SLUG'/'agetab_PHANGS_FS.tab', skip_header = 1)\n",
    "qtab = np.genfromtxt(basedir/'data'/'external'/'SLUG'/'qtab_PHANGS_FS.tab', skip_header = 1)\n",
    "avtab = np.genfromtxt(basedir/'data'/'external'/'SLUG'/'avtab_PHANGS_FS.tab', skip_header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum = np.cumsum(mtab[1:],axis=1)/np.sum(mtab[1:],axis=1)[:,np.newaxis]\n",
    "m_min = 10**mtab[0][np.argmin(np.abs(cum-0.16),axis=1)]\n",
    "m_med = 10**mtab[0][np.argmin(np.abs(cum-0.5),axis=1)]\n",
    "m_max = 10**mtab[0][np.argmin(np.abs(cum-0.84),axis=1)]\n",
    "\n",
    "cum = np.cumsum(atab[1:],axis=1)/np.sum(atab[1:],axis=1)[:,np.newaxis]\n",
    "a_min = 10**atab[0][np.argmin(np.abs(cum-0.16),axis=1)] / 1e6\n",
    "a_med = 10**atab[0][np.argmin(np.abs(cum-0.5),axis=1)] / 1e6\n",
    "a_max = 10**atab[0][np.argmin(np.abs(cum-0.84),axis=1)] / 1e6\n",
    "\n",
    "cum = np.cumsum(avtab[1:],axis=1)/np.sum(avtab[1:],axis=1)[:,np.newaxis]\n",
    "av_min = 10**avtab[0][np.argmin(np.abs(cum-0.16),axis=1)]\n",
    "av_med = 10**avtab[0][np.argmin(np.abs(cum-0.5),axis=1)]\n",
    "av_max = 10**avtab[0][np.argmin(np.abs(cum-0.84),axis=1)]\n",
    "\n",
    "cum = np.cumsum(qtab[1:],axis=1)/np.sum(qtab[1:],axis=1)[:,np.newaxis]\n",
    "q_min = 10**qtab[0][np.argmin(np.abs(cum-0.16),axis=1)]\n",
    "q_med = 10**qtab[0][np.argmin(np.abs(cum-0.5),axis=1)]\n",
    "q_max = 10**qtab[0][np.argmin(np.abs(cum-0.84),axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "    SLUG_table= Table(hdul[1].data)['gal_name','assoc_ID']\n",
    "\n",
    "SLUG_table['mass_SLUG'] = m_med\n",
    "SLUG_table['mass_SLUG_err_plus'] = m_max - m_med\n",
    "SLUG_table['mass_SLUG_err_minus'] = m_med - m_min\n",
    "\n",
    "SLUG_table['age_SLUG'] = a_med\n",
    "SLUG_table['age_SLUG_err_plus'] = a_max - a_med\n",
    "SLUG_table['age_SLUG_err_minus'] = a_med - a_min\n",
    "\n",
    "SLUG_table['av_SLUG'] = av_med\n",
    "SLUG_table['av_SLUG_err_plus'] = av_max - av_med\n",
    "SLUG_table['av_SLUG_err_minus'] = av_med - av_min\n",
    "\n",
    "SLUG_table['q_SLUG'] = q_med\n",
    "SLUG_table['q_SLUG_err_plus'] = q_max - q_med\n",
    "SLUG_table['q_SLUG_err_minus'] = q_med - q_min\n",
    "\n",
    "mask = []\n",
    "for pdf in qtab[1:]:\n",
    "    if np.all(np.ediff1d(pdf)==0):\n",
    "        mask.append(True)\n",
    "    else:\n",
    "        mask.append(False)\n",
    "for col in SLUG_table.columns[2:]:\n",
    "    SLUG_table[col][mask] = np.nan\n",
    "\n",
    "SLUG_table.write(basedir/'data'/'interim'/'associations_SLUG.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.scatter(SLUG_table['age_SLUG'],SLUG_table['q_SLUG']/SLUG_table['mass_SLUG']*1e6,label='SLUG')\n",
    "\n",
    "mean, edges, _ = binned_statistic(SLUG_table['age_SLUG'],SLUG_table['q_SLUG']/SLUG_table['mass_SLUG']*1e6,statistic='median',bins=np.linspace(0.5,20.5,40))\n",
    "x = (edges[1:]+edges[:-1])/2\n",
    "ax.plot(x,mean,color='gray',label='SLUG median')\n",
    "\n",
    "ax.plot(GENEVAv00.quanta['Time'].value/1e6,GENEVAv00.quanta['HI_rate'].value,label='GENEVAv00',ls='-',color='black')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,20],ylim=[1e47,1e54],yscale='log',xlabel='age / Myr',ylabel='q / s-1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different stellar models/population synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_z014 = read_bpass(metallicity='z014')\n",
    "bpass_z008 = read_bpass(metallicity='z008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(nrows=3,figsize=(two_column,two_column),sharex=True)\n",
    "\n",
    "ax1.plot(bpass['age']/1e6,bpass['Halpha'],label='BPASSv014')\n",
    "#ax1.plot(bpass_z008['age']/1e6,bpass_z008['Halpha'],label='BPASSv008')\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A'],label='GENEVAv40')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],ylabel=r'$\\mathrm{H}\\alpha \\,/\\, \\mathrm{erg}\\ \\mathrm{s}^{-1}$',\n",
    "        yscale='log',ylim=[1e39,2e41])\n",
    "\n",
    "ax2.plot(bpass['age']/1e6,bpass['FUV'],label='BPASSv014')\n",
    "#ax2.plot(bpass_z008['age']/1e6,bpass_z008['FUV'],label='BPASSv008')\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.FUV['FUV'],label='GENEVAv40')\n",
    "#ax2.legend()\n",
    "ax2.set(xlim=[0,10],ylabel=r'$\\mathrm{FUV}\\,/\\, \\mathrm{erg}\\ \\mathrm{s}^{-1}\\ \\mathrm{\\AA}^{-1}$',\n",
    "       yscale='log',ylim=[8e37,3e39])\n",
    "\n",
    "ax3.plot(bpass['age']/1e6,bpass['Halpha']/bpass['FUV'],label='BPASSv014')\n",
    "#ax3.plot(bpass_z008['age']/1e6,bpass_z008['Halpha']/bpass_z008['FUV'],label='BPASSv008')\n",
    "ax3.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],label='GENEVAv40')\n",
    "#ax3.legend()\n",
    "ax3.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'$\\mathrm{H}\\alpha/\\mathrm{FUV}$')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dust from JWST\n",
    "\n",
    "how can we disentangle the photons that are lost due to dust and those that escape the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jwst_sample = ['IC5332','NGC0628','NGC1365','NGC7496']\n",
    "\n",
    "jwst_catalogue = catalogue[np.isin(catalogue['gal_name'],jwst_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for gal_name in jwst_sample:\n",
    "    tbl = Table.read(basedir/'data'/'external'/'MIRI'/f'{gal_name}_MIRI.fits')\n",
    "    lst.append(tbl[['gal_name','region_ID']+[x for x in tbl.columns if x.startswith('F')]])\n",
    "MIRI = vstack(lst)\n",
    "jwst_catalogue = join(jwst_catalogue,MIRI,keys=['gal_name','region_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = jwst_catalogue\n",
    "\n",
    "vmin,vmax = -4.5,-2.5\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "cmap = plt.cm.get_cmap('cool')\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "ax.scatter(tmp['fesc'],tmp['dig/hii'],c=np.log10(tmp['F2100W']),cmap=cmap,vmin=vmin,vmax=vmax,s=8,rasterized=True)\n",
    "\n",
    "ax.set(xlim=[0.4,1],ylim=[0,1],xlabel=r'$f_\\mathrm{esc}$',ylabel=r'$I_\\mathrm{DIG}\\,/\\,I_{\\mathrm{H}\\,\\tiny{\\textsc{ii}}}$')\n",
    "fig.subplots_adjust(right=0.95,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.96, 0.126, 0.02, 0.75])\n",
    "#r'$21\\,\\mu\\mathrm{m}$' r'$E(B-V)_\\mathrm{Balmer}$'\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,label=r'$21\\,\\mu\\mathrm{m}$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'fesc_dust.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat\n",
    "\n",
    "jwst_catalogue['dust_Ha'] = np.log10(jwst_catalogue['F2100W']/jwst_catalogue['HA6562_FLUX_CORR'])\n",
    "\n",
    "tmp = jwst_catalogue #[jwst_catalogue['mass']>5e3]\n",
    "\n",
    "vmin,vmax = -10,-7\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "cmap = plt.cm.get_cmap('cool')\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "bins = np.linspace(0.5,1.05,5)\n",
    "\n",
    "ax.scatter(tmp['fesc'],tmp['dig/hii'],c=tmp['dust_Ha'],cmap=cmap,vmin=vmin,vmax=vmax,s=8,rasterized=True)\n",
    "#corner_binned_stat(tmp['fesc'],tmp['dig/hii'],ax=ax,bins=bins)\n",
    "\n",
    "sub = tmp[(tmp['dust_Ha']<-9)]\n",
    "corner_binned_stat(sub['fesc'],sub['dig/hii'],ax=ax,bins=bins,color=cmap(norm(-9.5)),lw=4)\n",
    "\n",
    "sub = tmp[(tmp['dust_Ha']>=-9) & (tmp['dust_Ha']<-8)]\n",
    "corner_binned_stat(sub['fesc'],sub['dig/hii'],ax=ax,bins=bins,color=cmap(norm(-8.5)),lw=4)\n",
    "\n",
    "sub = tmp[(tmp['dust_Ha']>=-8)]\n",
    "corner_binned_stat(sub['fesc'],sub['dig/hii'],ax=ax,bins=bins,color=cmap(norm(-7.5)),lw=4)\n",
    "\n",
    "ax.set(xlim=[0.4,1],ylim=[0,1],xlabel=r'$f_\\mathrm{esc}$',ylabel=r'$I_\\mathrm{DIG}\\,/\\,I_{\\mathrm{H}\\,\\tiny{\\textsc{ii}}}$')\n",
    "fig.subplots_adjust(right=0.95,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.96, 0.126, 0.02, 0.75])\n",
    "#r'$21\\,\\mu\\mathrm{m}$' r'$E(B-V)_\\mathrm{Balmer}$'\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,label=r'$21\\,\\mu\\mathrm{m}\\,/\\,\\mathrm{H}\\alpha$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'fesc_dust.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## are the age trends due to fesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2.2))\n",
    "\n",
    "sc=ax1.scatter(tmp['HA/FUV'],tmp['logq_D91'],c=tmp['fesc'],vmin=0,vmax=1)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['logq_D91'],[0,80],nbins=5,statistic='median')\n",
    "ax1.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax1.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\log q$',xlim=[0,80],ylim=[6,8])\n",
    "\n",
    "ax2.scatter(tmp['HA/FUV'],tmp['Delta_met_scal'],c=tmp['fesc'],vmin=0,vmax=1)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['Delta_met_scal'],[0,80],nbins=5,statistic='median')\n",
    "ax2.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax2.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\Delta$(O/H)',xlim=[0,80],ylim=[-0.1,0.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.87, 0.145, 0.02, 0.8])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2.2))\n",
    "\n",
    "sc=ax1.scatter(tmp['HA/FUV'],tmp['logq_D91'],c=tmp['age'],vmin=0,vmax=5)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['logq_D91'],[0,80],nbins=5,statistic='median')\n",
    "ax1.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax1.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\log q$',xlim=[0,80],ylim=[6,8])\n",
    "\n",
    "ax2.scatter(tmp['HA/FUV'],tmp['Delta_met_scal'],c=tmp['age'],vmin=0,vmax=5)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['Delta_met_scal'],[0,80],nbins=5,statistic='median')\n",
    "ax2.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax2.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\Delta$(O/H)',xlim=[0,80],ylim=[-0.1,0.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.87, 0.145, 0.02, 0.8])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/20105364/how-can-i-make-a-scatter-plot-colored-by-density-in-matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "x,y = tmp['HA/FUV'],tmp['fesc']\n",
    "\n",
    "x,y = x[~np.isnan(x) & ~np.isnan(y)], y[~np.isnan(x) & ~np.isnan(y)]\n",
    "xy = np.vstack([x,y])\n",
    "density = gaussian_kde(xy)(xy)\n",
    "\n",
    "ax.scatter(x,y,cmap=plt.cm.Reds)\n",
    "bins,mean,std = bin_stat(tmp['HA/FUV'],tmp['fesc'],[0,80],nbins=8,statistic='median')\n",
    "ax.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax.set(xlim=[0,80],ylim=[0,1.2],\n",
    "       xlabel=r'H$\\alpha$ / FUV',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(tmp['age'],tmp['fesc'])\n",
    "bins,mean,std = bin_stat(tmp['age'],tmp['fesc'],[0.5,10.5],nbins=10,statistic='median')\n",
    "ax1.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax1.set(xlim=[0,10],ylim=[0,1],\n",
    "       xlabel=r'age / Myr',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "x,y = tmp['HA/FUV'],tmp['fesc']\n",
    "ax2.scatter(x,y,cmap=plt.cm.Reds)\n",
    "bins,mean,std = bin_stat(tmp['HA/FUV'],tmp['fesc'],[0,80],nbins=8,statistic='median')\n",
    "ax2.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax2.set(xlim=[0,80],ylim=[0,1],\n",
    "       xlabel=r'H$\\alpha$ / FUV',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/'trends_due_to_fesc.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ha/FUV vs ionization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with log q from Diaz+91\n",
    "\n",
    "$$\n",
    "\\log u = (-1.6840.076)\\cdot \\log([SII]/[SIII])-(2.986 0.027)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, spearmanr\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "bins = np.linspace(*np.nanpercentile(catalogue['logq_D91'],[1,99]),10)\n",
    "xlim = [5.7,7.8]\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = 'logq_D91','eq_width','HA/FUV_corr','Delta_met_scal'\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "table = nebulae.copy()\n",
    "table = table[(table['HA/FUV_corr']>3*table['HA/FUV_corr_err']) | np.isnan(table['FUV_FLUX'])]\n",
    "#table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "print(f'{len(table)} objects in sample')\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "sample_table.sort('mass')\n",
    "\n",
    "rho1,rho2,rho3 = [], [], []\n",
    "for i,gal_name in enumerate(sample_table['name']):\n",
    "    #print(gal_name)\n",
    "    \n",
    "    tmp = table[table['gal_name']==gal_name]\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "    rho1.append(spearmanr(tmp[x_name],tmp[y1_name],nan_policy='omit')[0])\n",
    "    \n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "    try:\n",
    "        rho2.append(spearmanr(tmp[x_name],tmp[y2_name],nan_policy='omit')[0])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "    rho3.append(spearmanr(tmp[x_name],tmp[y3_name],nan_policy='omit')[0])\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "    \n",
    "\n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "\n",
    "ax1.set(xlim=xlim, yscale='log',ylim=[3,5e2],\n",
    "        xlabel=r'$\\log q$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "ax2.set(xlim=xlim, yscale='log',ylim=[2,2e2],\n",
    "        xlabel=r'$\\log q$',ylabel=r'H$\\alpha$/FUV')\n",
    "ax3.set(xlim=xlim,ylim=[-0.15,0.15],\n",
    "        xlabel=r'$\\log q$',ylabel=r'$\\Delta$(O/H)')\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax3.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "\n",
    "# get a second x axis on top with [SIII]/[SII]\n",
    "xmin,xmax=ax1.get_xlim()\n",
    "ax1_top = ax1.twiny()\n",
    "ax1_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax1_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax1_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "ax2_top = ax3.twiny()\n",
    "ax2_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax2_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax2_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "ax3_top = ax2.twiny()\n",
    "ax3_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax3_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax3_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "for i, rho in enumerate([rho1,rho2,rho3]):\n",
    "    print(f'plot{i}: rho={np.nanmean(rho):.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.6])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$\\log (m/\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "plt.savefig(basedir/'reports'/'nebulae_correlations.pdf',dpi=600)\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram/binned stat with log bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = '[SIII]/[SII]', 'Delta_met_scal','HA/FUV_corr','eq_width'\n",
    "xlim,ylim1,ylim2,ylim3 = [1e-2,1],[-0.15,0.15],[8e-1,8e1],[3,3e2]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3.2))\n",
    "\n",
    "table = nebulae.copy()\n",
    "table = table[(table['HA/FUV_corr']>3*table['HA/FUV_corr_err']) | np.isnan(table['FUV_FLUX'])]\n",
    "table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "sample_table.sort('mass')\n",
    "\n",
    "# the density histogram\n",
    "\n",
    "nbins=20\n",
    "\n",
    "x,y = table[x_name],table[y1_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim1,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax1.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y2_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim2),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax2.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y3_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim3),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax3.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "# the bins for the binned mean\n",
    "#bins = np.logspace(-1.9,-0.2,10)\n",
    "bins = np.logspace(-2.4,-0.4,12)\n",
    "\n",
    "for i,gal_name in enumerate(sample_table['name']):\n",
    "    #print(gal_name)\n",
    "    \n",
    "    tmp = table[table['gal_name']==gal_name]\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "    \n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "\n",
    "ax1.set(xscale='log',xlim=xlim,ylim=ylim1,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "ax2.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim2,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "ax3.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim3,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax1.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.73])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$\\log (m/\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same plot but now binned by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = '[SIII]/[SII]', 'Delta_met_scal','HA/FUV','eq_width'\n",
    "xlim,ylim1,ylim2,ylim3 = [1e-2,1],[-0.15,0.15],[8e-1,8e1],[3,3e2]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3.2))\n",
    "\n",
    "table = catalogue.copy()\n",
    "#table = table[(table['FUV_FLUX_CORR']>3*table['FUV_FLUX_CORR_ERR']) | np.isnan(table['FUV_FLUX_CORR'])]\n",
    "table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=0,vmax=20)\n",
    "\n",
    "\n",
    "nbins=20\n",
    "\n",
    "x,y = table[x_name],table[y1_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim1,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax1.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y2_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim2),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax2.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y3_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim3),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax3.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "# the bins for the binned mean\n",
    "#bins = np.logspace(-1.9,-0.2,10)\n",
    "bins = np.logspace(-2.4,-0.4,12)\n",
    "\n",
    "\n",
    "tmp = catalogue #[catalogue['mass']>1e4]\n",
    "\n",
    "for age in [(0,2),(2,5),(5,10),(10,20)]:\n",
    "    age_mean = np.mean(age)\n",
    "    tmp = table[(table['age']>age[0]) & (table['age']<age[1])]\n",
    "    print(f'{age_mean}: {len(tmp)} objects')\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "    \n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    #ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=0,vmax=12)\n",
    "\n",
    "ax1.set(xscale='log',xlim=xlim,ylim=ylim1,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "ax2.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim2,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "ax3.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim3,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax1.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.73])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'age / Myr',ticks=np.arange(0,20,4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations_age_bin.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a dedicated subplot for each galaxie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, pearsonr, spearmanr\n",
    "\n",
    "sample = set(astrosat_sample) & set(muse_sample)\n",
    "filename = basedir/'reports'/'all_objects_HaFUV_over_SII'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "    \n",
    "#vmin,vmax = np.min(catalogue['HA6562_FLUX_CORR']),np.max(catalogue['HA6562_FLUX_CORR'])\n",
    "vmin,vmax = 1e-16,1e-14\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample):  \n",
    "    \n",
    "    tmp = nebulae[(nebulae['gal_name']==name)]\n",
    "        \n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    tmp = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err']) | np.isnan(tmp['FUV_FLUX'])]\n",
    "    tmp = tmp[tmp['[SIII]/[SII]']>3*tmp['[SIII]/[SII]_err']]\n",
    "\n",
    "    r,p = spearmanr(tmp['[SIII]/[SII]'],tmp['HA/FUV_corr'])\n",
    "    print(f'{name}: rho={r:.2f}, {len(tmp)} objects')\n",
    "\n",
    "    sc = ax.scatter(tmp['[SIII]/[SII]'],tmp['HA/FUV_corr'],\n",
    "               c=1e-20*tmp['HA6562_FLUX_CORR'],vmin=vmin,vmax=vmax,\n",
    "               cmap=plt.cm.plasma,\n",
    "               norm=mpl.colors.LogNorm(),\n",
    "               s=1,marker='.')\n",
    "    \n",
    "    #q = 98\n",
    "    #bins = np.logspace(*np.log10(np.percentile(tmp['[SIII]/[SII]'],[100-q,q])),10)\n",
    "    #x,mean,std = bin_stat(tmp['[SIII]/[SII]'],tmp['HA/FUV'],[None,None],nbins=bins)\n",
    "    #ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label=gal_name)\n",
    "    \n",
    "    ax.text(0.05,0.9,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.75,0.15,r'$\\rho$'+f'={r:.2f}',transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.62,0.05,f'{len(tmp):.0f} objects', transform=ax.transAxes,fontsize=7)\n",
    "    \n",
    "    ax.set(xscale='log',yscale='log',xlim=[1e-2,1],ylim=[2,4e2])\n",
    "    # https://stackoverflow.com/questions/21920233/matplotlib-log-scale-tick-label-number-formatting/33213196\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(r'H$\\alpha$ / FUV')\n",
    "\n",
    "        \n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    \n",
    "    if i==0:\n",
    "        #ax.remove()\n",
    "        ax.axis('off')\n",
    "        cbar = fig.colorbar(sc, ax=ax,\n",
    "                            label=r'$\\mathrm{H}\\alpha$ / (erg s$^{-1}$ cm$^{-2}$ Hz$^{-1}$)',\n",
    "                            orientation='horizontal',\n",
    "                           )\n",
    "    else:\n",
    "        ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "\n",
    "\n",
    "#plt.savefig(filename.with_suffix('.png'),dpi=600)\n",
    "#plt.savefig(filename.with_suffix('.pdf'),dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nebulae[(nebulae['gal_name']=='NGC4254')]\n",
    "#tmp = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err']) | np.isnan(tmp['FUV_FLUX'])]\n",
    "tmp = tmp[tmp['[SIII]/[SII]']>3*tmp['[SIII]/[SII]_err']]\n",
    "\n",
    "tmp[['HA/FUV_corr','HA/FUV_corr_err']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0.5,10.5,1)\n",
    "\n",
    "tmp = associations[(associations['mass']>1e4) & (associations['age']<=10)]\n",
    "print(len(tmp))\n",
    "ages_con = tmp[tmp['overlap']=='contained']['age']\n",
    "ages_par = tmp[tmp['overlap']=='partial']['age']\n",
    "ages_iso = tmp[(tmp['overlap']=='isolated') & (tmp['in_frame'])]['age']\n",
    "\n",
    "print(f'ages: con={np.mean(ages_con):.2f}, par={np.mean(ages_par):.2f}, iso={np.mean(ages_iso):.2f}')\n",
    "\n",
    "n1,_,_=ax1.hist(ages_con,bins=bins,histtype='step',label='contained')\n",
    "n2,_,_=ax2.hist(ages_par,bins=bins,histtype='step',label='partially')\n",
    "n3,_,_=ax3.hist(ages_iso,bins=bins,histtype='step',label='isolated')\n",
    "\n",
    "ax1.set_title(f'contained ({np.nanmean(ages_con):.2f} Myr)')\n",
    "ax2.set_title(f'partial ({np.nanmean(ages_par):.2f} Myr)')\n",
    "ax3.set_title(f'isolated ({np.nanmean(ages_iso):.2f} Myr)')\n",
    "\n",
    "ymax = np.max([n1,n2,n3])\n",
    "ymax = np.round(ymax,-int(np.log10(ymax))+1)\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,ymax],xlim=[0,10],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'tmp_age_hist_contained.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "tmp = associations[(associations['mass']>1e4)]\n",
    "idx,sep,_=match_coordinates_sky(tmp['SkyCoord_asc'],nebulae['SkyCoord_neb'])\n",
    "\n",
    "ages1 = tmp[(sep<0.4*u.arcsec)]['age']\n",
    "ages2 = tmp[(sep>0.4*u.arcsec) & (sep<0.8*u.arcsec)]['age']\n",
    "ages3 = tmp[(sep>0.8*u.arcsec)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'$s<0.4\"$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'$0.4\"<s<0.8\"$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'$0.8\"<s$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,1100],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_sep.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['age']<10)]\n",
    "\n",
    "p1,p2=np.nanpercentile(tmp['eq_width'],[33,66])\n",
    "\n",
    "ages1 = tmp[tmp['eq_width']<p1]['age']\n",
    "ages2 = tmp[(tmp['eq_width']>p1) & (tmp['eq_width']<p2)]['age']\n",
    "ages3 = tmp[(tmp['eq_width']>p2)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'1$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'2$^\\mathrm{nd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'3$^\\mathrm{rd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,160],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_eq_width.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue[(catalogue['mass']>1e4)]\n",
    "#catalogue[(catalogue['mass']>1e4) & (catalogue['age']<10)]\n",
    "\n",
    "p1,p2=np.nanpercentile(tmp['eq_width'],[33,66])\n",
    "\n",
    "ages1 = tmp[tmp['eq_width']<p1]['age']\n",
    "ages2 = tmp[(tmp['eq_width']>p1) & (tmp['eq_width']<p2)]['age']\n",
    "ages3 = tmp[(tmp['eq_width']>p2)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'1$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'2$^\\mathrm{nd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'3$^\\mathrm{rd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,150],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_eq_width.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass-to-Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from scipy.stats import spearmanr, gaussian_kde, binned_statistic, binned_statistic_2d\n",
    "\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.2))\n",
    "\n",
    "x,y,z = tmp['mass'],tmp['HA6562_LUM_CORR'],tmp['age']\n",
    "\n",
    "\n",
    "xlim =[1e2,1e5]\n",
    "ylim = [5e35,8e39]\n",
    "nbins = 25\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim),nbins)]\n",
    "\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins)\n",
    "stat, x_e, y_e,_ = binned_statistic_2d(x,y,z,bins=bins)\n",
    "#stat[hist<5] = np.nan\n",
    "img = ax.pcolormesh(x_e,y_e,stat.T,cmap=plt.cm.viridis,vmin=0,vmax=10)\n",
    "\n",
    "#sc = ax.scatter(x,y,c=z,vmin=0,vmax=10)\n",
    "x_stat,mean,std = bin_stat(x,y,xlim,nbins=bins[0],statistic='median')\n",
    "ax.errorbar(x_stat,mean,fmt='o-',ms=1.5,color='white')\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax.text(0.05,0.93,label,transform=ax.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "fig.colorbar(img,label='age / Myr')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'tmp_mass_HA_age.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax1=plt.subplots(nrows=1,figsize=(single_column,single_column))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<=8) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "vmin,vmax = 0,12\n",
    "nbins = 8\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',6)\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "rho = []\n",
    "age_bins = [0,2,4,6,8,10,12]\n",
    "for i in range(len(age_bins)-1):\n",
    "\n",
    "    sub = tmp[(tmp['age']>=age_bins[i]) & (tmp['age']<age_bins[i+1])]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    x,mean,std = bin_stat(x,y,xlim,nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_bins[i]+1)))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=19*[1],cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'tmp_mass_HA.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import spearmanr, binned_statistic, binned_statistic_2d\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(nrows=2,figsize=(single_column,single_column*2))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "criteria &= (catalogue['age']<=8) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "nbins = 8\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "rho = []\n",
    "for k,gal_name in enumerate(np.unique(sample_table['name'])):\n",
    "\n",
    "    sub = tmp[tmp['gal_name']==gal_name]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    if len(x)>5:\n",
    "        rho.append(spearmanr(x,y,nan_policy='omit')[0]) \n",
    "    x,mean,std = bin_stat(x,y,xlim,nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "#x,mean,std = bin_stat(x,y,[None,None],nbins=bins,statistic='median')\n",
    "#ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')    \n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['distance'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='log M / M$_\\odot$',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "# ----------------------- ax2 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'ax1: {len(tmp)} objects')\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis',10)\n",
    "\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax2.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=cmap)\n",
    "EBV_balmer_err = np.mean(tmp['EBV_balmer_err'])\n",
    "EBV_stars_err = np.mean(tmp['EBV_stars_err'])\n",
    "ax2.errorbar(0.5,0.1,xerr=EBV_stars_err,yerr=EBV_balmer_err,fmt='ko',ms=0)\n",
    "\n",
    "ax2.plot([0,1],[0,2],color='black')\n",
    "ax2.plot([0,2],[0,2],color='black')\n",
    "ax2.set(xlim=[0,0.7],ylim=[0,0.7],xlabel=r'$E(B-V)$ stars',ylabel=r'$E(B-V)$ Balmer')\n",
    "\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar.set_ticks([0,2,4,6,8,10])\n",
    "cbar.set_ticklabels([0,2,4,6,8,'10+'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'mass_HA_EBV.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define limits and labels for corner plots\n",
    "limits   = {'age':(0.5,8.5),\n",
    "            'log_age':(5.9,7),\n",
    "            'age_mw':(0,20),\n",
    "            'met_scal':(8.3,8.7),\n",
    "            'Delta_met_scal':(-0.07,0.07),\n",
    "            'density':(0,150),\n",
    "            'dig/hii':(0.2,0.9),\n",
    "            'fesc':(0,1),\n",
    "            'HA/FUV':(0,80),\n",
    "            'HA/NUV':(0,50),\n",
    "            'HA/FUV_corr':(0,80),\n",
    "            'log_HA':(4.5,7.4),\n",
    "            'EBV_balmer':(0,1),\n",
    "            'sb_HA_pc':(0,200),\n",
    "            'sb_HA_arcsec':(0,1e5),\n",
    "            'log[SIII]/[SII]':(-1.5,0),\n",
    "            'OIII/HB':(0,1),\n",
    "            'logq_D91':(6.2,7.8),\n",
    "            'eq_width':(10,150),\n",
    "            'log_eq_width':(1.2,2.6),\n",
    "            'temperature':(6000,8e3),\n",
    "            'T_N2_REFIT':(6000,8e3),\n",
    "            'log_mass':(4,5.5)}\n",
    "\n",
    "labels   = {'age':'age / Myr',\n",
    "            'log_age' : r'$\\log (\\mathrm{age / Myr})$',\n",
    "            'age_mw':'age (stellar pops) / Myr',\n",
    "            'met_scal':'12+logO/H',\n",
    "            'Delta_met_scal':r'$\\Delta$(O/H)',\n",
    "            'density':r'density / cm$^{-3}$','temperature':'T / K',\n",
    "            'fesc':r'$f_\\mathrm{esc}$',\n",
    "            'HA/FUV':r'H$\\alpha$ / FUV',\n",
    "            'HA/FUV_corr':r'H$\\alpha$ / FUV',\n",
    "            'OIII/HB':r'$[\\mathrm{O}\\,\\textsc{iii}]/\\mathrm{H}\\beta$',\n",
    "            'log_HA':r'$\\log \\mathrm{H}\\alpha$ ',\n",
    "            'EBV_balmer':r'$E(B-V)_\\mathrm{Balmer}$',\n",
    "            'sb_HA':r'$SB_{\\mathrm{H}\\alpha}$',\n",
    "            'eq_width' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$',\n",
    "            'log_eq_width':r'$\\log (\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA})$',\n",
    "            'logq_D91':r'$\\log q$',\n",
    "            'log[SIII]/[SII]':r'$\\log([\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}])$',\n",
    "            'T_N2_REFIT':'temperature / K',\n",
    "            'log_mass':r'$\\log M/\\mathrm{M}_\\odot$'}\n",
    "scale = {'eq_width':'log','HA/FUV':'log'}\n",
    "\n",
    "# calculate a few additional columns\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    catalogue['log_age'] = 6+np.log10(catalogue['age'])\n",
    "    catalogue['log_eq_width'] = np.log10(catalogue['eq_width'])\n",
    "    catalogue['log_HA'] = np.log10(catalogue['HA6562_FLUX_CORR'])\n",
    "    area_per_pixel = ((0.2*u.arcsec).to(u.rad).value*catalogue['distance']*u.Mpc).to(u.pc)**2\n",
    "    catalogue['sb_HA_pc']=catalogue['HA6562_FLUX_CORR']/(catalogue['area_neb']*area_per_pixel.value)\n",
    "    catalogue['sb_HA_arcsec']=catalogue['HA6562_FLUX_CORR']/(catalogue['area_neb']*0.2**2)\n",
    "    catalogue['log[SIII]/[SII]'] = np.log10(catalogue['[SIII]/[SII]'])\n",
    "    catalogue['log_mass'] = np.log10(catalogue['mass'])\n",
    "    catalogue['OIII/HB'] = catalogue['OIII5006_FLUX']/catalogue['HB4861_FLUX_CORR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "correlations with age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['HA/FUV']>3*catalogue['HA/FUV_err']) | np.isnan(catalogue['FUV_FLUX'])\n",
    "#criteria &= catalogue['[SIII]/[SII]']>3*catalogue['[SIII]/[SII]_err']\n",
    "criteria &= (catalogue['age']<=8)\n",
    "# young objects should be associated with a GMC\n",
    "#criteria &= ((catalogue['GMC_sep']<4) | (catalogue['age']>2))\n",
    "#criteria &= catalogue['U_dolmag_vega']-catalogue['B_dolmag_vega']<-1.\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "tmp['HA/FUV_corr'][tmp['FUV_FLUX_CORR']/tmp['FUV_FLUX_CORR_ERR']<3] = np.nan\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "columns  = ['age','eq_width','HA/FUV_corr','logq_D91','Delta_met_scal']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=filename,histogram=False,\n",
    "           figsize=two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "correlations in the nebulae catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds,rasterized=True)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "\n",
    "tmp = nebulae.copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "tmp['log_HA'] = np.log10(tmp['HA6562_FLUX_CORR'])\n",
    "\n",
    "columns  = ['eq_width','HA/FUV_corr','log_HA','T_N2_REFIT','density','EBV_balmer']\n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=basedir/'reports'/f'corner_large.pdf',\n",
    "           figsize=2*two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_by_galaxy\n",
    "\n",
    "columns  = ['age','log_eq_width','HA/FUV_corr','logq_D91','Delta_met_scal']\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner_by_galaxy(tmp[tmp['gal_name']!='NGC1365'],sample_table=sample_table,columns=columns,\n",
    "           limits=limits,labels=labels,nbins=5,filename=None,\n",
    "           figsize=two_column,aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "histograms binned by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>=1e4) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= ((catalogue['GMC_sep']<2) | (catalogue['age']>2))\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4,figsize=(1.5*two_column,1.5*two_column/3))\n",
    "\n",
    "style = {'alpha':0.5}\n",
    "nbins = 10\n",
    "age_bins = [0,2,8]\n",
    "for i in range(len(age_bins)-1):\n",
    "    sub = tmp[(tmp['age']>age_bins[i]) & (tmp['age']<=age_bins[i+1])]\n",
    "    label = f'{age_bins[i]}\\> age \\>= {age_bins[i+1]} Myr: '\n",
    "    \n",
    "    ax1.hist(sub['eq_width'],bins=np.linspace(*limits['eq_width'],nbins),**style,\n",
    "             label=label+f\"{np.nanmean(sub['eq_width']):.1f}\")\n",
    "    ax2.hist(sub['HA/FUV_corr'],bins=np.linspace(*limits['HA/FUV_corr'],nbins),**style,\n",
    "            label=label+f\"{np.nanmedian(sub['HA/FUV_corr']):.1f}\")\n",
    "    ax3.hist(sub['Delta_met_scal'],bins=np.linspace(*limits['Delta_met_scal'],nbins),**style,\n",
    "            label=label+f\"{np.nanmean(sub['Delta_met_scal']):.2f}\")\n",
    "    ax4.hist(sub['logq_D91'],bins=np.linspace(*limits['logq_D91'],nbins),**style,\n",
    "             label=label+f\"{np.nanmean(sub['logq_D91']):.1f}\")\n",
    "    \n",
    "ax1.set(xlabel=labels['eq_width'],xlim=limits['eq_width'])\n",
    "ax2.set(xlabel=labels['HA/FUV_corr'],xlim=limits['HA/FUV_corr'])\n",
    "ax3.set(xlabel=labels['Delta_met_scal'],xlim=limits['Delta_met_scal'])\n",
    "ax4.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])\n",
    "ax1.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax2.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax3.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax4.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with starburst99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_CSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'continuous_SF')\n",
    "cluster_SSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'single_burst_SF')\n",
    "cluster_SSF.measure_FUV()\n",
    "cluster_CSF.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Halpha/FUV\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['overlap_asc']==1)]\n",
    "ax1.scatter(tmp['age'],tmp['HA/FUV_corr'],color='gray')\n",
    "\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_CSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = np.interp(cluster_CSF.ewidth['Time'],cluster_CSF.FUV['Time'],cluster_CSF.FUV['FUV'])\n",
    "ax1.plot(time,Ha/FUV,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_SSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster_SSF.FUV['FUV'].copy()\n",
    "ax1.plot(time,Ha/FUV,label='single burst')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],ylim=[0,100],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "\n",
    "ax2.scatter(tmp['age'],tmp['eq_width'],color='gray')\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_CSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,0.1*eq_width,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_SSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,0.1*eq_width,label='singel burst')\n",
    "ax2.legend()\n",
    "ax2.set(xlim=[0,10],ylim=[0,400],yscale='linear',xlabel='age / Myr',ylabel=r'EW(H$\\alpha$)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the distribution of EW observed vs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_solar = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster_subsolar = Cluster(stellar_model='GENEVAv40',metallicity=0.004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue.copy()\n",
    "\n",
    "eq_width_model_solar = cluster_solar.ewidth['eq_width_H_A']\n",
    "eq_width_model_subsolar = cluster_subsolar.ewidth['eq_width_H_A']\n",
    "age_model = cluster_solar.ewidth['Time']\n",
    "\n",
    "tmp['eq_width_model_solar'] = np.nan\n",
    "tmp['eq_width_model_subsolar'] = np.nan\n",
    "for row in tmp:\n",
    "    idx = np.argmin(np.abs(age_model-row['age']*u.Myr))\n",
    "    row['eq_width_model_solar'] = eq_width_model_solar[idx].value\n",
    "    row['eq_width_model_subsolar'] = eq_width_model_subsolar[idx].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(0,4,15)\n",
    "\n",
    "ax.hist(tmp['eq_width'],bins=bins,label='observed',alpha=0.6)\n",
    "ax.hist(tmp['eq_width_model_solar'],bins=bins,label='model solar',alpha=0.6)\n",
    "#ax.hist(tmp['eq_width_model_subsolar'],bins=bins,label='model subsolar',alpha=0.6)\n",
    "ax.legend()\n",
    "ax.set(xscale='log',xlim=[1,1e4],xlabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other things like violine plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['HA/FUV']>3*catalogue['HA/FUV_err']) | np.isnan(catalogue['FUV_FLUX'])\n",
    "#criteria &= catalogue['[SIII]/[SII]']>3*catalogue['[SIII]/[SII]_err']\n",
    "criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "# young objects should be associated with a GMC\n",
    "#criteria &= ((catalogue['GMC_sep']<4) | (catalogue['age']>2))\n",
    "#criteria &= catalogue['U_dolmag_vega']-catalogue['B_dolmag_vega']<-1.\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['log_age'] = 6+np.log10(tmp['age'])\n",
    "    tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "    tmp['log[SIII]/[SII]'] = np.log10(tmp['[SIII]/[SII]'])\n",
    "\n",
    "columns  = ['logq_D91','Delta_met_scal','HA/FUV_corr','log_eq_width']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=None,\n",
    "           figsize=two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "\n",
    "x = tmp['r_R25']\n",
    "y = tmp['log_eq_width']\n",
    "\n",
    "ax.scatter(x,y,c='gray')\n",
    "corner_binned_stat(x,y,ax)\n",
    "corner_spearmanr(x,y,ax,pvalue=True)\n",
    "\n",
    "ax.set(xlim=[0,1],ylim=[0,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a violin plot to show the distribution of points in each age bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "x_label = 'age'\n",
    "y_label = 'eq_width'\n",
    "\n",
    "x,y = tmp[x_label], tmp[y_label]\n",
    "x,y = x[~np.isnan(y)],y[~np.isnan(y)]\n",
    "x,y = x[(x>=limits[x_label][0]) & (x<=limits[x_label][1]) & (y>=limits[y_label][0]) & (y<=limits[y_label][1])], \\\n",
    "      y[(x>=limits[x_label][0]) & (x<=limits[x_label][1]) & (y>=limits[y_label][0]) & (y<=limits[y_label][1])]\n",
    "\n",
    "positions = np.arange(0,10)\n",
    "\n",
    "#binned_data = corner_violin(x,y,ax,positions,showmedians=True)\n",
    "\n",
    "bins = (positions[1:]+positions[:-1])/2\n",
    "binned_data = [y[(bins[i]<x) & (x<bins[i+1])] for i in range(len(bins)-1)]\n",
    "ax.violinplot(binned_data,positions=positions[1:-1],showmeans=True)\n",
    "\n",
    "ax.set(xlabel=x_label,ylabel=y_label.replace('_',''))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter age with color-color-diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[-0.33 , -1.662],[-0.307, -1.577],[-0.25 , -1.468],[-0.218, -1.394],[-0.186, -1.321],\n",
    "                   [-0.138, -1.235],[-0.076, -1.213],[-0.007, -1.206],[ 0.065, -1.197],[ 0.135, -1.187],\n",
    "                   [ 0.203, -1.198],[ 0.276, -1.201],[ 0.347, -1.209],[ 0.418, -1.219],[ 0.488, -1.233],\n",
    "                   [ 0.559, -1.251],[ 0.63 , -1.27 ],[ 0.706, -1.264],[ 0.771, -1.287],[ 0.648, -1.119],\n",
    "                   [ 0.626, -1.026],[ 0.565, -0.989],[ 0.559, -0.87 ],[ 0.502, -0.807],[ 0.481, -0.706],\n",
    "                   [ 0.457, -0.546],[ 0.505, -0.449],[ 0.487, -0.384],[ 0.488, -0.328],[ 0.468, -0.239],\n",
    "                   [ 0.517, -0.109],[ 0.555, -0.015],[ 0.6  ,  0.041],[ 0.65 ,  0.132],[ 0.724,  0.174],\n",
    "                   [ 0.864,  0.169],[ 0.948,  0.166],[ 1.02 ,  0.152],[ 1.138,  0.182],[ 1.203,  0.194],\n",
    "                   [ 1.253,  0.282],[ 1.331,  0.378],[ 1.354,  0.469]])\n",
    "\n",
    "x,y=points.T\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(2.3*single_column,1*single_column))\n",
    "\n",
    "tmp = associations\n",
    "#ax.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],s=0.5)\n",
    "tmp = catalogue\n",
    "sc=ax1.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['GMC_sep'],vmin=0,vmax=10,\n",
    "           s=0.5)\n",
    "ax1.plot(x,y,color='red')\n",
    "ax1.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "ax1.invert_yaxis()\n",
    "plt.colorbar(sc,label='GMC sep / arcsec',ax=ax1)\n",
    "\n",
    "sc=ax2.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['age'],vmin=0,vmax=20,cmap=plt.cm.plasma,\n",
    "           s=0.5)\n",
    "ax2.plot(x,y,color='red')\n",
    "ax2.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "ax2.invert_yaxis()\n",
    "fig.colorbar(sc,label='age / Myr',ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('UBVI_matched_associations.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "bins=np.arange(0,150,10)\n",
    "tmp = catalogue[(catalogue['age']==1) & (catalogue['overlap_asc']==1)]\n",
    "mask = tmp['GMC_sep']<4\n",
    "\n",
    "ax.hist(tmp['eq_width'][mask],bins=bins,label=f'nearby GMC: {np.mean(tmp[\"eq_width\"][mask]):.2f} AA',histtype='step',alpha=0.8)\n",
    "ax.hist(tmp['eq_width'][~mask],bins=bins,label=f'no GMC {np.mean(tmp[\"eq_width\"][~mask]):.2f} AA',histtype='step',alpha=0.8)\n",
    "ax.set(xlabel=r'EW(H$\\alpha$)')\n",
    "ax.set_title('young associations (age 1 Myr)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "\n",
    "# define limits and labels for corner plots\n",
    "limits   = {'age':(0,20),\n",
    "            'UB':(-2.0,0),\n",
    "            'CO':(0,200),\n",
    "            'GMC_sep':(0,10)}\n",
    "\n",
    "labels   = {'age':'age / Myr',\n",
    "            'UB':r'$U-B$',\n",
    "            'CO':r'CO flux',\n",
    "            'GMC_sep':r'GMC sep / arcsec'}\n",
    "\n",
    "                             \n",
    "tmp = catalogue.copy()\n",
    "#tmp = tmp[tmp['GMC_sep']<5]\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['UB'] = tmp['U_dolmag_vega']-tmp['B_dolmag_vega']\n",
    "\n",
    "columns  = ['age','GMC_sep','CO','UB']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=None,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=None,\n",
    "           figsize=two_column,aspect_ratio=1,s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the old nebulae with high EW might come from Wolf Rayet Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the old nebulae with high EW might come from Wolf Rayet Stars\n",
    "WR_candidates = catalogue[(catalogue['age']>6) & (catalogue['age']<10) & (catalogue['eq_width']>80) & (catalogue['mass']>5e3)]\n",
    "print(f'{len(WR_candidates)} WR candidates')\n",
    "groups = WR_candidates.group_by('gal_name')\n",
    "spectra = []\n",
    "for key, group in zip(groups.groups.keys, groups.groups):\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spectra'/f'{key[\"gal_name\"]}_VorSpectra.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        tbl = Table(hdul[1].data)\n",
    "        spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    tbl['region_ID'] = np.arange(len(tbl))\n",
    "    tbl.add_index('region_ID')\n",
    "    for region_ID in group['region_ID']:\n",
    "        spectra.append((key['gal_name'],region_ID,spectral_axis,tbl.loc[region_ID]['SPEC']))\n",
    "        \n",
    "        hdu = fits.BinTableHDU(WR_candidates[['gal_name','region_ID','assoc_ID','x_neb','y_neb','age','mass','eq_width']],\n",
    "                       name='WR_candidates')\n",
    "hdu.writeto(basedir/'data'/'WR_candidates.fits',overwrite=True)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "filename = basedir/'reports'/'WR_candidates_spectra'\n",
    "nrows=6\n",
    "ncols=1\n",
    "width = two_column\n",
    "N = len(WR_candidates)\n",
    "Npage = nrows*ncols\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = spectra[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width*1.41))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            gal_name,region_ID,spectral_axis,spectrum = row\n",
    "            ax = next(axes_iter)\n",
    "            ax.plot(spectral_axis,spectrum,lw=0.5)\n",
    "            ax.set(xlim=[4860,6800],yscale='log',ylim=[1e2,1e6])\n",
    "            t = ax.text(0.01,0.87,f'{gal_name}: {region_ID:.0f}', transform=ax.transAxes,color='black',fontsize=8)\n",
    "        plt.subplots_adjust(wspace=-0.1, hspace=0)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "            h,l = fig.axes[0].get_legend_handles_labels()\n",
    "            ax = next(axes_iter)\n",
    "            ax.axis('off')\n",
    "            #ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "            #t = ax.text(0.06,0.87,'galaxy name: region ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use elipses for uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "ellipses = [Ellipse((row['age'],row['HA/FUV']),\n",
    "                     row['age_err'],row['HA/FUV_err'],\n",
    "                    alpha=0.5) for row in tmp]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for e in ellipses:\n",
    "    ax.add_artist(e)\n",
    "\n",
    "ax.set(xlim=(0, 10),ylim=(0,80),xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "ax.errorbar(tmp['age'],tmp['HA/FUV'],xerr=tmp['age_err'],yerr=tmp['HA/FUV_err'],fmt='o')\n",
    "\n",
    "ax.set(xlim=(0, 10),ylim=(0,80),xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look for correlations by comparing all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=8)\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['log_age'] = 6+np.log10(tmp['age'])\n",
    "    tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "    tmp['log[SIII]/[SII]'] = np.log10(tmp['[SIII]/[SII]'])\n",
    "print(len(tmp))\n",
    "    \n",
    "columns = ['HA/FUV','eq_width','Delta_met_scal','logq_D91','age','mass',\n",
    "           'EBV_balmer','EBV_stars','GMC_sep','density','temperature']\n",
    "\n",
    "correlation = []\n",
    "pairs = []\n",
    "for a,b in combinations(columns,2):\n",
    "    y,x = tmp[a],tmp[b] \n",
    "    not_nan = ~np.isnan(x) & ~np.isnan(y)\n",
    "    r,p = spearmanr(x[not_nan],y[not_nan])\n",
    "    correlation.append(r)\n",
    "    pairs.append(a+'+'+b)\n",
    "correlation = Table([correlation,pairs],names=['r','pair'])\n",
    "correlation.sort('r')\n",
    "\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits['EBV_balmer'] = (0,1)\n",
    "limits['EBV_stars'] = (0,1)\n",
    "limits['GMC_sep'] = (0,10)\n",
    "\n",
    "corner(tmp,['EBV_balmer','EBV_stars','HA/FUV','eq_width','GMC_sep'],function=plot_function,\n",
    "       limits=limits,labels=labels,\n",
    "       filename=None,\n",
    "       figsize=two_column,aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends with escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from cluster.plot import corner\n",
    "\n",
    "mass_slider = widgets.FloatSlider(\n",
    "    value=3.7,\n",
    "    min=2,\n",
    "    max=4.5,\n",
    "    step=0.2,\n",
    "    description='Cluster mass:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    ")\n",
    "\n",
    "age_slider = widgets.FloatLogSlider(\n",
    "    value=10,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Cluster age:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    ")\n",
    "    \n",
    "def plot_corner(mass,age):\n",
    "    criteria  = (catalogue['mass']>mass) \n",
    "    criteria &= (catalogue['age']<age) \n",
    "    criteria &= (catalogue['overlap_neb']>0.1) \n",
    "    criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "    tmp = catalogue[criteria].copy()\n",
    "    print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "    filename = None #basedir/'reports'/f'all_galaxies_corner.png'\n",
    "    columns  = ['age','HA/FUV','density','fesc','dig/hii','logq_D91','Delta_met_scal']\n",
    "    limits   = {'HA/FUV':(0,100),'age':(0,10),'fesc':(0,1.1),'Delta_met_scal':(-0.1,0.1),\n",
    "                'logq_D91':(6,8),'density':(0,250),\n",
    "                'dig/hii':(0.2,0.9),'met_scal':(8.3,8.7)}\n",
    "\n",
    "    corner(tmp,columns,limits,nbins=5,filename=None,vmin=1000,vmax=1e6,figsize=12,aspect_ratio=1)\n",
    "    plt.show()\n",
    "    \n",
    "widgets.interact(plot_corner, mass=mass_slider,age=age_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>5e3) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "xlim = [0,1]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['fesc'],tmp['density'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "#x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='fesc',ylabel=r'[SII]6716 / [SII]6730',xlim=xlim)\n",
    "\n",
    "#fig.colorbar(sc,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'all_galaxies_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>5e3) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "#xlim = [0,1]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['age'],tmp['mass'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "#x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='age',ylabel=r'mass')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Abundance gradients\n",
    "\n",
    "to help better understand the difference between abundances and local abundance offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "bins = np.logspace(-1.5,-0.2,10)\n",
    "xlim = (1e-2,1e0)\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "table = nebulae[nebulae['[SIII]/[SII]']>3*nebulae['[SIII]/[SII]_err']]\n",
    "bins = np.logspace(*np.log10(np.nanpercentile(table['[SIII]/[SII]'],(2,98))),10)\n",
    "\n",
    "groups = table.group_by('gal_name')\n",
    "\n",
    "ax1.scatter(table['[SIII]/[SII]'],table['met_scal'],color='gray',s=0.1)\n",
    "ax2.scatter(table['[SIII]/[SII]'],table['Delta_met_scal'],color='gray',s=0.1)\n",
    "\n",
    "rho1,rho2 = [], []\n",
    "for group in groups.groups:\n",
    "    gal_name = group[0]['gal_name']\n",
    "\n",
    "    x,mean,std = bin_stat(group['[SIII]/[SII]'],group['met_scal'],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(group['[SIII]/[SII]'],group['Delta_met_scal'],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "  \n",
    "    r1,p1 = spearmanr(group['[SIII]/[SII]'],group['met_scal'],nan_policy='omit')\n",
    "    r2,p2 = spearmanr(group['[SIII]/[SII]'],group['Delta_met_scal'],nan_policy='omit')\n",
    "    #print(f'rho = {r1:.2f}, {r2:.2f}')\n",
    "    rho1.append(r1)\n",
    "    rho2.append(r2)\n",
    "r,p = spearmanr(table['[SIII]/[SII]'],table['met_scal'],nan_policy='omit')\n",
    "t = ax1.text(0.07,0.9,r'$\\rho'+f'={r:.2f}$',transform=ax1.transAxes,fontsize=7)\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "r,p = spearmanr(table['[SIII]/[SII]'],table['Delta_met_scal'],nan_policy='omit')\n",
    "t = ax2.text(0.07,0.9,r'$\\rho'+f'={r:.2f}$',transform=ax2.transAxes,fontsize=7)\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',xlim=[2e-2,1],ylim=[8,8.8],\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$12+\\log (\\mathrm{O}/\\mathrm{H})$')\n",
    "ax2.set(xscale='log',xlim=[2e-2,1],ylim=[-0.15,0.15],\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "abundance_gradients = ascii.read(basedir/'data'/'external'/'radial_abundance_gradients.txt',\n",
    "                                names=['name','R0','g_r25'])\n",
    "abundance_gradients.add_index('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\Delta = 12+\\log (O/H) - g\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['r_R25'],tmp['met_scal'])\n",
    "r = np.linspace(0,0.5)\n",
    "ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gal_name in np.unique(catalogue['gal_name']):\n",
    "    tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "    plt.scatter(tmp['met_scal'],tmp['Delta_met_scal'],c=tmp['r_R25'])\n",
    "    #plt.scatter(tmp['galactic_radius'],tmp['Delta_met_scal'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc=ax.scatter(tmp['met_scal'],tmp['Delta_met_scal'],c=tmp['r_R25'])\n",
    "fig.colorbar(sc,label='r / R25')\n",
    "ax.set(xlabel='12+logOH',ylabel=r'$\\Delta$')\n",
    "plt.show()\n",
    "#r = np.linspace(0,0.5)\n",
    "#ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc=ax.scatter(tmp['logq_D91'],tmp['met_scal'],c=tmp['r_R25'])\n",
    "fig.colorbar(sc,label='r / R25')\n",
    "ax.set(xlabel='log q',ylabel=r'$\\Delta$(O/H)')\n",
    "plt.show()\n",
    "#r = np.linspace(0,0.5)\n",
    "#ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "for gal_name in np.unique(nebulae['gal_name']):\n",
    "    tmp = nebulae[nebulae['gal_name']==gal_name]\n",
    "    x,mean,std = bin_stat(tmp['logq_D91'],tmp['Delta_met_scal'],[5,8.5],nbins=10)\n",
    "    ax.errorbar(x,mean,yerr=std,fmt='-',label=gal_name)\n",
    "    #ax.scatter(tmp['logq_D91'],tmp['Delta_met_scal'],label=gal_name)\n",
    "ax.set(xlim=[5,8.5],ylim=[-0.2,0.2],xscale='linear')\n",
    "#ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Age and Ha/FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "xlim = [0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['age'],tmp['HA/FUV'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel=r'H$\\alpha$ / FUV',xlim=xlim,ylim=[0,100])\n",
    "\n",
    "#fig.colorbar(sc,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'all_galaxies_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(8,3))\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['overlap']=='contained')]\n",
    "ax1.scatter(tmp['dig/hii'],tmp['HA/FUV'],c=tmp['age'],vmin=0,vmax=20,s=2)\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['overlap']=='partial')]\n",
    "sc=ax2.scatter(tmp['dig/hii'],tmp['HA/FUV'],c=tmp['age'],vmin=0,vmax=20,s=2)\n",
    "\n",
    "ax1.set(xlim=[0,1],ylim=[0,100],xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$',ylabel=r'H$\\alpha$/FUV')\n",
    "ax2.set(xlim=[0,1],ylim=[0,100],xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.11, 0.02, 0.84])\n",
    "fig.colorbar(sc,cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "seperate subplot for each galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "from scipy.stats import binned_statistic, pearsonr, spearmanr\n",
    "\n",
    "# '[SIII]/[SII]' , 'HA/FUV', 'AGE_MINCHISQ', 'AGE_BAYES'\n",
    "x_name, y_name = 'age', 'HA/FUV'\n",
    "xlim = [0.5,10.5]\n",
    "bins = 10\n",
    "max_sep = 1*u.arcsec\n",
    "\n",
    "#sample = set(np.unique(nebulae['gal_name'])) & hst_sample\n",
    "sample = np.unique(catalogue['gal_name'])[:-1]\n",
    "\n",
    "filename = basedir/'reports'/'all_objects_age_over_SII.pdf'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample): \n",
    "\n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    criteria = (catalogue['gal_name']==name) \n",
    "    criteria &= (catalogue['mass']>1e3) \n",
    "    #criteria &= ~np.isnan(catalogue['HA/FUV'])\n",
    "    criteria &= (catalogue['overlap'] == 'contained')\n",
    "    #criteria &= (catalogue['neighbors'] == 0)\n",
    "    \n",
    "    tmp = catalogue[criteria]\n",
    "    print(f'{name}: {len(tmp)} objects')\n",
    "    \n",
    "    mean, bin_edges, binnumber = binned_statistic(tmp[x_name],\n",
    "                                                  tmp[y_name],\n",
    "                                                  statistic='mean',\n",
    "                                                  bins=bins,\n",
    "                                                  range=xlim)\n",
    "    std, _, _ = binned_statistic(tmp[x_name],\n",
    "                                  tmp[y_name],\n",
    "                                  statistic='std',\n",
    "                                  bins=bins,\n",
    "                                  range=xlim)\n",
    "\n",
    "    ax.scatter(tmp[x_name],tmp[y_name],color='tab:blue',s=1)\n",
    "    # plot the standard divation with yerr=std\n",
    "    ax.errorbar((bin_edges[1:]+bin_edges[:-1])/2,mean,fmt='-')\n",
    "    ax.text(0.65,0.85,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "\n",
    "    ax.set(xlim=xlim)\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(f'{y_name.replace(\"_\",\" \")}')\n",
    "\n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "\n",
    "\n",
    "#plt.savefig(filename,dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "\n",
    "# '[SIII]/[SII]' , 'HA/FUV'\n",
    "x_name, y_name, z_name = '[SIII]/[SII]', 'HA/FUV', 'AGE_MINCHISQ'\n",
    "xlim = [0.5,10.5]\n",
    "bins = 10\n",
    "max_sep = 2*u.arcsec\n",
    "\n",
    "sample = muse_sample & hst_sample & astrosat_sample\n",
    "\n",
    "filename = basedir/'reports'/'all_objects_FUV_over_SII_with_age.pdf'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample): \n",
    "\n",
    "    # it makes a different if we match the clusters to the nebulae or the other way around\n",
    "    catalogcoord = clusters[clusters['gal_name']==name].copy()\n",
    "    matchcoord   = nebulae[nebulae['gal_name']==name].copy()\n",
    "\n",
    "    idx, sep, _ = match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n",
    "\n",
    "    catalogue = matchcoord.copy()\n",
    "\n",
    "    for col in catalogcoord.columns:\n",
    "        if col in catalogue.columns:\n",
    "            catalogue[f'{col}2'] = catalogcoord[idx][col]\n",
    "        else:\n",
    "            catalogue[col] = catalogcoord[idx][col]\n",
    "\n",
    "    catalogue['sep'] = sep\n",
    "    catalogue = catalogue[sep.__lt__(max_sep)]\n",
    "    catalogue = catalogue[~np.isnan(catalogue[x_name]) & ~np.isnan(catalogue[y_name]) & (catalogue['AGE_MINCHISQ']<10)]\n",
    "    print(f'{name}: {len(catalogue)} objects in joined catalogue')\n",
    "\n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    #catalogue = catalogue[catalogue['HA6562_FLUX']>np.nanpercentile(catalogue['HA6562_FLUX'],50)]\n",
    "    #catalogue = catalogue[catalogue['FUV_FLUX_CORR']>3*catalogue['FUV_FLUX_CORR_ERR']]\n",
    "    #catalogue = catalogue[catalogue['SII6716_FLUX_CORR']>3*catalogue['SII6716_FLUX_CORR_ERR']]\n",
    "    #catalogue = catalogue[catalogue['SIII9068_FLUX_CORR']>3*catalogue['SIII9068_FLUX_CORR_ERR']]\n",
    "\n",
    "    r,p = spearmanr(catalogue['[SIII]/[SII]'],catalogue['HA/FUV'])\n",
    "    print(f'{name}: rho={r:.2f}, {len(catalogue)} objects')\n",
    "\n",
    "    sc = ax.scatter(catalogue['[SIII]/[SII]'],catalogue['HA/FUV'],\n",
    "                    c=catalogue[z_name],vmin=0, vmax=10,cmap=plt.cm.RdBu_r,\n",
    "                    s=3,marker='.')\n",
    "    \n",
    "    ax.text(0.05,0.9,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.7,0.15,r'$\\rho$'+f'={r:.2f}',transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.55,0.05,f'{len(catalogue):.0f} objects', transform=ax.transAxes,fontsize=7)\n",
    "    \n",
    "    ax.set(xscale='log',yscale='log',xlim=[1e-2,1],ylim=[2,2e2])\n",
    "    # https://stackoverflow.com/questions/21920233/matplotlib-log-scale-tick-label-number-formatting/33213196\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel('[SIII]/[SII]')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(r'H$\\alpha$ / FUV')\n",
    "\n",
    "fig.colorbar(sc,ax=axes.ravel().tolist(),label=f'{z_name.replace(\"_\",\" \")}')\n",
    "        \n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "\n",
    "\n",
    "plt.savefig(filename,dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age trend or escape fraction\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\mathrm{esc}} = \\frac{Q_p-Q_o}{Q_p} \n",
    "\\end{equation}\n",
    "\n",
    "first we assume a constant escape fraction of ~70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time']\n",
    "HaFUV_pred=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "HaFUV_obs = 0.3*HaFUV_pred\n",
    "\n",
    "fesc = 1-HaFUV_obs/HaFUV_pred\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age/1e6,HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,80],ylabel='Ha/FUV',xlabel='age / Myr')\n",
    "\n",
    "ax2.scatter(age/1e6,HaFUV_obs)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],ylabel='Ha/FUV observed',xlabel='age / Myr')\n",
    "\n",
    "ax3.scatter(age/1e6,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_obs,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV observed',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we assume that we still observe the theoretical trend, but with the original flux being constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time'].value/1e6\n",
    "HaFUV_obs=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "HaFUV_pred = 80\n",
    "\n",
    "fesc = 1-HaFUV_obs/HaFUV_pred\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age,0*age+HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,90],ylabel='Ha/FUV',xlabel='age / Myr')\n",
    "\n",
    "ax2.scatter(age,HaFUV_obs)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],ylabel='Ha/FUV observed',xlabel='age / Myr')\n",
    "\n",
    "ax3.scatter(age,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_obs,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we assume an escape fraction that is increasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time'].value/1e6\n",
    "HaFUV_pred=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "fesc = 0.7/(1+np.exp(-1*(age-3)))\n",
    "\n",
    "HaFUV_obs = (1-fesc)*HaFUV_pred\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age,HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV')\n",
    "\n",
    "ax2.scatter(age,HaFUV_obs)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV observed')\n",
    "\n",
    "\n",
    "ax3.scatter(age,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_obs,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we don't observe any trends but there should be? How is the escape fraction behaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time'].value/1e6\n",
    "HaFUV_pred=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "HaFUV_obs = 30\n",
    "fesc = 1-HaFUV_obs/HaFUV_pred\n",
    "\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age,HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV')\n",
    "\n",
    "ax2.scatter(age,HaFUV_obs+0*age)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV observed')\n",
    "\n",
    "\n",
    "ax3.scatter(age,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_pred,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'NGC0628'\n",
    "\n",
    "# DAP linemaps (Halpha and OIII)\n",
    "filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{name}*.fits'))\n",
    "copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    OIII = NDData(data=hdul['OIII5006_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['OIII5006_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['OIII5006_FLUX'].data),\n",
    "                    meta=hdul['OIII5006_FLUX'].header,\n",
    "                    wcs=WCS(hdul['OIII5006_FLUX'].header)) \n",
    "\n",
    "filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "# WFI image (larger FOV)\n",
    "filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    WFI = NDData(data=hdul[0].data,\n",
    "                 meta=hdul[0].header,\n",
    "                 wcs=WCS(hdul[0].header))\n",
    "\n",
    "# and for HST\n",
    "from cluster.io import read_associations, ReadHST\n",
    "\n",
    "target  = name.lower()\n",
    "\n",
    "associations, associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,\n",
    "                                                    HSTband=HSTband,scalepc=scalepc,version=version)\n",
    "\n",
    "# read the compact cluster catalogues\n",
    "filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "compact_clusters = Table.read(filename)\n",
    "compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "\n",
    "hst_images = ReadHST(name,data_ext / 'HST' / 'filterImages' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, RectangleSkyRegion\n",
    "\n",
    "position = SkyCoord('03h33m36.36s','-36d08m25.45s')\n",
    "position = SkyCoord(*list(sample_table.loc[name][['R.A.','Dec.']]))\n",
    "position = nebulae[(nebulae['Nassoc']>2) & (nebulae['gal_name']==name) & (nebulae['neighbors']==0)][3]['SkyCoord_neb']\n",
    "size  = (10*u.arcsec,10*u.arcsec)\n",
    "\n",
    "f275w_cutout = Cutout2D(hst_images.f275w.data,position,size=size,wcs=hst_images.f275w.wcs)\n",
    "f336w_cutout = reproject_interp(hst_images.f336w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "f438w_cutout = reproject_interp(hst_images.f438w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "f555w_cutout = reproject_interp(hst_images.f555w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "f814w_cutout = reproject_interp(hst_images.f814w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "halpha_cutout  = reproject_interp(Halpha,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "\n",
    "print('search for mask contours')\n",
    "cutout_nebulae, _  = reproject_interp(nebulae_mask,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_nebulae[~np.isnan(cutout_nebulae)])\n",
    "\n",
    "contours_nebulae = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_nebulae)\n",
    "    blank_mask[cutout_nebulae==i] = 1\n",
    "    contours_nebulae += find_contours(blank_mask, 0.5)\n",
    "\n",
    "cutout_assoc, _  = reproject_interp(associations_mask,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,order='nearest-neighbor')    \n",
    "assoc_ID = np.unique(cutout_assoc[~np.isnan(cutout_assoc)])\n",
    "\n",
    "contours_assoc = []\n",
    "for i in assoc_ID:\n",
    "    blank_mask = np.zeros_like(cutout_assoc)\n",
    "    blank_mask[cutout_assoc==i] = 1\n",
    "    contours_assoc += find_contours(blank_mask, 0.5)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astrotools.plot.multicolorfits as mcf\n",
    "\n",
    "\n",
    "def create_multicolorimage(*args):\n",
    "    '''combine multiple images to a three color image'''\n",
    "    \n",
    "    color_images = []\n",
    "    for arg in args:\n",
    "        color = arg.pop('color')\n",
    "        greyscale = mcf.greyRGBize_image(**arg)\n",
    "        color_images.append(mcf.colorize_image(greyscale,color, colorintype='hex',gammacorr_color=2.2))\n",
    "    \n",
    "    return mcf.combine_multicolor(color_images, gamma=2.2)\n",
    "\n",
    "p1,p2 = np.nanpercentile(halpha_cutout.data,[0,100])\n",
    "f275_white  = {'color':'#d5c5e3','datin':f275w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.9]}\n",
    "f336_purple = {'color':'#9C4FFF','datin':f336w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.9]}\n",
    "f438_blue   = {'color':'#61b3cf','datin':f438w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.8]}\n",
    "f555_green  = {'color':'#00e32d','datin':f555w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.8]}\n",
    "f814_yellow = {'color':'#e08e1b','datin':f814w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.8]}\n",
    "ha_red_peak = {'color':'#ed7f5a','datin':halpha_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[70,99.8]}\n",
    "ha_red_dif  = {'color':'#ed5a95','datin':halpha_cutout.data,'rescalefn':'log','scaletype':'abs','min_max':[4*p1,8*p2]}\n",
    "\n",
    "rgb = create_multicolorimage(f275_white,f336_purple,f438_blue,f555_green,f814_yellow,ha_red_peak,ha_red_dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column))\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "for coords in contours_nebulae:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='red',lw=0.5,label=r'H\\textsc{ii} region')\n",
    "\n",
    "mask = np.zeros((*cutout_nebulae.shape,4))\n",
    "mask[~np.isnan(cutout_nebulae.data),:] = (0.84, 0.15, 0.16,0.1)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "for coords in contours_assoc:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='blue',lw=0.5,label='association')\n",
    "\n",
    "mask = np.zeros((*cutout_assoc.shape,4))\n",
    "mask[~np.isnan(cutout_assoc.data),:] = (0.12,0.47,0.71,0.1)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "region = RectangleSkyRegion(position,0.9*size[0],0.9*size[0])\n",
    "in_frame = compact_clusters[region.contains(compact_clusters['SkyCoord'],f275w_cutout.wcs)]\n",
    "for row in in_frame:\n",
    "    x,y = row['SkyCoord'].to_pixel(f275w_cutout.wcs)\n",
    "    if 5<x<f275w_cutout.data.shape[0]-5 and 5<y<f275w_cutout.data.shape[1]-5:\n",
    "        ax.scatter(x,y,marker='o',facecolors='none',s=40,lw=0.8,color='green',label='DOLPHOT peaks')\n",
    "        if 'label' in compact_clusters.columns:\n",
    "            ax.annotate(row['label'], (x+4, y),color='green')\n",
    "    \n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'{name}_rgb.png',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = SkyCoord(*list(sample_table.loc[name][['R.A.','Dec.']]))\n",
    "size  = (4*u.arcmin,4*u.arcmin)\n",
    "\n",
    "f275w_cutout = Cutout2D(hst_images.f275w.data,position,size=size,wcs=hst_images.f275w.wcs)\n",
    "halpha_cutout  = reproject_interp(Halpha,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "\n",
    "cutout_nebulae, _  = reproject_interp(nebulae_mask,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,order='nearest-neighbor')    \n",
    "\n",
    "cutout_assoc, _  = reproject_interp(associations_mask,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,order='nearest-neighbor')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "norm = simple_norm(f275w_cutout.data,clip=False,percent=99)\n",
    "\n",
    "ax.imshow(f275w_cutout.data,norm=norm,cmap=plt.cm.Greys,origin='lower')\n",
    "ax.imshow(cutout_nebulae.data,cmap=plt.cm.Reds,alpha=0.5)\n",
    "ax.imshow(cutout_assoc.data,cmap=plt.cm.Blues,alpha=0.5)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_nebulae_associations.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPT diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot import BPT_diagram\n",
    "\n",
    "tmp = nebulae[~HIIregion_mask]\n",
    "\n",
    "fig = BPT_diagram(R3=np.log10(tmp['OIII5006_FLUX_CORR']/tmp['HB4861_FLUX_CORR']),\n",
    "            N2=np.log10(tmp['NII6583_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),\n",
    "            S2=np.log10((tmp['SII6716_FLUX_CORR']+tmp['SII6730_FLUX_CORR'])/tmp['HA6562_FLUX_CORR']),\n",
    "            O1=np.log10(tmp['OI6300_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),s=0.1,\n",
    "            #c=tmp['fesc'],vmin=0,vmax=1,cmap=plt.cm.get_cmap('plasma_r', 5),alpha=0.9,s=2,label=r'$f_\\mathrm{esc}$',\n",
    "            filename=None)\n",
    "\n",
    "tmp = nebulae[HIIregion_mask]\n",
    "BPT_diagram(R3=np.log10(tmp['OIII5006_FLUX_CORR']/tmp['HB4861_FLUX_CORR']),\n",
    "            N2=np.log10(tmp['NII6583_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),\n",
    "            S2=np.log10((tmp['SII6716_FLUX_CORR']+tmp['SII6730_FLUX_CORR'])/tmp['HA6562_FLUX_CORR']),\n",
    "            O1=np.log10(tmp['OI6300_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),s=0.1,\n",
    "            #c=tmp['fesc'],vmin=0,vmax=1,cmap=plt.cm.get_cmap('plasma_r', 5),alpha=0.9,s=2,label=r'$f_\\mathrm{esc}$',\n",
    "            #fig=fig,filename=basedir/'reports'/'BPT_nebulae.jpg'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.fromarray(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dominant_color(pil_img):\n",
    "    img = pil_img.copy()\n",
    "    img = img.convert(\"RGBA\")\n",
    "    img = img.resize((1, 1), resample=0)\n",
    "    dominant_color = img.getpixel((0, 0))\n",
    "    return mpl.colors.to_hex([x/255 for x in dominant_color])\n",
    "\n",
    "get_dominant_color(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for gal_name in sample_table_v1p6['gal_name']:\n",
    "    folder = data_ext / 'Products'/'stellar_associations'/f'associations_{version}'/f'{gal_name.lower()}_{HSTband}'/f'{scalepc}pc'\n",
    "    catalogue_file = folder / f'{gal_name.lower()}_phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits'\n",
    "\n",
    "    table = Table.read(catalogue_file)\n",
    "    table.rename_columns(['reg_id'],['assoc_ID'])\n",
    "    table.add_column(gal_name,index=0,name='gal_name')\n",
    "    table.add_column(sample_table_v1p6.loc[gal_name]['dist'],index=1,name='distance')\n",
    "    colnames = sum([[f'{filter}_dolmag_vega',f'{filter}_dolmag_vega_err'] for filter in ['NUV','U','B','V','I']],[])\n",
    "    lst.append(table[['gal_name','assoc_ID','distance']+colnames])\n",
    "table = vstack(lst)\n",
    "table.write(basedir/'data'/f'associations_{HSTband}_{scalepc}pc_{version}.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "450.85px",
    "left": "38px",
    "top": "110.133px",
    "width": "272.867px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
