{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions Multi <a class=\"tocSkip\">\n",
    "\n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues.\n",
    "   \n",
    "This notebook useses multiple galaxies at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from astrotools.packages import *\n",
    "from astrotools.constants import tab10, single_column, two_column\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout,\n",
    "                    #format='(levelname)s %(name)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# first we need to specify the path to the raw data\n",
    "basedir = Path('..')\n",
    "data_ext = Path('a:')/'Archive' #basedir / 'data' / 'raw' \n",
    "\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])\n",
    "sample_table['power_index'] = 2.3\n",
    "sample_table['power_index'][sample_table['AO'].mask]=2.8\n",
    "sample_table['distance'] = Distance(distmod=sample_table['(m-M)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the association catalogue matched with the nebuale catalogue\n",
    "version = 'v1p2'\n",
    "for HSTband in ['nuv','v']:\n",
    "    for scalepc in [8,16,32,64]:\n",
    "        folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "        lst = []\n",
    "        for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "            gal_name = file.stem.split('_')[0]\n",
    "            tbl = Table(fits.getdata(file,ext=1))\n",
    "            tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "            lst.append(tbl)\n",
    "        assoc_tmp = vstack(lst)\n",
    "        # missing = set(sample_table['name']) - set(np.unique(assoc_tmp['gal_name']))\n",
    "        print(f'{HSTband:>3}, {scalepc:>2}pc: {len(lst)} galaxies, {len(assoc_tmp):>5} associations ({np.sum(assoc_tmp[\"1to1\"]):>4} 1to1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which version of the association catalogue to use\n",
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The nebulae catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original catalogue from Francesco\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v2p1.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "HIIregion_mask = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "\n",
    "nebulae.rename_columns(['cen_x','cen_y','cen_ra','cen_dec','region_area',\n",
    "                          'EBV','EBV_ERR','SkyCoord'],\n",
    "                         ['x_neb','y_neb','ra_neb','dec_neb','area_neb',\n",
    "                          'EBV_balmer','EBV_balmer_err','SkyCoord_neb'])\n",
    "    \n",
    "# some additional properties \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_dig.fits') as hdul:\n",
    "    dig = Table(hdul[1].data)\n",
    "dig['dig/hii'] = dig['dig_median'] / dig['hii_median']\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_eq.fits') as hdul:\n",
    "    eq_width = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_density.fits') as hdul:\n",
    "    density = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_refitNII.fits') as hdul:\n",
    "    refitNII = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_in_frame.fits') as hdul:\n",
    "    in_frame = Table(hdul[1].data)\n",
    "    \n",
    "nebulae = join(nebulae,fuv,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,eq_width,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,dig,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,density,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,refitNII,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,in_frame,keys=['gal_name','region_ID'])\n",
    "\n",
    "# this will rais a few errors that we just ignore\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "    #SIII = nebulae['SIII6312_FLUX_CORR']+nebulae['SIII9068_FLUX_CORR']\n",
    "    SIII = (1+2.47)*nebulae['SIII9068_FLUX_CORR']\n",
    "    nebulae['[SIII]/[SII]'][SII>0] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae['[SIII]/[SII]_err'] = np.sqrt( ( ((3.47*nebulae['SIII9068_FLUX_CORR'])**2)*SII**2 + (nebulae['SII6716_FLUX_CORR_ERR']**2+nebulae['SII6730_FLUX_CORR_ERR']**2)*SIII**2) / SII**4)\n",
    "\n",
    "    nebulae['HA/FUV_corr'] = nebulae['HA6562_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    nebulae['HA/FUV_corr_err'] = nebulae['HA/FUV_corr']*np.sqrt((nebulae['FUV_FLUX_CORR_ERR']/nebulae['FUV_FLUX_CORR'])**2+(nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2)\n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX']/nebulae['FUV_FLUX']\n",
    "    nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['FUV_FLUX_ERR']/nebulae['FUV_FLUX'])**2+(nebulae['HA6562_FLUX_ERR']/nebulae['HA6562_FLUX'])**2)\n",
    "\n",
    "    # remove temperatures with low S/N\n",
    "    nebulae['temperature'][nebulae['NII5754_FLUX_CORR']/nebulae['NII5754_FLUX_CORR_ERR']<10] = np.nan\n",
    "    nebulae['density'][nebulae['SII6730_FLUX_CORR']/nebulae['SII6730_FLUX_CORR_ERR']<10] = np.nan\n",
    "\n",
    "    #nebulae['HA/FUV'] = nebulae['HA_conv_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    #nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['FUV_FLUX_CORR_ERR']/nebulae['FUV_FLUX_CORR'])**2+(nebulae['HA_conv_FLUX_CORR_ERR']/nebulae['HA_conv_FLUX_CORR'])**2)\n",
    "    \n",
    "nebulae['distance'] = np.nan\n",
    "for gal_name in np.unique(nebulae['gal_name']):\n",
    "    distance = Distance(distmod=sample_table.loc[gal_name]['(m-M)'])\n",
    "    nebulae['distance'][nebulae['gal_name']==gal_name] = distance\n",
    "nebulae['HA6562_LUM_CORR'] = (nebulae['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(nebulae['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "\n",
    "# the nebulae catalogue with additional information\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*{scalepc}pc_nebulae.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    name=gal_name.lower()\n",
    "    lst.append(tbl)\n",
    "nebulae_tmp = vstack(lst)\n",
    "\n",
    "nebulae = join(nebulae,nebulae_tmp,keys=['gal_name','region_ID'],join_type='outer')\n",
    "\n",
    "print(f'{len(nebulae)} nebulae in initial catalogue (all galaxies)')\n",
    "print(f'{len(np.unique(fuv[~np.isnan(fuv[\"FUV_FLUX\"])][\"gal_name\"]))} galaxies with AstroSat ({np.sum(~np.isnan(fuv[\"FUV_FLUX\"]))} regions)')\n",
    "print(f'we use {len(lst)} galaxies with {np.sum(HIIregion_mask & ~nebulae[\"overlap_neb\"].mask)} HII regions')\n",
    "# only use HII regions and only the galaxies with associations\n",
    "#nebulae = nebulae[HIIregion_mask & ~nebulae[\"overlap_neb\"].mask]\n",
    "nebulae = nebulae[HIIregion_mask]\n",
    "print(f'{np.sum(nebulae[\"overlap_neb\"]>0)} HII regions ({100*np.sum(nebulae[\"overlap_neb\"]>0)/len(nebulae):.1f}%) overlap with association')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those files hold the merged association catalogues\n",
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "    associations = Table(hdul[1].data)\n",
    "    \n",
    "associations['SkyCoord'] = SkyCoord(associations['reg_ra']*u.degree,associations['reg_dec']*u.degree)\n",
    "associations.rename_columns(['reg_ra','reg_dec','reg_x','reg_y',\n",
    "                             'reg_dolflux_Age_MinChiSq','reg_dolflux_Mass_MinChiSq','reg_dolflux_Ebv_MinChiSq',\n",
    "                             'reg_dolflux_Age_MinChiSq_err','reg_dolflux_Mass_MinChiSq_err','reg_dolflux_Ebv_MinChiSq_err',\n",
    "                             'SkyCoord'],\n",
    "                            ['ra_asc','dec_asc','x_asc','y_asc','age','mass',\n",
    "                             'EBV_stars','age_err','mass_err','EBV_stars_err','SkyCoord_asc'])\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'association_CO_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "    assoc_CO = Table(hdul[1].data)\n",
    "associations = join(associations,assoc_CO,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "# Halpha measured in the association masks\n",
    "#with fits.open(basedir/'data'/'interim'/f'phangshst_associations_nuv_ws32pc_v1p1_Halpha.fits') as hdul:\n",
    "#    assoc_Halpha = Table(hdul[1].data)\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}_in_frame.fits') as hdul:\n",
    "    in_frame = Table(hdul[1].data)\n",
    "associations = join(associations,in_frame,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "# the association catalogue matched with the nebuale catalogue\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    #print(f'reading {gal_name}')\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    lst.append(tbl)\n",
    "assoc_tmp = vstack(lst)\n",
    "# combine both catalogues\n",
    "associations = join(associations,assoc_tmp,keys=['gal_name','assoc_ID'])\n",
    "print(f'{len(lst)} galaxies in sample ({len(associations)} associations)')\n",
    "print(f'{np.sum(associations[\"overlap_asc\"]>0)} associations ({100*np.sum(associations[\"overlap_asc\"]>0)/len(associations):.1f}) overlap with an HII region')\n",
    "\n",
    "#criteria = np.abs(associations['age']-associations['age_16'])>associations['age_err']\n",
    "#criteria |= np.abs(associations['age']-associations['age_64'])>associations['age_err']\n",
    "#associations['uniform_age'] = ~criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The joined catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also recreate the table from the association an nebulae catalogues\n",
    "catalogue = join(nebulae[~nebulae['assoc_ID'].mask],associations,keys=['gal_name','assoc_ID','region_ID'])\n",
    "\n",
    "# add dig and calculate galactic radius\n",
    "catalogue['galactic_radius'] = np.nan\n",
    "for gal_name in np.unique(catalogue['gal_name']):\n",
    "    centre = sample_table.loc[gal_name]['SkyCoord']\n",
    "    catalogue['galactic_radius'][catalogue['gal_name']==gal_name] = catalogue[catalogue['gal_name']==gal_name]['SkyCoord_neb'].separation(centre).to(u.arcmin)\n",
    "#catalogue['HA/NUV'] = catalogue['HA6562_FLUX_CORR']/catalogue['NUV_FLUX']/1e20\n",
    "    \n",
    "#del catalogue[['SkyCoord_asc','SkyCoord_neb']]\n",
    "#hdu = fits.BinTableHDU(catalogue,name='matched catalogue')\n",
    "#hdu.writeto(basedir/'data'/'interim'/f'matched_catalogue_{HSTband}_ws{scalepc}pc_{version}.fits',overwrite=True)\n",
    "\n",
    "    \n",
    "print(f'{len(catalogue)} objects in final catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_sample      = set(np.unique(associations['gal_name']))\n",
    "astrosat_sample = set(np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name']))\n",
    "muse_sample     = set(sample_table['name'])\n",
    "complete_sample = hst_sample & astrosat_sample & muse_sample\n",
    "\n",
    "\n",
    "print(f'nebulae: {len(nebulae[np.isin(nebulae[\"gal_name\"],list(hst_sample))])}')\n",
    "print(f'associations: {len(associations)} from {len(hst_sample)} galaxies')\n",
    "print(f'FUV: {len(astrosat_sample & hst_sample)} galaxies')\n",
    "print(f'match: {len(catalogue)}')\n",
    "print(f'contained: {len(catalogue[catalogue[\"overlap\"]==\"contained\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Table to showcase the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "astrosat_sample = np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name'])\n",
    "muse_sample     = sample_table['name']\n",
    "hst_sample      = associations['gal_name']\n",
    "sitelle_sample  = ['NGC0628','NGC2835','NGC3351','NGC3627','NGC4535']\n",
    "\n",
    "t = Table({\n",
    "    'name':muse_sample,\n",
    "    'MUSE':np.isin(muse_sample,muse_sample),\n",
    "    'HST':np.isin(muse_sample,hst_sample),\n",
    "    'Astrosat':np.isin(muse_sample,astrosat_sample),\n",
    "    'Sitelle':np.isin(muse_sample,sitelle_sample)}\n",
    "     )\n",
    "\n",
    "for col in t.columns[1:]:\n",
    "    t[col] = ['\\checkmark' if x else '' for x in t[col] ]\n",
    "ascii.write(t,sys.stdout, Writer = ascii.Latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sample table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table*',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Galaxy sample',\n",
    "'units': {'R.A.':'(J2000)','Dec.':'(J2000)','$i$':'deg','PA':'deg','Distance':'$\\si{\\mega\\parsec}$',\n",
    "          'r25':'arcmin'},\n",
    "'preamble': '\\\\centering',\n",
    "'tablefoot': f'\\\\label{{tbl:sample}}'\n",
    "            }\n",
    "\n",
    "tbl = sample_table[['name','Type','R.A.','Dec.','Inclination','posang','r25','mass','SFR','PSF','(m-M)']]\n",
    "\n",
    "tbl['R.A.'] = [row.replace('h','x').replace('m','y').replace('.','z').replace('s','') for row in tbl['R.A.']]\n",
    "tbl['R.A.'] = [row.replace('x','$^\\mathrm{h}$').replace('y','$^\\mathrm{m}$').replace('z','$^\\mathrm{s}\\kern -3pt.$') for row in tbl['R.A.']]\n",
    "\n",
    "tbl['Dec.'] = [row.replace('d','x').replace('m','y').replace('.','z').replace('s','') for row in tbl['Dec.']]\n",
    "tbl['Dec.'] = [row.replace('-','$-$').replace('+','$+$').replace('x','$^\\mathrm{d}$').replace('y','$^\\mathrm{m}$').replace('z','$^\\mathrm{s}\\kern -3pt.$') for row in tbl['Dec.']]\n",
    "tbl.add_column(Distance(distmod=tbl['(m-M)']).to(u.Mpc),index=4,name='Distance')\n",
    "tbl['Distance'].info.format = '%.2f' \n",
    "tbl['Nneb'] = [np.sum((nebulae['gal_name']==name) & (nebulae['in_frame'])) for name in tbl['name']]\n",
    "tbl['Nasc'] = [np.sum((associations['gal_name']==name) & (associations['in_frame'])) for name in tbl['name']]\n",
    "\n",
    "#tbl['name'] = [f'\\\\galaxyname{{{row[\"name\"][:-4]}}}{{{row[\"name\"][-4:]}}}' for row in tbl]\n",
    "tbl.rename_columns(['Inclination','posang','r25'],['$i$','PA','$r_{25}$'])\n",
    "#tbl = join(tbl,t,keys='name')\n",
    "\n",
    "#with open(basedir / 'data' / 'interim' /'sample.tex','w',newline='\\n') as f:\n",
    "#    ascii.write(tbl,f,Writer=ascii.Latex, latexdict=latexdict,overwrite=True,exclude_names=['(m-M)','Sitelle'])\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict,exclude_names=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table',\n",
    "'col_align':'lrrrrc',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Galaxy sample',\n",
    "'units': {'R.A.':'(J2000)','Dec.':'(J2000)','$i$':'deg','PA':'deg','Distance':'$\\si{\\mega\\parsec}$',\n",
    "          'r25':'arcmin'},\n",
    "'preamble': '\\\\centering',\n",
    "'tablefoot': f'\\\\label{{tbl:sample}}'\n",
    "            }\n",
    "\n",
    "astrosat_sample = np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name'])\n",
    "\n",
    "tbl = sample_table[['name','(m-M)']]\n",
    "tbl.sort('name')\n",
    "tbl.add_column(Distance(distmod=tbl['(m-M)']).to(u.Mpc),index=1,name='Distance')\n",
    "tbl['Distance'].info.format = '%.2f' \n",
    "del tbl['(m-M)']\n",
    "tbl[r'$N_{\\HII}$'] = [np.sum((nebulae['gal_name']==name) & (nebulae['in_frame'])) for name in tbl['name']]\n",
    "tbl[r'$N_\\mathrm{asc}$'] = [np.sum((associations['gal_name']==name) & (associations['in_frame'])) for name in tbl['name']]\n",
    "tbl[r'$N_\\mathrm{match}$'] = [np.sum(catalogue['gal_name']==name) for name in tbl['name']]\n",
    "tbl.add_column([f'\\\\galaxyname{{{row[\"name\"][:-4]}}}{{{row[\"name\"][-4:]}}}' for row in tbl],index=0,name='Name')\n",
    "\n",
    "tbl['\\\\textit{AstroSat}'] = ['\\checkmark' if x else '' for x in np.isin(tbl['name'],astrosat_sample)]\n",
    "del tbl['name']\n",
    "\n",
    "tbl.add_row(['',None,np.sum(nebulae['in_frame']),np.sum(associations['in_frame']),len(catalogue),''])\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict,exclude_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot the sample (cutouts)\n",
    "\n",
    "plot all objects in the merged catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from cluster.plot import single_cutout\n",
    "\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria]\n",
    "\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "size=10*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_{HSTband}_{scalepc}pc'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                # nebulae mask\n",
    "                filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            ax = single_cutout(ax,\n",
    "                             position = row['SkyCoord_neb'],\n",
    "                             image = F275,\n",
    "                             mask1 = nebulae_mask,\n",
    "                             mask2 = associations_mask,\n",
    "                             label = f\"{row['gal_name']}: {row['region_ID']:.0f}/{row['assoc_ID']:.0f}\",\n",
    "                             size  = 4*u.arcsecond)\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "three color composit with CO emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "from pnlf.plot import create_RGB\n",
    "\n",
    "\n",
    "criteria = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_asc'] == 1)\n",
    "criteria &= (catalogue['age'] < 10)\n",
    "#tmp = catalogue[criteria]\n",
    "\n",
    "print(f'{len(tmp)} objects')\n",
    "\n",
    "size=5*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_rgb_{HSTband}_{scalepc}pc'\n",
    "\n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for j in range(Npages):\n",
    "        print(f'working on page {j+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[j*Npage:(j+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                print(f'reading files for {gal_name}')\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                filename = data_ext/'MUSE'/'DR2.1'/'MUSEDAP'/ f'{gal_name}_MAPS.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                                    meta=hdul['HA6562_FLUX'].header,\n",
    "                                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    \n",
    "                # nebulae mask\n",
    "                filename = data_ext/'Products'/ 'Nebulae catalogue'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'HST'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "            \n",
    "                with fits.open(data_ext/'ALMA'/'v4p0'/f'{gal_name.lower()}_12m+7m+tp_co21_broad_tpeak.fits') as hdul:\n",
    "                    CO = NDData(data=hdul[0].data,\n",
    "                                meta=hdul[0].header,\n",
    "                                wcs=WCS(hdul[0].header))\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            \n",
    "            position = row['SkyCoord_neb']\n",
    "            \n",
    "            label = f\"{row['gal_name']}: {row['region_ID']:.0f}/{row['assoc_ID']:.0f}\"\n",
    "\n",
    "            cutout_F275 = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "            norm = simple_norm(cutout_F275.data,stretch='linear',clip=False,percent=99.9)\n",
    "\n",
    "            cutout_CO, _  = reproject_interp(CO,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "            cutout_Halpha, _  = reproject_interp(Halpha,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "\n",
    "            rgb = create_RGB(cutout_CO,cutout_Halpha,cutout_F275.data,\n",
    "                             percentile=[98,98,99.8],weights=[0.7,0.6,1])\n",
    "            #ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "            ax.imshow(rgb,origin='lower')\n",
    "\n",
    "            # plot the nebulae catalogue\n",
    "            cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape,order='nearest-neighbor')    \n",
    "            region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_mask)\n",
    "                blank_mask[cutout_mask==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=0.8,label='HII-region')\n",
    "\n",
    "            # 32 pc\n",
    "            cutout_32 = Cutout2D(associations_mask.data,position,size=size,wcs=associations_mask.wcs)\n",
    "            region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_32.data)\n",
    "                blank_mask[cutout_32.data==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='blue',lw=0.8,label='32pc assoc.')\n",
    "            t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "            t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if j == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fesc from Starburst99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each association, calculate the predicted number of ionizing photons based on the mass and the age of the association.\n",
    "\n",
    "The observed number of ionizing photos is calulcated based on the conversion factor from Niederhofer+2016 (or Kennicutt+98)\n",
    "\n",
    "$$\n",
    "Q(\\mathrm{H}^0) = 7.31\\cdot 10^{11} L(\\mathrm{H}\\alpha)\n",
    "$$\n",
    "\n",
    "Starburst99 uses a factor of 7.354+-0.008 (no Idea why it changes over time) to convert between quanta (number of ionizing photons) and ewidth (Halpha luminosity).\n",
    "\n",
    "we only use a subsample (high mass, contained, speparated etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "HI_rate = cluster.quanta['HI_rate'].value\n",
    "time = cluster.quanta['Time']\n",
    "for row in tqdm(catalogue):\n",
    "    idx = np.argmin(np.abs(time-row['age']*u.Myr))\n",
    "    row['Qpredicted'] = HI_rate[idx] * row['mass'] / cluster.mass\n",
    "    \n",
    "catalogue['distance'] = np.nan\n",
    "for gal_name in catalogue['gal_name']:\n",
    "    distance = Distance(distmod=sample_table.loc[gal_name]['(m-M)'])\n",
    "    catalogue['distance'][catalogue['gal_name']==gal_name] = distance\n",
    "    \n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(catalogue['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for objects with negative fesc we redo the analysis with age-age_err\n",
    "Qpredict_new = []\n",
    "for row in tqdm(catalogue):\n",
    "    if row['fesc']<0:\n",
    "        idx = np.argmin(np.abs(time-(row['age']-3*row['age_err'])*u.Myr))\n",
    "        row['Qpredicted'] = ( HI_rate[idx] * row['mass'] / cluster.mass )\n",
    "\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative escape fraction by enviornment\n",
    "criteria  = (catalogue['mass']>5e3) \n",
    "criteria &= (catalogue['fesc']<0.) \n",
    "print(f'fesc<0: {np.sum(criteria)}')\n",
    "for env in np.unique(catalogue['env_asc']):\n",
    "    print(f\"{env}: {np.sum(criteria&(catalogue['env_asc']==env))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria  = (catalogue['overlap_asc']==1) \n",
    "criteria  &= (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "#criteria &= fesc>0\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "print(f'fesc={np.nanmean(tmp[tmp[\"fesc\"]>0][\"fesc\"]):.2f} (from {np.sum(criteria)} objects)')\n",
    "print(f\"{np.sum(tmp['fesc']<0)} of {len(tmp)} ({np.sum(tmp['fesc']<0)/len(tmp)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mass vs observed Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Ha on a grid of different ages/masses with Starburst99\n",
    "age_grid = np.array([0,6,12,18])*u.Myr/2\n",
    "mass_grid = np.logspace(2,7,10)\n",
    "ionizing_photons = np.zeros((len(age_grid),len(mass_grid)))\n",
    "for i,mass in enumerate(mass_grid):\n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    for j,age in enumerate(age_grid):\n",
    "        idx = np.argmin(np.abs(scaled_cluster.quanta['Time']-age))\n",
    "        ionizing_photons[j,i] = scaled_cluster.quanta['HI_rate'][idx].value\n",
    "distance = 10*u.Mpc\n",
    "LHa = ionizing_photons / 7.31e11 * u.erg \n",
    "FHa = LHa / (1e-20*u.erg/u.s/u.cm**2 *4*np.pi*distance**2).to(u.erg/u.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 3,5\n",
    "\n",
    "aspect_ratio = 1.1\n",
    "cmap = plt.cm.get_cmap('copper',6)\n",
    "fig=plt.figure(figsize=(two_column,aspect_ratio*nrows/ncols*two_column))\n",
    "axes = []\n",
    "\n",
    "sample = list(np.unique(tmp['gal_name']))\n",
    "sample.remove('IC5332')\n",
    "\n",
    "for i,gal_name in enumerate(sample):\n",
    "    ax = fig.add_subplot(nrows,ncols,i+1)\n",
    "    axes.append(ax)\n",
    "    sub = tmp[tmp['gal_name']==gal_name]\n",
    "    \n",
    "    distance = sub['distance'][0]*u.Mpc\n",
    "    LHa = ionizing_photons / 7.31e11 * u.erg \n",
    "    FHa = LHa / (u.erg/u.s/u.cm**2 *4*np.pi*distance**2).to(u.erg/u.s)\n",
    "    \n",
    "    sc=ax.scatter(sub['mass'],1e-20*sub['HA6562_FLUX_CORR'],s=3,\n",
    "                  c=sub['age'],cmap=cmap,vmin=0,vmax=9,zorder=2)\n",
    "    \n",
    "    # plot the theoretical liens\n",
    "    for j,age in enumerate(age_grid):\n",
    "        color = cmap(age/9/u.Myr)\n",
    "        ax.plot(mass_grid,FHa[j,:],label=age,color=color)\n",
    "    \n",
    "    ax.set(xlim=[1e3,1e6],ylim=[5e-17,5e-12],xscale='log',yscale='log')\n",
    "    ax.text(0.05,0.9,f'{gal_name}', transform=ax.transAxes,fontsize=7)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1e4))\n",
    "    #ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(1e5))\n",
    "    \n",
    "    if i%ncols==0:\n",
    "        ax.set(ylabel=r'$F(\\mathrm{H}\\alpha)$ / erg s$^{-1}$ cm$^{-2}$')\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    if i//ncols==nrows-1:\n",
    "        ax.set(xlabel=r'mass / M$_\\odot$')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "    #print(np.diff(ax.get_xlim())/np.diff(ax.get_ylim()))\n",
    "    #ax.set_aspect(0.5)\n",
    "\n",
    "# Create the legend\n",
    "h,l = axes[0].get_legend_handles_labels()\n",
    "fig.legend(h,l,\n",
    "           ncol=5,\n",
    "           loc=\"upper center\",   # Position of legend\n",
    "           borderaxespad=0.1,    # Small spacing around legend box\n",
    "           )\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.07, 0.02, 0.89])\n",
    "fig.colorbar(sc,cax=cbar_ax,label='age / Myr',ticks=age_grid)\n",
    "\n",
    "plt.savefig(basedir/'reports'/f'mass_vs_Halpha_{HSTband}_{scalepc}pc.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,0.9*single_column))\n",
    "\n",
    "print(f'fesc={np.mean(fesc[fesc>0]):.2f}')\n",
    "\n",
    "Qpredicted = np.logspace(48,52)\n",
    "cmap = plt.cm.get_cmap('copper',6)\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "for i,f in enumerate([0.0,0.5,0.9,0.99]):\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    ax.plot(np.log10(Qpredicted),np.log10(Qobserved),ls=lines[i],c='k',label=f'$f_\\mathrm{{esc}}={f}$',zorder=1)\n",
    "\n",
    "sc=ax.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['dig/hii'],cmap=cmap,vmin=0.,vmax=1.,s=2,zorder=2)\n",
    "ax.legend()\n",
    "fig.colorbar(sc,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "#fig.colorbar(sc,label='$\\log_{10} q$ (from Diaz+91)')\n",
    "\n",
    "ax.set(xlabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ predicted (SB99)',\n",
    "       ylabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ observed (MUSE)',\n",
    "       xlim=[48,52],ylim=[48,52])\n",
    "\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(4)) \n",
    "ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(4)) \n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'escape_fraction.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig,((ax1,ax2),(ax3,ax4),(ax5,ax6)) =plt.subplots(ncols=2,nrows=3,figsize=(two_column,1.2*two_column))\n",
    "fig,((ax1,ax2,ax3),(ax4,ax5,ax6)) =plt.subplots(ncols=3,nrows=2,figsize=(1.2*two_column,two_column/1.5))\n",
    "\n",
    "print(f'fesc={np.mean(fesc_classic[fesc_classic>0]):.2f}')\n",
    "\n",
    "Qpredicted = np.logspace(47,52)\n",
    "lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "for i,f in enumerate([0.0,0.5,0.9,0.99]):\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    for ax in (ax1,ax2,ax3,ax4,ax5,ax6):\n",
    "        ax.plot(np.log10(Qpredicted),np.log10(Qobserved),ls=lines[i],c='k',label=f'$f_\\mathrm{{esc}}={f}$',zorder=1)\n",
    "\n",
    "cmap = plt.cm.get_cmap('plasma',5)\n",
    "sc1=ax1.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['logq_D91'],cmap=cmap,vmin=6,vmax=8,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('copper',5)\n",
    "sc2=ax2.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['dig/hii'],cmap=cmap,vmin=0,vmax=1,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('viridis',4)\n",
    "sc3=ax3.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['EBV_stars'],cmap=cmap,vmin=0,vmax=0.8,s=2,zorder=2)\n",
    "cmap = plt.cm.get_cmap('cividis',5)\n",
    "sc4=ax4.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "\n",
    "                c=tmp['age'],cmap=cmap,vmin=0,vmax=10,s=2,zorder=2)\n",
    "\n",
    "cmap = plt.cm.get_cmap('ocean',5)\n",
    "#https://stackoverflow.com/questions/18926031/how-to-extract-a-subset-of-a-colormap-as-a-new-colormap-in-matplotlib\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list('new_ocean',cmap(np.linspace(0, 0.79, 4)),6)\n",
    "sc5=ax5.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=np.log10(tmp['mass']),cmap=cmap,vmin=4,vmax=5.5,s=2,zorder=2)\n",
    "\n",
    "cmap = plt.cm.get_cmap('magma',5)\n",
    "sc6=ax6.scatter(np.log10(tmp['Qpredicted']),np.log10(tmp['Qobserved']),\n",
    "              c=tmp['galactic_radius'],cmap=cmap,vmin=0,vmax=2,s=2,zorder=2)\n",
    "\n",
    "\n",
    "ax1.legend()\n",
    "fig.colorbar(sc1,ax=ax1,label=r'$\\log q$ (from Diaz 91)',ticks=[6,6.4,6.8,7.2,7.6,8])\n",
    "fig.colorbar(sc2,ax=ax2,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "fig.colorbar(sc3,ax=ax3,label=r'$E(B-V)_\\mathrm{stars}$',ticks=[0,0.2,0.4,0.6,0.8])\n",
    "fig.colorbar(sc4,ax=ax4,label=r'age / Myr')\n",
    "fig.colorbar(sc5,ax=ax5,label=r'$\\log_{10} M/\\mathrm{M}_\\odot$',ticks=[4.0,4.5,5.0,5.5])\n",
    "fig.colorbar(sc6,ax=ax6,label=r'galactic radius / arcmin',ticks=[0,0.4,0.8,1.2,1.6,2])\n",
    "\n",
    "for ax in (ax4,ax5,ax6):\n",
    "    ax.set(xlabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ predicted')\n",
    "for ax in (ax1,ax4):\n",
    "    ax.set(ylabel=r'$\\log_{10} (\\mathcal{Q (\\mathrm{H}^0)} / \\mathrm{s}^{-1})$ observed')\n",
    "\n",
    "for ax in (ax1,ax2,ax3,ax4,ax5,ax6):\n",
    "    ax.set(xlim=[49,52],ylim=[49,52])\n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(3)) \n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(3)) \n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/f'escape_fraction_{HSTband}_{scalepc}pc.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPT_diagram(R3,N2,S2,O1,label='',filename=None,fig=None,**kwargs):\n",
    "    '''create a BPT diagram\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    \n",
    "    R3 : array\n",
    "        log10([OIII]/Hbeta)\n",
    "        \n",
    "    N2 : array\n",
    "        log10([NII]/Halpha)\n",
    "        \n",
    "    S2 : array\n",
    "        log10([SII]/Halpha)\n",
    "        \n",
    "    O1 : array\n",
    "        log10([OI]/Halpha)\n",
    "        \n",
    "    Other Parameters\n",
    "    ----------------\n",
    "    \n",
    "    **kwargs : passed to plt.scatter()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if not fig:\n",
    "        fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/2.8),sharey=True)\n",
    "    else:\n",
    "        (ax1,ax2,ax3) = fig.get_axes()\n",
    "        \n",
    "    x = np.linspace(-2.5,2.5,200)\n",
    "    \n",
    "    # ---- N2 vs R3 plot ----\n",
    "    ax1.scatter(N2,R3,**kwargs)\n",
    "    ax1.plot(x[x<0.47],0.61/(x[x<0.47]-0.47)+1.19,color='black',ls='--',label='Kewley+2001')\n",
    "    ax1.plot(x[x<-0.032],0.359/(x[x<-0.032]+0.032)+1.083,color='black',ls='-',label='Law+2021')\n",
    "    #ax1.legend() \n",
    "    \n",
    "    ax1.set(xlim=[-1.4,0.5],ylim=[-1.5,1],\n",
    "            xlabel=r'$\\log ( [\\mathrm{N}\\,\\textsc{ii}] / \\mathrm{H}\\alpha )$',\n",
    "            ylabel=r'$\\log ([\\mathrm{O}\\,\\textsc{iii}] / \\mathrm{H}\\beta )$')\n",
    "    \n",
    "    # ---- S2 vs R3 plot ----    \n",
    "    ax2.scatter(S2,R3,**kwargs)\n",
    "    ax2.plot(x[x<0.32],0.72/(x[x<0.32]-0.32)+1.3,color='black',ls='--',label='Kewley+2001')\n",
    "    ax2.plot(x[x>-0.33],1.89*x[x>-0.33]+0.76,color='black',ls='--',label='Kewley+2001')\n",
    "    ax2.plot(x[x<0.198],0.41/(x[x<0.198]-0.198)+1.164,color='black',ls='-',label='Law+2021')\n",
    "\n",
    "    ax2.set(xlim=[-1.4,0.5],ylim=[-1.5,1],\n",
    "            xlabel=r'$\\log ( [\\mathrm{S}\\,\\textsc{ii}] / \\mathrm{H}\\alpha )$')\n",
    "\n",
    "    # ---- O1 vs R3 plot ----\n",
    "    sc = ax3.scatter(O1,R3,**kwargs)\n",
    "    ax3.plot(x[x<-0.53],0.73/(x[x<-0.53]+0.53)+1.33,color='black',ls='--',label='Kewley+2001')\n",
    "    ax3.plot(x,0.612/(x+0.36)+1.179,color='black',ls='-',label='Law+2021')\n",
    "    \n",
    "    ax3.set(xlim=[-2.4,-0.5],ylim=[-1.5,1],\n",
    "            xlabel=r'$\\log ( [\\mathrm{O}\\,\\textsc{i}] / \\mathrm{H}\\alpha )$')\n",
    "\n",
    "    for ax in (ax1,ax2,ax3):\n",
    "        ax.grid()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.0)\n",
    "    \n",
    "    if 'c' in kwargs:\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([0.95, 0.2, 0.02, 0.72])\n",
    "        fig.colorbar(sc,cax=cbar_ax,label=label)\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename,dpi=600)\n",
    "        plt.show()\n",
    "\n",
    "    else: \n",
    "        return fig\n",
    "\n",
    "tmp = nebulae[~HIIregion_mask]\n",
    "\n",
    "fig = BPT_diagram(R3=np.log10(tmp['OIII5006_FLUX_CORR']/tmp['HB4861_FLUX_CORR']),\n",
    "            N2=np.log10(tmp['NII6583_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),\n",
    "            S2=np.log10((tmp['SII6716_FLUX_CORR']+tmp['SII6730_FLUX_CORR'])/tmp['HA6562_FLUX_CORR']),\n",
    "            O1=np.log10(tmp['OI6300_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),s=0.1,\n",
    "            #c=tmp['fesc'],vmin=0,vmax=1,cmap=plt.cm.get_cmap('plasma_r', 5),alpha=0.9,s=2,label=r'$f_\\mathrm{esc}$',\n",
    "            filename=None)\n",
    "\n",
    "tmp = nebulae[HIIregion_mask]\n",
    "BPT_diagram(R3=np.log10(tmp['OIII5006_FLUX_CORR']/tmp['HB4861_FLUX_CORR']),\n",
    "            N2=np.log10(tmp['NII6583_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),\n",
    "            S2=np.log10((tmp['SII6716_FLUX_CORR']+tmp['SII6730_FLUX_CORR'])/tmp['HA6562_FLUX_CORR']),\n",
    "            O1=np.log10(tmp['OI6300_FLUX_CORR']/tmp['HA6562_FLUX_CORR']),s=0.1,\n",
    "            #c=tmp['fesc'],vmin=0,vmax=1,cmap=plt.cm.get_cmap('plasma_r', 5),alpha=0.9,s=2,label=r'$f_\\mathrm{esc}$',\n",
    "            fig=fig,filename=basedir/'reports'/'BPT_nebulae.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "\n",
    "logbins = np.logspace(3,7,20)\n",
    "sub = associations[associations['overlap']=='isolated']\n",
    "ax.hist(sub['mass'],bins=logbins,label='isolated',alpha=0.6)\n",
    "\n",
    "sub = associations[associations['overlap']=='partial']\n",
    "ax.hist(sub['mass'],bins=logbins,label='partial',alpha=0.6)\n",
    "\n",
    "sub = associations[associations['overlap']=='contained']\n",
    "ax.hist(sub['mass'],bins=logbins,label='contained',alpha=0.6)\n",
    "\n",
    "ax.set(xlabel='mass / Msun',xscale='log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### are the trends just due to fesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2.2))\n",
    "\n",
    "sc=ax1.scatter(tmp['HA/FUV'],tmp['logq_D91'],c=tmp['fesc'],vmin=0,vmax=1)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['logq_D91'],[0,80],nbins=5,statistic='median')\n",
    "ax1.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax1.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\log q$',xlim=[0,80],ylim=[6,8])\n",
    "\n",
    "ax2.scatter(tmp['HA/FUV'],tmp['Delta_met_scal'],c=tmp['fesc'],vmin=0,vmax=1)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['Delta_met_scal'],[0,80],nbins=5,statistic='median')\n",
    "ax2.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax2.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\Delta$(O/H)',xlim=[0,80],ylim=[-0.1,0.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.87, 0.145, 0.02, 0.8])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2.2))\n",
    "\n",
    "sc=ax1.scatter(tmp['HA/FUV'],tmp['logq_D91'],c=tmp['age'],vmin=0,vmax=5)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['logq_D91'],[0,80],nbins=5,statistic='median')\n",
    "ax1.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax1.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\log q$',xlim=[0,80],ylim=[6,8])\n",
    "\n",
    "ax2.scatter(tmp['HA/FUV'],tmp['Delta_met_scal'],c=tmp['age'],vmin=0,vmax=5)\n",
    "x,mean,std = bin_stat(tmp['HA/FUV'],tmp['Delta_met_scal'],[0,80],nbins=5,statistic='median')\n",
    "ax2.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax2.set(xlabel=r'H$\\alpha$ / FUV',ylabel=r'$\\Delta$(O/H)',xlim=[0,80],ylim=[-0.1,0.1])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.3)\n",
    "cbar_ax = fig.add_axes([0.87, 0.145, 0.02, 0.8])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/20105364/how-can-i-make-a-scatter-plot-colored-by-density-in-matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "x,y = tmp['HA/FUV'],tmp['fesc']\n",
    "\n",
    "x,y = x[~np.isnan(x) & ~np.isnan(y)], y[~np.isnan(x) & ~np.isnan(y)]\n",
    "xy = np.vstack([x,y])\n",
    "density = gaussian_kde(xy)(xy)\n",
    "\n",
    "ax.scatter(x,y,cmap=plt.cm.Reds)\n",
    "bins,mean,std = bin_stat(tmp['HA/FUV'],tmp['fesc'],[0,80],nbins=8,statistic='median')\n",
    "ax.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax.set(xlim=[0,80],ylim=[0,1.2],\n",
    "       xlabel=r'H$\\alpha$ / FUV',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(tmp['age'],tmp['fesc'])\n",
    "bins,mean,std = bin_stat(tmp['age'],tmp['fesc'],[0.5,10.5],nbins=10,statistic='median')\n",
    "ax1.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax1.set(xlim=[0,10],ylim=[0,1],\n",
    "       xlabel=r'age / Myr',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "x,y = tmp['HA/FUV'],tmp['fesc']\n",
    "ax2.scatter(x,y,cmap=plt.cm.Reds)\n",
    "bins,mean,std = bin_stat(tmp['HA/FUV'],tmp['fesc'],[0,80],nbins=8,statistic='median')\n",
    "ax2.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax2.set(xlim=[0,80],ylim=[0,1],\n",
    "       xlabel=r'H$\\alpha$ / FUV',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/'trends_due_to_fesc.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fesc from Ha/FUV\n",
    "\n",
    "If we assume that only H$\\alpha$, but not FUV is affected by leaking radiation, we can use the modeled H$\\alpha$/FUV ratio and compare it to the observed ratio to estiamte $f_\\mathrm{esc}$. \n",
    "\\begin{equation}\n",
    "f_{\\mathrm{esc}} = \\frac{Q_p-Q_o}{Q_p} \n",
    "\\end{equation}\n",
    "with the assumption above, we can also write\n",
    "\\begin{equation}\n",
    "f_{\\mathrm{esc}} = \\frac{(\\mathrm{H}\\alpha/FUV)_p-(\\mathrm{H}\\alpha/FUV)_o}{(\\mathrm{H}\\alpha/FUV)_p} \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "catalogue['HA/FUV_pred'] = np.nan\n",
    "HI_rate = cluster.quanta['HI_rate'].value\n",
    "time = cluster.quanta['Time']\n",
    "for row in tqdm(catalogue):\n",
    "    idx = np.argmin(np.abs(time-row['age']*u.Myr))\n",
    "    row['HA/FUV_pred'] = cluster.ewidth['Luminosity_H_A'][idx].value/cluster.FUV['FUV'][idx].value\n",
    "\n",
    "fesc_HaFUV = (catalogue['HA/FUV_pred']-catalogue['HA/FUV'])/catalogue['HA/FUV_pred']\n",
    "catalogue['fesc_HaFUV'] = fesc_HaFUV\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_HaFUV)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_HaFUV[fesc_HaFUV>0]):.2f} (from {np.sum(fesc_HaFUV>0)} objects)')\n",
    "print(f\"{np.sum(fesc_HaFUV<0)} of {len(catalogue)} ({np.sum(fesc_HaFUV<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "mask = (catalogue['mass']>1e4) & (catalogue['overlap_asc']>0.9) & (catalogue['overlap_neb']>0.1) \n",
    "tmp = catalogue[mask]\n",
    "\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "FUV  = cluster.FUV['FUV']\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "sc = ax1.scatter(tmp['age'],tmp['HA/FUV'])\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],[0.5,10.5],nbins=10)\n",
    "ax1.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "ax1.plot(cluster.FUV['Time'].to(u.Myr).value,Ha/FUV,color='black')\n",
    "ax1.set(xlim=[0,10],ylim=[0,100],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "#ax1.set_title('')\n",
    "#plt.colorbar(sc,ax=ax1)\n",
    "\n",
    "ax2.scatter(tmp['age'],tmp['HA/FUV']/(1-tmp['fesc_HaFUV']))\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV']/(1-tmp['fesc_HaFUV']),[0,10],nbins=10)\n",
    "ax2.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "\n",
    "ax2.plot(cluster.FUV['Time'].to(u.Myr).value,Ha/FUV,color='black')\n",
    "ax2.set(xlim=[0,10],ylim=[0,100],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "ax2.set_title(r'correct $\\mathrm{H}\\alpha/\\mathrm{FUV}$ with $f_\\mathrm{esc}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/'fesc_from_HaFUV.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "mask = (catalogue['mass']>1e4) & (catalogue['overlap_asc']>0.9) & (catalogue['overlap_neb']>0.1) \n",
    "tmp = catalogue[mask]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.scatter(tmp['fesc_HaFUV'],tmp['fesc'])\n",
    "\n",
    "x,mean,std = bin_stat(tmp['fesc_HaFUV'],tmp['fesc'],[0,1],nbins=10,statistic='median')\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "\n",
    "not_nan = ~np.isnan(tmp['fesc_HaFUV']) & ~np.isnan(tmp['fesc'])\n",
    "r,p = spearmanr(tmp[not_nan]['fesc_HaFUV'],tmp[not_nan]['fesc'])\n",
    "t = ax.text(0.75,0.05,r'$\\rho'+f'={r:.2f}$',transform=ax.transAxes,fontsize=10)\n",
    "#t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax.set(xlim=[0,1],ylim=[0,1],\n",
    "       xlabel=r'$f_\\mathrm{esc}$ from H$\\alpha$/FUV',\n",
    "       ylabel=r'$f_\\mathrm{esc}$ from H$\\alpha$')\n",
    "plt.savefig(basedir/'reports'/'compare_fesc.png',dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/20105364/how-can-i-make-a-scatter-plot-colored-by-density-in-matplotlib\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "mask = (catalogue['mass']>1e4) & (catalogue['overlap_asc']>0.9) & (catalogue['overlap_neb']>0.1) \n",
    "tmp = catalogue[mask]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "x,y = tmp['HA/FUV'],tmp['fesc_HaFUV']\n",
    "\n",
    "x,y = x[~np.isnan(x) & ~np.isnan(y)], y[~np.isnan(x) & ~np.isnan(y)]\n",
    "xy = np.vstack([x,y])\n",
    "density = gaussian_kde(xy)(xy)\n",
    "\n",
    "ax.scatter(x,y,cmap=plt.cm.Reds)\n",
    "bins,mean,std = bin_stat(x,y,[0,80],nbins=8,statistic='median')\n",
    "ax.errorbar(bins,mean,fmt='o-',color='black')\n",
    "ax.set(xlim=[0,80],ylim=[0,1.2],\n",
    "       xlabel=r'H$\\alpha$ / FUV',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mask = (catalogue['mass']>1e4) & (catalogue['overlap_asc']>0.9) & (catalogue['overlap_neb']>0.1) \n",
    "tmp = catalogue[mask]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "ax.scatter(tmp['dig/hii'],tmp['fesc'],label='from Ha')\n",
    "x,mean,std = bin_stat(tmp['dig/hii'],tmp['fesc'],[0,1],nbins=10,statistic='median')\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "\n",
    "ax.scatter(tmp['dig/hii'],tmp['fesc_HaFUV'],label='from Ha/FUV')\n",
    "x,mean,std = bin_stat(tmp['dig/hii'],tmp['fesc_HaFUV'],[0,1],nbins=10,statistic='median')\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xlim=[0,1],ylim=[0,1],\n",
    "       xlabel=r'dig / hii',\n",
    "       ylabel=r'$f_\\mathrm{esc}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "is this bad? it could mean that what we observe is due to an age dependance of fesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = tmp[(tmp['fesc']-tmp['fesc_HaFUV'])/tmp['fesc']<0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.plot import corner\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<10)\n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "columns  = ['age','HA/FUV_corr','eq_width','logq_D91','Delta_met_scal']\n",
    "limits   = {'HA/FUV_corr':(-5,105),'age':(3,9),'fesc':(0,1.1),'Delta_met_scal':(-0.1,0.1),\n",
    "            'logq_D91':(6,8),'density':(0,150),'eq_width':(0,200),\n",
    "            'dig/hii':(0.1,0.9),'met_scal':(8.3,8.7)}\n",
    "labels   = {'HA/FUV_corr':r'H$\\alpha$ / FUV','age_new':'age / Myr','fesc':'fesc','eq_width':r'EW(H$\\alpha$)',\n",
    "            'Delta_met_scal':r'$\\Delta$(O/H)','logq_D91':r'$\\log q$','density':'density / cm-3',\n",
    "            'met_scal':'12+logO/H','dig/hii':r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$'}\n",
    "\n",
    "filename = None #basedir/'reports'/f'corner_age_from_fesc.png'\n",
    "\n",
    "corner(tmp,columns,limits,labels=labels,nbins=5,\n",
    "       filename=filename,vmin=1000,vmax=1e6,\n",
    "       figsize=two_column,aspect_ratio=1,s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(tmp['age'],tmp['EBV_balmer']/tmp['EBV_stars'],alpha=0.5)\n",
    "plt.ylim([0,8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ha/FUV vs ionization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with log q from Diaz+91\n",
    "\n",
    "$$\n",
    "\\log u = (-1.6840.076)\\cdot \\log([SII]/[SIII])-(2.986 0.027)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, spearmanr\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "bins = np.linspace(*np.nanpercentile(catalogue['logq_D91'],[1,99]),10)\n",
    "xlim = [5.7,7.8]\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = 'logq_D91','eq_width','HA/FUV_corr','Delta_met_scal'\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "table = nebulae.copy()\n",
    "table = table[(table['HA/FUV_corr']>3*table['HA/FUV_corr_err']) | np.isnan(table['FUV_FLUX'])]\n",
    "#table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "print(f'{len(table)} objects in sample')\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "sample_table.sort('mass')\n",
    "\n",
    "rho1,rho2,rho3 = [], [], []\n",
    "for i,gal_name in enumerate(sample_table['name']):\n",
    "    #print(gal_name)\n",
    "    \n",
    "    tmp = table[table['gal_name']==gal_name]\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "    rho1.append(spearmanr(tmp[x_name],tmp[y1_name],nan_policy='omit')[0])\n",
    "    \n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "    try:\n",
    "        rho2.append(spearmanr(tmp[x_name],tmp[y2_name],nan_policy='omit')[0])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "    rho3.append(spearmanr(tmp[x_name],tmp[y3_name],nan_policy='omit')[0])\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "    \n",
    "\n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "\n",
    "ax1.set(xlim=xlim, yscale='log',ylim=[3,5e2],\n",
    "        xlabel=r'$\\log q$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "ax2.set(xlim=xlim, yscale='log',ylim=[2,2e2],\n",
    "        xlabel=r'$\\log q$',ylabel=r'H$\\alpha$/FUV')\n",
    "ax3.set(xlim=xlim,ylim=[-0.15,0.15],\n",
    "        xlabel=r'$\\log q$',ylabel=r'$\\Delta$(O/H)')\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax3.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "\n",
    "# get a second x axis on top with [SIII]/[SII]\n",
    "xmin,xmax=ax1.get_xlim()\n",
    "ax1_top = ax1.twiny()\n",
    "ax1_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax1_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax1_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "ax2_top = ax3.twiny()\n",
    "ax2_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax2_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax2_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "ax3_top = ax2.twiny()\n",
    "ax3_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax3_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax3_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "for i, rho in enumerate([rho1,rho2,rho3]):\n",
    "    print(f'plot{i}: rho={np.nanmean(rho):.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.6])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$\\log (m/\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "plt.savefig(basedir/'reports'/'nebulae_correlations.pdf',dpi=600)\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram/binned stat with log bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = '[SIII]/[SII]', 'Delta_met_scal','HA/FUV_corr','eq_width'\n",
    "xlim,ylim1,ylim2,ylim3 = [1e-2,1],[-0.15,0.15],[8e-1,8e1],[3,3e2]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3.2))\n",
    "\n",
    "table = nebulae.copy()\n",
    "table = table[(table['HA/FUV_corr']>3*table['HA/FUV_corr_err']) | np.isnan(table['FUV_FLUX'])]\n",
    "table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "sample_table.sort('mass')\n",
    "\n",
    "# the density histogram\n",
    "\n",
    "nbins=20\n",
    "\n",
    "x,y = table[x_name],table[y1_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim1,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax1.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y2_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim2),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax2.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y3_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim3),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax3.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "# the bins for the binned mean\n",
    "#bins = np.logspace(-1.9,-0.2,10)\n",
    "bins = np.logspace(-2.4,-0.4,12)\n",
    "\n",
    "for i,gal_name in enumerate(sample_table['name']):\n",
    "    #print(gal_name)\n",
    "    \n",
    "    tmp = table[table['gal_name']==gal_name]\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "    \n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "\n",
    "ax1.set(xscale='log',xlim=xlim,ylim=ylim1,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "ax2.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim2,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "ax3.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim3,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax1.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.73])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$\\log (m/\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same plot but now binned by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = '[SIII]/[SII]', 'Delta_met_scal','HA/FUV','eq_width'\n",
    "xlim,ylim1,ylim2,ylim3 = [1e-2,1],[-0.15,0.15],[8e-1,8e1],[3,3e2]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3.2))\n",
    "\n",
    "table = catalogue.copy()\n",
    "#table = table[(table['FUV_FLUX_CORR']>3*table['FUV_FLUX_CORR_ERR']) | np.isnan(table['FUV_FLUX_CORR'])]\n",
    "table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=0,vmax=20)\n",
    "\n",
    "\n",
    "nbins=20\n",
    "\n",
    "x,y = table[x_name],table[y1_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim1,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax1.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y2_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim2),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax2.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y3_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim3),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax3.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "# the bins for the binned mean\n",
    "#bins = np.logspace(-1.9,-0.2,10)\n",
    "bins = np.logspace(-2.4,-0.4,12)\n",
    "\n",
    "\n",
    "tmp = catalogue #[catalogue['mass']>1e4]\n",
    "\n",
    "for age in [(0,2),(2,5),(5,10),(10,20)]:\n",
    "    age_mean = np.mean(age)\n",
    "    tmp = table[(table['age']>age[0]) & (table['age']<age[1])]\n",
    "    print(f'{age_mean}: {len(tmp)} objects')\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "    \n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    #ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=0,vmax=12)\n",
    "\n",
    "ax1.set(xscale='log',xlim=xlim,ylim=ylim1,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "ax2.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim2,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "ax3.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim3,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax1.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.73])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'age / Myr',ticks=np.arange(0,20,4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations_age_bin.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a dedicated subplot for each galaxie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, pearsonr, spearmanr\n",
    "\n",
    "sample = set(astrosat_sample) & set(muse_sample)\n",
    "filename = basedir/'reports'/'all_objects_HaFUV_over_SII'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "    \n",
    "#vmin,vmax = np.min(catalogue['HA6562_FLUX_CORR']),np.max(catalogue['HA6562_FLUX_CORR'])\n",
    "vmin,vmax = 1e-16,1e-14\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample):  \n",
    "    \n",
    "    tmp = nebulae[(nebulae['gal_name']==name)]\n",
    "        \n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    tmp = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err']) | np.isnan(tmp['FUV_FLUX'])]\n",
    "    tmp = tmp[tmp['[SIII]/[SII]']>3*tmp['[SIII]/[SII]_err']]\n",
    "\n",
    "    r,p = spearmanr(tmp['[SIII]/[SII]'],tmp['HA/FUV_corr'])\n",
    "    print(f'{name}: rho={r:.2f}, {len(tmp)} objects')\n",
    "\n",
    "    sc = ax.scatter(tmp['[SIII]/[SII]'],tmp['HA/FUV_corr'],\n",
    "               c=1e-20*tmp['HA6562_FLUX_CORR'],vmin=vmin,vmax=vmax,\n",
    "               cmap=plt.cm.plasma,\n",
    "               norm=mpl.colors.LogNorm(),\n",
    "               s=1,marker='.')\n",
    "    \n",
    "    #q = 98\n",
    "    #bins = np.logspace(*np.log10(np.percentile(tmp['[SIII]/[SII]'],[100-q,q])),10)\n",
    "    #x,mean,std = bin_stat(tmp['[SIII]/[SII]'],tmp['HA/FUV'],[None,None],nbins=bins)\n",
    "    #ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label=gal_name)\n",
    "    \n",
    "    ax.text(0.05,0.9,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.75,0.15,r'$\\rho$'+f'={r:.2f}',transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.62,0.05,f'{len(tmp):.0f} objects', transform=ax.transAxes,fontsize=7)\n",
    "    \n",
    "    ax.set(xscale='log',yscale='log',xlim=[1e-2,1],ylim=[2,4e2])\n",
    "    # https://stackoverflow.com/questions/21920233/matplotlib-log-scale-tick-label-number-formatting/33213196\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(r'H$\\alpha$ / FUV')\n",
    "\n",
    "        \n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    \n",
    "    if i==0:\n",
    "        #ax.remove()\n",
    "        ax.axis('off')\n",
    "        cbar = fig.colorbar(sc, ax=ax,\n",
    "                            label=r'$\\mathrm{H}\\alpha$ / (erg s$^{-1}$ cm$^{-2}$ Hz$^{-1}$)',\n",
    "                            orientation='horizontal',\n",
    "                           )\n",
    "    else:\n",
    "        ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "\n",
    "\n",
    "#plt.savefig(filename.with_suffix('.png'),dpi=600)\n",
    "#plt.savefig(filename.with_suffix('.pdf'),dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nebulae[(nebulae['gal_name']=='NGC4254')]\n",
    "#tmp = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err']) | np.isnan(tmp['FUV_FLUX'])]\n",
    "tmp = tmp[tmp['[SIII]/[SII]']>3*tmp['[SIII]/[SII]_err']]\n",
    "\n",
    "tmp[['HA/FUV_corr','HA/FUV_corr_err']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0.5,10.5,1)\n",
    "\n",
    "tmp = associations[(associations['mass']>1e4) & (associations['age']<=10)]\n",
    "print(len(tmp))\n",
    "ages_con = tmp[tmp['overlap']=='contained']['age']\n",
    "ages_par = tmp[tmp['overlap']=='partial']['age']\n",
    "ages_iso = tmp[(tmp['overlap']=='isolated') & (tmp['in_frame'])]['age']\n",
    "\n",
    "print(f'ages: con={np.mean(ages_con):.2f}, par={np.mean(ages_par):.2f}, iso={np.mean(ages_iso):.2f}')\n",
    "\n",
    "n1,_,_=ax1.hist(ages_con,bins=bins,histtype='step',label='contained')\n",
    "n2,_,_=ax2.hist(ages_par,bins=bins,histtype='step',label='partially')\n",
    "n3,_,_=ax3.hist(ages_iso,bins=bins,histtype='step',label='isolated')\n",
    "\n",
    "ax1.set_title(f'contained ({np.nanmean(ages_con):.2f} Myr)')\n",
    "ax2.set_title(f'partial ({np.nanmean(ages_par):.2f} Myr)')\n",
    "ax3.set_title(f'isolated ({np.nanmean(ages_iso):.2f} Myr)')\n",
    "\n",
    "ymax = np.max([n1,n2,n3])\n",
    "ymax = np.round(ymax,-int(np.log10(ymax))+1)\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,ymax],xlim=[0,10],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'tmp_age_hist_contained.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "tmp = associations[(associations['mass']>1e4)]\n",
    "idx,sep,_=match_coordinates_sky(tmp['SkyCoord_asc'],nebulae['SkyCoord_neb'])\n",
    "\n",
    "ages1 = tmp[(sep<0.4*u.arcsec)]['age']\n",
    "ages2 = tmp[(sep>0.4*u.arcsec) & (sep<0.8*u.arcsec)]['age']\n",
    "ages3 = tmp[(sep>0.8*u.arcsec)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'$s<0.4\"$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'$0.4\"<s<0.8\"$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'$0.8\"<s$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,1100],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_sep.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['age']<10)]\n",
    "\n",
    "p1,p2=np.nanpercentile(tmp['eq_width'],[33,66])\n",
    "\n",
    "ages1 = tmp[tmp['eq_width']<p1]['age']\n",
    "ages2 = tmp[(tmp['eq_width']>p1) & (tmp['eq_width']<p2)]['age']\n",
    "ages3 = tmp[(tmp['eq_width']>p2)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'1$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'2$^\\mathrm{nd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'3$^\\mathrm{rd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,160],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_eq_width.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue[(catalogue['mass']>1e4)]\n",
    "#catalogue[(catalogue['mass']>1e4) & (catalogue['age']<10)]\n",
    "\n",
    "p1,p2=np.nanpercentile(tmp['eq_width'],[33,66])\n",
    "\n",
    "ages1 = tmp[tmp['eq_width']<p1]['age']\n",
    "ages2 = tmp[(tmp['eq_width']>p1) & (tmp['eq_width']<p2)]['age']\n",
    "ages3 = tmp[(tmp['eq_width']>p2)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'1$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'2$^\\mathrm{nd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'3$^\\mathrm{rd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,150],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_eq_width.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass-to-Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from scipy.stats import spearmanr, gaussian_kde, binned_statistic, binned_statistic_2d\n",
    "\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.2))\n",
    "\n",
    "x,y,z = tmp['mass'],tmp['HA6562_LUM_CORR'],tmp['age']\n",
    "\n",
    "\n",
    "xlim =[1e2,1e5]\n",
    "ylim = [5e35,8e39]\n",
    "nbins = 25\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim),nbins)]\n",
    "\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins)\n",
    "stat, x_e, y_e,_ = binned_statistic_2d(x,y,z,bins=bins)\n",
    "#stat[hist<5] = np.nan\n",
    "img = ax.pcolormesh(x_e,y_e,stat.T,cmap=plt.cm.viridis,vmin=0,vmax=10)\n",
    "\n",
    "#sc = ax.scatter(x,y,c=z,vmin=0,vmax=10)\n",
    "x_stat,mean,std = bin_stat(x,y,xlim,nbins=bins[0],statistic='median')\n",
    "ax.errorbar(x_stat,mean,fmt='o-',ms=1.5,color='white')\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax.text(0.05,0.93,label,transform=ax.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "fig.colorbar(img,label='age / Myr')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'tmp_mass_HA_age.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax1=plt.subplots(nrows=1,figsize=(single_column,single_column))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<=8) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "vmin,vmax = 0,12\n",
    "nbins = 8\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',6)\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "rho = []\n",
    "age_bins = [0,2,4,6,8,10,12]\n",
    "for i in range(len(age_bins)-1):\n",
    "\n",
    "    sub = tmp[(tmp['age']>=age_bins[i]) & (tmp['age']<age_bins[i+1])]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    x,mean,std = bin_stat(x,y,xlim,nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_bins[i]+1)))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=19*[1],cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'tmp_mass_HA.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import spearmanr, binned_statistic, binned_statistic_2d\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(nrows=2,figsize=(single_column,single_column*2))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "criteria &= (catalogue['age']<=8) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "nbins = 8\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "rho = []\n",
    "for k,gal_name in enumerate(np.unique(sample_table['name'])):\n",
    "\n",
    "    sub = tmp[tmp['gal_name']==gal_name]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    if len(x)>5:\n",
    "        rho.append(spearmanr(x,y,nan_policy='omit')[0]) \n",
    "    x,mean,std = bin_stat(x,y,xlim,nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "#x,mean,std = bin_stat(x,y,[None,None],nbins=bins,statistic='median')\n",
    "#ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')    \n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['distance'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='log M / M$_\\odot$',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "# ----------------------- ax2 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'ax1: {len(tmp)} objects')\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis',10)\n",
    "\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax2.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=cmap)\n",
    "EBV_balmer_err = np.mean(tmp['EBV_balmer_err'])\n",
    "EBV_stars_err = np.mean(tmp['EBV_stars_err'])\n",
    "ax2.errorbar(0.5,0.1,xerr=EBV_stars_err,yerr=EBV_balmer_err,fmt='ko',ms=0)\n",
    "\n",
    "ax2.plot([0,1],[0,2],color='black')\n",
    "ax2.plot([0,2],[0,2],color='black')\n",
    "ax2.set(xlim=[0,0.7],ylim=[0,0.7],xlabel=r'$E(B-V)$ stars',ylabel=r'$E(B-V)$ Balmer')\n",
    "\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar.set_ticks([0,2,4,6,8,10])\n",
    "cbar.set_ticklabels([0,2,4,6,8,'10+'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'mass_HA_EBV.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H$\\alpha$ luminosity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "tmp = nebulae[(nebulae['overlap_neb']==0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='isolated')\n",
    "\n",
    "tmp = nebulae[(nebulae['overlap_neb']>0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='matched')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='linear',ylim=[1,2e3],ylabel='N')\n",
    "plt.savefig(basedir/'reports'/'L(Ha).png',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neb_overlap = np.sum((nebulae['overlap_neb']>0) & (nebulae['in_frame'])) \n",
    "neb_in_frame = np.sum(nebulae['in_frame'])\n",
    "print(f'{neb_overlap} of {neb_in_frame} HII regions overlap ({100*neb_overlap/neb_in_frame:.1f}%)')\n",
    "\n",
    "asc_overlap = np.sum((associations['overlap_asc']>0) & (associations['in_frame'])) \n",
    "asc_in_frame = np.sum(associations['in_frame'])\n",
    "print(f'{asc_overlap} of {asc_in_frame} associations overlap ({100*asc_overlap/asc_in_frame:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.linspace(0.5,10.5,10)\n",
    "\n",
    "tmp = associations[(associations['overlap_asc']==1) & (associations['in_frame'])]\n",
    "print(np.mean(tmp['age']))\n",
    "ax.hist(tmp['age'],bins=bins,alpha=0.6,label='contained')\n",
    "\n",
    "tmp = associations[(associations['overlap_asc']<1) & (associations['overlap_asc']>0) & (associations['in_frame'])]\n",
    "print(np.mean(tmp['age']))\n",
    "ax.hist(tmp['age'],bins=bins,alpha=0.6,label='partial')\n",
    "\n",
    "tmp = associations[(associations['overlap_asc']==0) & (associations['in_frame'])]\n",
    "print(np.mean(tmp['age']))\n",
    "ax.hist(tmp['age'],bins=bins,alpha=0.6,label='isolated')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='linear',xlim=[1,10],xlabel=r'age / Myr',\n",
    "       yscale='linear',ylabel='N')\n",
    "#plt.savefig(basedir/'reports'/'age_hist.png',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EBV\n",
    "\n",
    "we expect \n",
    "$$\n",
    "E(B-V)_{balmer} = 2 \\cdot E(B-V)_{stars}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis',10)\n",
    "fig = plt.figure(figsize=(single_column,single_column/1.1))\n",
    "ax = fig.add_subplot()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=\"10%\", pad=0.2,)\n",
    "\n",
    "xlim = [0,10]\n",
    "#ax.errorbar(tmp['EBV_stars'],tmp['EBV_balmer'],xerr=tmp['EBV_stars_err'],yerr=tmp['EBV_balmer_err'],ms=0,fmt='ok')\n",
    "sc=ax.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=cmap)\n",
    "#x,mean,std = bin_stat(tmp['EBV_stars'],tmp['EBV_balmer'],[0,0.7],nbins=7,statistic='mean')\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='red')\n",
    "EBV_balmer_err = np.median(tmp['EBV_balmer_err'])\n",
    "EBV_stars_err = np.median(tmp['EBV_stars_err'])\n",
    "ax.errorbar(0.5,0.1,xerr=EBV_stars_err,yerr=EBV_balmer_err,fmt='ko',ms=0)\n",
    "\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel=r'$E(B-V)$ stars',ylabel=r'$E(B-V)$ Balmer')\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "cbar.set_ticks([0,2,4,6,8,10])\n",
    "cbar.set_ticklabels([0,2,4,6,8,'10+'])\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'EBV_Balmer_vs_Stars_{HSTband}_{scalepc}pc.pdf',dpi=600)\n",
    "#plt.savefig(basedir/'reports'/f'EBV_Balmer_vs_Stars_{HSTband}_{scalepc}pc.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "age_bins = np.array([0,2,4,6,8,10])\n",
    "\n",
    "for low,high in zip(age_bins[:-1],age_bins[1:]):\n",
    "    t = tmp[(tmp['age']>low) & (tmp['age']<high)]\n",
    "    r,p = spearmanr(t['EBV_stars'],t['EBV_balmer'])\n",
    "    print(f'{low} to {high} Myr: rho={r:.2f}, {len(t)} objects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.scatter(catalogue['age'],catalogue['density'])\n",
    "x,mean,std = bin_stat(catalogue['age'],catalogue['density'],[0,10],nbins=10,statistic='median')\n",
    "ax.errorbar(x,mean,yerr=None,fmt='-',color='black')\n",
    "\n",
    "ax.set(xlim=[0,10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = 4\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=2)\n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "print(f'{np.sum(criteria)} objects are 2 Myr or younger')\n",
    "\n",
    "tmp1 = catalogue[criteria & (catalogue['GMC_sep']>=sep)]\n",
    "print(f'{len(tmp1)} not associated with GMC')\n",
    "print(f'EW(Ha): {np.mean(tmp1[\"eq_width\"]):.1f}')\n",
    "print(f'Ha/FUV: {np.nanmean(tmp1[\"HA/FUV_corr\"][np.isfinite(tmp1[\"HA/FUV_corr\"])]):.1f}\\n')\n",
    "\n",
    "tmp2 = catalogue[criteria & (catalogue['GMC_sep']<sep)]\n",
    "print(f'{len(tmp2)} associated with GMC')\n",
    "print(f'EW(Ha): {np.mean(tmp2[\"eq_width\"]):.1f}')\n",
    "print(f'Ha/FUV: {np.nanmean(tmp2[\"HA/FUV_corr\"][np.isfinite(tmp2[\"HA/FUV_corr\"])]):.1f}\\n')\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['overlap_asc']==1)]\n",
    "print(f\"age GMC: {np.mean(tmp[tmp['GMC_sep']<sep]['age']):.2f}\")\n",
    "print(f\"age without GMC: {np.mean(tmp[tmp['GMC_sep']>=sep]['age']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define limits and labels for corner plots\n",
    "limits   = {'age':(0.5,8.5),\n",
    "            'log_age':(5.9,7),\n",
    "            'age_mw':(0,20),\n",
    "            'met_scal':(8.3,8.7),\n",
    "            'Delta_met_scal':(-0.07,0.07),\n",
    "            'density':(0,150),\n",
    "            'dig/hii':(0.2,0.9),\n",
    "            'fesc':(0,1),\n",
    "            'HA/FUV':(0,80),\n",
    "            'HA/NUV':(0,50),\n",
    "            'HA/FUV_corr':(0,80),\n",
    "            'log_HA':(4.5,7.4),\n",
    "            'EBV_balmer':(0,1),\n",
    "            'sb_HA_pc':(0,200),\n",
    "            'sb_HA_arcsec':(0,1e5),\n",
    "            'log[SIII]/[SII]':(-1.5,0),\n",
    "            'OIII/HB':(0,1),\n",
    "            'logq_D91':(6.2,7.8),\n",
    "            'eq_width':(10,150),\n",
    "            'log_eq_width':(1.2,2.6),\n",
    "            'temperature':(6000,8e3),\n",
    "            'T_N2_REFIT':(6000,8e3),\n",
    "            'log_mass':(4,5.5)}\n",
    "\n",
    "labels   = {'age':'age / Myr',\n",
    "            'log_age' : r'$\\log (\\mathrm{age / Myr})$',\n",
    "            'age_mw':'age (stellar pops) / Myr',\n",
    "            'met_scal':'12+logO/H',\n",
    "            'Delta_met_scal':r'$\\Delta$(O/H)',\n",
    "            'density':r'density / cm$^{-3}$','temperature':'T / K',\n",
    "            'fesc':r'$f_\\mathrm{esc}$',\n",
    "            'HA/FUV':r'H$\\alpha$ / FUV',\n",
    "            'HA/FUV_corr':r'H$\\alpha$ / FUV',\n",
    "            'OIII/HB':r'$[\\mathrm{O}\\,\\textsc{iii}]/\\mathrm{H}\\beta$',\n",
    "            'log_HA':r'$\\log \\mathrm{H}\\alpha$ ',\n",
    "            'EBV_balmer':r'$E(B-V)_\\mathrm{Balmer}$',\n",
    "            'sb_HA':r'$SB_{\\mathrm{H}\\alpha}$',\n",
    "            'eq_width' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$',\n",
    "            'log_eq_width':r'$\\log (\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA})$',\n",
    "            'logq_D91':r'$\\log q$',\n",
    "            'log[SIII]/[SII]':r'$\\log([\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}])$',\n",
    "            'T_N2_REFIT':'temperature / K',\n",
    "            'log_mass':r'$\\log M/\\mathrm{M}_\\odot$'}\n",
    "scale = {'eq_width':'log','HA/FUV':'log'}\n",
    "\n",
    "# calculate a few additional columns\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    catalogue['log_age'] = 6+np.log10(catalogue['age'])\n",
    "    catalogue['log_eq_width'] = np.log10(catalogue['eq_width'])\n",
    "    catalogue['log_HA'] = np.log10(catalogue['HA6562_FLUX_CORR'])\n",
    "    area_per_pixel = ((0.2*u.arcsec).to(u.rad).value*catalogue['distance']*u.Mpc).to(u.pc)**2\n",
    "    catalogue['sb_HA_pc']=catalogue['HA6562_FLUX_CORR']/(catalogue['area_neb']*area_per_pixel.value)\n",
    "    catalogue['sb_HA_arcsec']=catalogue['HA6562_FLUX_CORR']/(catalogue['area_neb']*0.2**2)\n",
    "    catalogue['log[SIII]/[SII]'] = np.log10(catalogue['[SIII]/[SII]'])\n",
    "    catalogue['log_mass'] = np.log10(catalogue['mass'])\n",
    "    catalogue['OIII/HB'] = catalogue['OIII5006_FLUX']/catalogue['HB4861_FLUX_CORR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlations with age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['HA/FUV']>3*catalogue['HA/FUV_err']) | np.isnan(catalogue['FUV_FLUX'])\n",
    "#criteria &= catalogue['[SIII]/[SII]']>3*catalogue['[SIII]/[SII]_err']\n",
    "criteria &= (catalogue['age']<=8)\n",
    "# young objects should be associated with a GMC\n",
    "#criteria &= ((catalogue['GMC_sep']<4) | (catalogue['age']>2))\n",
    "#criteria &= catalogue['U_dolmag_vega']-catalogue['B_dolmag_vega']<-1.\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "tmp['HA/FUV_corr'][tmp['FUV_FLUX_CORR']/tmp['FUV_FLUX_CORR_ERR']<3] = np.nan\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "columns  = ['age','eq_width','HA/FUV_corr','logq_D91','Delta_met_scal']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=filename,histogram=False,\n",
    "           figsize=two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlations in the nebulae catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds,rasterized=True)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "\n",
    "tmp = nebulae.copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "tmp['log_HA'] = np.log10(tmp['HA6562_FLUX_CORR'])\n",
    "\n",
    "columns  = ['eq_width','HA/FUV_corr','log_HA','T_N2_REFIT','density','EBV_balmer']\n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=basedir/'reports'/f'corner_large.pdf',\n",
    "           figsize=2*two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_by_galaxy\n",
    "\n",
    "columns  = ['age','log_eq_width','HA/FUV_corr','logq_D91','Delta_met_scal']\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner_by_galaxy(tmp[tmp['gal_name']!='NGC1365'],sample_table=sample_table,columns=columns,\n",
    "           limits=limits,labels=labels,nbins=5,filename=None,\n",
    "           figsize=two_column,aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histograms binned by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>=1e4) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= ((catalogue['GMC_sep']<2) | (catalogue['age']>2))\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4,figsize=(1.5*two_column,1.5*two_column/3))\n",
    "\n",
    "style = {'alpha':0.5}\n",
    "nbins = 10\n",
    "age_bins = [0,2,8]\n",
    "for i in range(len(age_bins)-1):\n",
    "    sub = tmp[(tmp['age']>age_bins[i]) & (tmp['age']<=age_bins[i+1])]\n",
    "    label = f'{age_bins[i]}\\> age \\>= {age_bins[i+1]} Myr: '\n",
    "    \n",
    "    ax1.hist(sub['eq_width'],bins=np.linspace(*limits['eq_width'],nbins),**style,\n",
    "             label=label+f\"{np.nanmean(sub['eq_width']):.1f}\")\n",
    "    ax2.hist(sub['HA/FUV_corr'],bins=np.linspace(*limits['HA/FUV_corr'],nbins),**style,\n",
    "            label=label+f\"{np.nanmedian(sub['HA/FUV_corr']):.1f}\")\n",
    "    ax3.hist(sub['Delta_met_scal'],bins=np.linspace(*limits['Delta_met_scal'],nbins),**style,\n",
    "            label=label+f\"{np.nanmean(sub['Delta_met_scal']):.2f}\")\n",
    "    ax4.hist(sub['logq_D91'],bins=np.linspace(*limits['logq_D91'],nbins),**style,\n",
    "             label=label+f\"{np.nanmean(sub['logq_D91']):.1f}\")\n",
    "    \n",
    "ax1.set(xlabel=labels['eq_width'],xlim=limits['eq_width'])\n",
    "ax2.set(xlabel=labels['HA/FUV_corr'],xlim=limits['HA/FUV_corr'])\n",
    "ax3.set(xlabel=labels['Delta_met_scal'],xlim=limits['Delta_met_scal'])\n",
    "ax4.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])\n",
    "ax1.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax2.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax3.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax4.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with starburst99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_CSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'continuous_SF')\n",
    "cluster_SSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'single_burst_SF')\n",
    "cluster_SSF.measure_FUV()\n",
    "cluster_CSF.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Halpha/FUV\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['overlap_asc']==1)]\n",
    "ax1.scatter(tmp['age'],tmp['HA/FUV_corr'],color='gray')\n",
    "\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_CSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = np.interp(cluster_CSF.ewidth['Time'],cluster_CSF.FUV['Time'],cluster_CSF.FUV['FUV'])\n",
    "ax1.plot(time,Ha/FUV,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_SSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster_SSF.FUV['FUV'].copy()\n",
    "ax1.plot(time,Ha/FUV,label='single burst')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],ylim=[0,100],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "\n",
    "ax2.scatter(tmp['age'],tmp['eq_width'],color='gray')\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_CSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,0.1*eq_width,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_SSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,0.1*eq_width,label='singel burst')\n",
    "ax2.legend()\n",
    "ax2.set(xlim=[0,10],ylim=[0,400],yscale='linear',xlabel='age / Myr',ylabel=r'EW(H$\\alpha$)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the distribution of EW observed vs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_solar = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster_subsolar = Cluster(stellar_model='GENEVAv40',metallicity=0.004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue.copy()\n",
    "\n",
    "eq_width_model_solar = cluster_solar.ewidth['eq_width_H_A']\n",
    "eq_width_model_subsolar = cluster_subsolar.ewidth['eq_width_H_A']\n",
    "age_model = cluster_solar.ewidth['Time']\n",
    "\n",
    "tmp['eq_width_model_solar'] = np.nan\n",
    "tmp['eq_width_model_subsolar'] = np.nan\n",
    "for row in tmp:\n",
    "    idx = np.argmin(np.abs(age_model-row['age']*u.Myr))\n",
    "    row['eq_width_model_solar'] = eq_width_model_solar[idx].value\n",
    "    row['eq_width_model_subsolar'] = eq_width_model_subsolar[idx].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(0,4,15)\n",
    "\n",
    "ax.hist(tmp['eq_width'],bins=bins,label='observed',alpha=0.6)\n",
    "ax.hist(tmp['eq_width_model_solar'],bins=bins,label='model solar',alpha=0.6)\n",
    "#ax.hist(tmp['eq_width_model_subsolar'],bins=bins,label='model subsolar',alpha=0.6)\n",
    "ax.legend()\n",
    "ax.set(xscale='log',xlim=[1,1e4],xlabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other things like violine plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['HA/FUV']>3*catalogue['HA/FUV_err']) | np.isnan(catalogue['FUV_FLUX'])\n",
    "#criteria &= catalogue['[SIII]/[SII]']>3*catalogue['[SIII]/[SII]_err']\n",
    "criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "# young objects should be associated with a GMC\n",
    "#criteria &= ((catalogue['GMC_sep']<4) | (catalogue['age']>2))\n",
    "#criteria &= catalogue['U_dolmag_vega']-catalogue['B_dolmag_vega']<-1.\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['log_age'] = 6+np.log10(tmp['age'])\n",
    "    tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "    tmp['log[SIII]/[SII]'] = np.log10(tmp['[SIII]/[SII]'])\n",
    "\n",
    "columns  = ['logq_D91','Delta_met_scal','HA/FUV_corr','log_eq_width']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=None,\n",
    "           figsize=two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "\n",
    "x = tmp['r_R25']\n",
    "y = tmp['log_eq_width']\n",
    "\n",
    "ax.scatter(x,y,c='gray')\n",
    "corner_binned_stat(x,y,ax)\n",
    "corner_spearmanr(x,y,ax,pvalue=True)\n",
    "\n",
    "ax.set(xlim=[0,1],ylim=[0,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a violin plot to show the distribution of points in each age bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "x_label = 'age'\n",
    "y_label = 'eq_width'\n",
    "\n",
    "x,y = tmp[x_label], tmp[y_label]\n",
    "x,y = x[~np.isnan(y)],y[~np.isnan(y)]\n",
    "x,y = x[(x>=limits[x_label][0]) & (x<=limits[x_label][1]) & (y>=limits[y_label][0]) & (y<=limits[y_label][1])], \\\n",
    "      y[(x>=limits[x_label][0]) & (x<=limits[x_label][1]) & (y>=limits[y_label][0]) & (y<=limits[y_label][1])]\n",
    "\n",
    "positions = np.arange(0,10)\n",
    "\n",
    "#binned_data = corner_violin(x,y,ax,positions,showmedians=True)\n",
    "\n",
    "bins = (positions[1:]+positions[:-1])/2\n",
    "binned_data = [y[(bins[i]<x) & (x<bins[i+1])] for i in range(len(bins)-1)]\n",
    "ax.violinplot(binned_data,positions=positions[1:-1],showmeans=True)\n",
    "\n",
    "ax.set(xlabel=x_label,ylabel=y_label.replace('_',''))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter age with color-color-diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[-0.33 , -1.662],[-0.307, -1.577],[-0.25 , -1.468],[-0.218, -1.394],[-0.186, -1.321],\n",
    "                   [-0.138, -1.235],[-0.076, -1.213],[-0.007, -1.206],[ 0.065, -1.197],[ 0.135, -1.187],\n",
    "                   [ 0.203, -1.198],[ 0.276, -1.201],[ 0.347, -1.209],[ 0.418, -1.219],[ 0.488, -1.233],\n",
    "                   [ 0.559, -1.251],[ 0.63 , -1.27 ],[ 0.706, -1.264],[ 0.771, -1.287],[ 0.648, -1.119],\n",
    "                   [ 0.626, -1.026],[ 0.565, -0.989],[ 0.559, -0.87 ],[ 0.502, -0.807],[ 0.481, -0.706],\n",
    "                   [ 0.457, -0.546],[ 0.505, -0.449],[ 0.487, -0.384],[ 0.488, -0.328],[ 0.468, -0.239],\n",
    "                   [ 0.517, -0.109],[ 0.555, -0.015],[ 0.6  ,  0.041],[ 0.65 ,  0.132],[ 0.724,  0.174],\n",
    "                   [ 0.864,  0.169],[ 0.948,  0.166],[ 1.02 ,  0.152],[ 1.138,  0.182],[ 1.203,  0.194],\n",
    "                   [ 1.253,  0.282],[ 1.331,  0.378],[ 1.354,  0.469]])\n",
    "\n",
    "x,y=points.T\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(2.3*single_column,1*single_column))\n",
    "\n",
    "tmp = associations\n",
    "#ax.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],s=0.5)\n",
    "tmp = catalogue\n",
    "sc=ax1.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['GMC_sep'],vmin=0,vmax=10,\n",
    "           s=0.5)\n",
    "ax1.plot(x,y,color='red')\n",
    "ax1.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "ax1.invert_yaxis()\n",
    "plt.colorbar(sc,label='GMC sep / arcsec',ax=ax1)\n",
    "\n",
    "sc=ax2.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['age'],vmin=0,vmax=20,cmap=plt.cm.plasma,\n",
    "           s=0.5)\n",
    "ax2.plot(x,y,color='red')\n",
    "ax2.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "ax2.invert_yaxis()\n",
    "fig.colorbar(sc,label='age / Myr',ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('UBVI_matched_associations.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "bins=np.arange(0,150,10)\n",
    "tmp = catalogue[(catalogue['age']==1) & (catalogue['overlap_asc']==1)]\n",
    "mask = tmp['GMC_sep']<4\n",
    "\n",
    "ax.hist(tmp['eq_width'][mask],bins=bins,label=f'nearby GMC: {np.mean(tmp[\"eq_width\"][mask]):.2f} AA',histtype='step',alpha=0.8)\n",
    "ax.hist(tmp['eq_width'][~mask],bins=bins,label=f'no GMC {np.mean(tmp[\"eq_width\"][~mask]):.2f} AA',histtype='step',alpha=0.8)\n",
    "ax.set(xlabel=r'EW(H$\\alpha$)')\n",
    "ax.set_title('young associations (age 1 Myr)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "\n",
    "# define limits and labels for corner plots\n",
    "limits   = {'age':(0,20),\n",
    "            'UB':(-2.0,0),\n",
    "            'CO':(0,200),\n",
    "            'GMC_sep':(0,10)}\n",
    "\n",
    "labels   = {'age':'age / Myr',\n",
    "            'UB':r'$U-B$',\n",
    "            'CO':r'CO flux',\n",
    "            'GMC_sep':r'GMC sep / arcsec'}\n",
    "\n",
    "                             \n",
    "tmp = catalogue.copy()\n",
    "#tmp = tmp[tmp['GMC_sep']<5]\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['UB'] = tmp['U_dolmag_vega']-tmp['B_dolmag_vega']\n",
    "\n",
    "columns  = ['age','GMC_sep','CO','UB']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=None,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=None,\n",
    "           figsize=two_column,aspect_ratio=1,s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the old nebulae with high EW might come from Wolf Rayet Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the old nebulae with high EW might come from Wolf Rayet Stars\n",
    "WR_candidates = catalogue[(catalogue['age']>6) & (catalogue['age']<10) & (catalogue['eq_width']>80) & (catalogue['mass']>5e3)]\n",
    "print(f'{len(WR_candidates)} WR candidates')\n",
    "groups = WR_candidates.group_by('gal_name')\n",
    "spectra = []\n",
    "for key, group in zip(groups.groups.keys, groups.groups):\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spectra'/f'{key[\"gal_name\"]}_VorSpectra.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        tbl = Table(hdul[1].data)\n",
    "        spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    tbl['region_ID'] = np.arange(len(tbl))\n",
    "    tbl.add_index('region_ID')\n",
    "    for region_ID in group['region_ID']:\n",
    "        spectra.append((key['gal_name'],region_ID,spectral_axis,tbl.loc[region_ID]['SPEC']))\n",
    "        \n",
    "        hdu = fits.BinTableHDU(WR_candidates[['gal_name','region_ID','assoc_ID','x_neb','y_neb','age','mass','eq_width']],\n",
    "                       name='WR_candidates')\n",
    "hdu.writeto(basedir/'data'/'WR_candidates.fits',overwrite=True)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "filename = basedir/'reports'/'WR_candidates_spectra'\n",
    "nrows=6\n",
    "ncols=1\n",
    "width = two_column\n",
    "N = len(WR_candidates)\n",
    "Npage = nrows*ncols\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = spectra[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width*1.41))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            gal_name,region_ID,spectral_axis,spectrum = row\n",
    "            ax = next(axes_iter)\n",
    "            ax.plot(spectral_axis,spectrum,lw=0.5)\n",
    "            ax.set(xlim=[4860,6800],yscale='log',ylim=[1e2,1e6])\n",
    "            t = ax.text(0.01,0.87,f'{gal_name}: {region_ID:.0f}', transform=ax.transAxes,color='black',fontsize=8)\n",
    "        plt.subplots_adjust(wspace=-0.1, hspace=0)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "            h,l = fig.axes[0].get_legend_handles_labels()\n",
    "            ax = next(axes_iter)\n",
    "            ax.axis('off')\n",
    "            #ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "            #t = ax.text(0.06,0.87,'galaxy name: region ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use elipses for uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "ellipses = [Ellipse((row['age'],row['HA/FUV']),\n",
    "                     row['age_err'],row['HA/FUV_err'],\n",
    "                    alpha=0.5) for row in tmp]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for e in ellipses:\n",
    "    ax.add_artist(e)\n",
    "\n",
    "ax.set(xlim=(0, 10),ylim=(0,80),xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "ax.errorbar(tmp['age'],tmp['HA/FUV'],xerr=tmp['age_err'],yerr=tmp['HA/FUV_err'],fmt='o')\n",
    "\n",
    "ax.set(xlim=(0, 10),ylim=(0,80),xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look for correlations by comparing all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=8)\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['log_age'] = 6+np.log10(tmp['age'])\n",
    "    tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "    tmp['log[SIII]/[SII]'] = np.log10(tmp['[SIII]/[SII]'])\n",
    "print(len(tmp))\n",
    "    \n",
    "columns = ['HA/FUV','eq_width','Delta_met_scal','logq_D91','age','mass',\n",
    "           'EBV_balmer','EBV_stars','GMC_sep','density','temperature']\n",
    "\n",
    "correlation = []\n",
    "pairs = []\n",
    "for a,b in combinations(columns,2):\n",
    "    y,x = tmp[a],tmp[b] \n",
    "    not_nan = ~np.isnan(x) & ~np.isnan(y)\n",
    "    r,p = spearmanr(x[not_nan],y[not_nan])\n",
    "    correlation.append(r)\n",
    "    pairs.append(a+'+'+b)\n",
    "correlation = Table([correlation,pairs],names=['r','pair'])\n",
    "correlation.sort('r')\n",
    "\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits['EBV_balmer'] = (0,1)\n",
    "limits['EBV_stars'] = (0,1)\n",
    "limits['GMC_sep'] = (0,10)\n",
    "\n",
    "corner(tmp,['EBV_balmer','EBV_stars','HA/FUV','eq_width','GMC_sep'],function=plot_function,\n",
    "       limits=limits,labels=labels,\n",
    "       filename=None,\n",
    "       figsize=two_column,aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends with escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from cluster.plot import corner\n",
    "\n",
    "mass_slider = widgets.FloatSlider(\n",
    "    value=3.7,\n",
    "    min=2,\n",
    "    max=4.5,\n",
    "    step=0.2,\n",
    "    description='Cluster mass:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    ")\n",
    "\n",
    "age_slider = widgets.FloatLogSlider(\n",
    "    value=10,\n",
    "    min=1,\n",
    "    max=20,\n",
    "    step=1,\n",
    "    description='Cluster age:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    ")\n",
    "    \n",
    "def plot_corner(mass,age):\n",
    "    criteria  = (catalogue['mass']>mass) \n",
    "    criteria &= (catalogue['age']<age) \n",
    "    criteria &= (catalogue['overlap_neb']>0.1) \n",
    "    criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "    tmp = catalogue[criteria].copy()\n",
    "    print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "    filename = None #basedir/'reports'/f'all_galaxies_corner.png'\n",
    "    columns  = ['age','HA/FUV','density','fesc','dig/hii','logq_D91','Delta_met_scal']\n",
    "    limits   = {'HA/FUV':(0,100),'age':(0,10),'fesc':(0,1.1),'Delta_met_scal':(-0.1,0.1),\n",
    "                'logq_D91':(6,8),'density':(0,250),\n",
    "                'dig/hii':(0.2,0.9),'met_scal':(8.3,8.7)}\n",
    "\n",
    "    corner(tmp,columns,limits,nbins=5,filename=None,vmin=1000,vmax=1e6,figsize=12,aspect_ratio=1)\n",
    "    plt.show()\n",
    "    \n",
    "widgets.interact(plot_corner, mass=mass_slider,age=age_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>5e3) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "xlim = [0,1]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['fesc'],tmp['density'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "#x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='fesc',ylabel=r'[SII]6716 / [SII]6730',xlim=xlim)\n",
    "\n",
    "#fig.colorbar(sc,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'all_galaxies_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>5e3) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "#xlim = [0,1]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['age'],tmp['mass'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "#x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='age',ylabel=r'mass')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Abundance gradients\n",
    "\n",
    "to help better understand the difference between abundances and local abundance offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "bins = np.logspace(-1.5,-0.2,10)\n",
    "xlim = (1e-2,1e0)\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "table = nebulae[nebulae['[SIII]/[SII]']>3*nebulae['[SIII]/[SII]_err']]\n",
    "bins = np.logspace(*np.log10(np.nanpercentile(table['[SIII]/[SII]'],(2,98))),10)\n",
    "\n",
    "groups = table.group_by('gal_name')\n",
    "\n",
    "ax1.scatter(table['[SIII]/[SII]'],table['met_scal'],color='gray',s=0.1)\n",
    "ax2.scatter(table['[SIII]/[SII]'],table['Delta_met_scal'],color='gray',s=0.1)\n",
    "\n",
    "rho1,rho2 = [], []\n",
    "for group in groups.groups:\n",
    "    gal_name = group[0]['gal_name']\n",
    "\n",
    "    x,mean,std = bin_stat(group['[SIII]/[SII]'],group['met_scal'],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(group['[SIII]/[SII]'],group['Delta_met_scal'],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "  \n",
    "    r1,p1 = spearmanr(group['[SIII]/[SII]'],group['met_scal'],nan_policy='omit')\n",
    "    r2,p2 = spearmanr(group['[SIII]/[SII]'],group['Delta_met_scal'],nan_policy='omit')\n",
    "    #print(f'rho = {r1:.2f}, {r2:.2f}')\n",
    "    rho1.append(r1)\n",
    "    rho2.append(r2)\n",
    "r,p = spearmanr(table['[SIII]/[SII]'],table['met_scal'],nan_policy='omit')\n",
    "t = ax1.text(0.07,0.9,r'$\\rho'+f'={r:.2f}$',transform=ax1.transAxes,fontsize=7)\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "r,p = spearmanr(table['[SIII]/[SII]'],table['Delta_met_scal'],nan_policy='omit')\n",
    "t = ax2.text(0.07,0.9,r'$\\rho'+f'={r:.2f}$',transform=ax2.transAxes,fontsize=7)\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',xlim=[2e-2,1],ylim=[8,8.8],\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$12+\\log (\\mathrm{O}/\\mathrm{H})$')\n",
    "ax2.set(xscale='log',xlim=[2e-2,1],ylim=[-0.15,0.15],\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "abundance_gradients = ascii.read(basedir/'data'/'external'/'radial_abundance_gradients.txt',\n",
    "                                names=['name','R0','g_r25'])\n",
    "abundance_gradients.add_index('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\Delta = 12+\\log (O/H) - g\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['r_R25'],tmp['met_scal'])\n",
    "r = np.linspace(0,0.5)\n",
    "ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gal_name in np.unique(catalogue['gal_name']):\n",
    "    tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "    plt.scatter(tmp['met_scal'],tmp['Delta_met_scal'],c=tmp['r_R25'])\n",
    "    #plt.scatter(tmp['galactic_radius'],tmp['Delta_met_scal'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc=ax.scatter(tmp['met_scal'],tmp['Delta_met_scal'],c=tmp['r_R25'])\n",
    "fig.colorbar(sc,label='r / R25')\n",
    "ax.set(xlabel='12+logOH',ylabel=r'$\\Delta$')\n",
    "plt.show()\n",
    "#r = np.linspace(0,0.5)\n",
    "#ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc=ax.scatter(tmp['logq_D91'],tmp['met_scal'],c=tmp['r_R25'])\n",
    "fig.colorbar(sc,label='r / R25')\n",
    "ax.set(xlabel='log q',ylabel=r'$\\Delta$(O/H)')\n",
    "plt.show()\n",
    "#r = np.linspace(0,0.5)\n",
    "#ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "for gal_name in np.unique(nebulae['gal_name']):\n",
    "    tmp = nebulae[nebulae['gal_name']==gal_name]\n",
    "    x,mean,std = bin_stat(tmp['logq_D91'],tmp['Delta_met_scal'],[5,8.5],nbins=10)\n",
    "    ax.errorbar(x,mean,yerr=std,fmt='-',label=gal_name)\n",
    "    #ax.scatter(tmp['logq_D91'],tmp['Delta_met_scal'],label=gal_name)\n",
    "ax.set(xlim=[5,8.5],ylim=[-0.2,0.2],xscale='linear')\n",
    "#ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Age and Ha/FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "xlim = [0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['age'],tmp['HA/FUV'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel=r'H$\\alpha$ / FUV',xlim=xlim,ylim=[0,100])\n",
    "\n",
    "#fig.colorbar(sc,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'all_galaxies_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(8,3))\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['overlap']=='contained')]\n",
    "ax1.scatter(tmp['dig/hii'],tmp['HA/FUV'],c=tmp['age'],vmin=0,vmax=20,s=2)\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['overlap']=='partial')]\n",
    "sc=ax2.scatter(tmp['dig/hii'],tmp['HA/FUV'],c=tmp['age'],vmin=0,vmax=20,s=2)\n",
    "\n",
    "ax1.set(xlim=[0,1],ylim=[0,100],xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$',ylabel=r'H$\\alpha$/FUV')\n",
    "ax2.set(xlim=[0,1],ylim=[0,100],xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.11, 0.02, 0.84])\n",
    "fig.colorbar(sc,cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "seperate subplot for each galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "from scipy.stats import binned_statistic, pearsonr, spearmanr\n",
    "\n",
    "# '[SIII]/[SII]' , 'HA/FUV', 'AGE_MINCHISQ', 'AGE_BAYES'\n",
    "x_name, y_name = 'age', 'HA/FUV'\n",
    "xlim = [0.5,10.5]\n",
    "bins = 10\n",
    "max_sep = 1*u.arcsec\n",
    "\n",
    "#sample = set(np.unique(nebulae['gal_name'])) & hst_sample\n",
    "sample = np.unique(catalogue['gal_name'])[:-1]\n",
    "\n",
    "filename = basedir/'reports'/'all_objects_age_over_SII.pdf'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample): \n",
    "\n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    criteria = (catalogue['gal_name']==name) \n",
    "    criteria &= (catalogue['mass']>1e3) \n",
    "    #criteria &= ~np.isnan(catalogue['HA/FUV'])\n",
    "    criteria &= (catalogue['overlap'] == 'contained')\n",
    "    #criteria &= (catalogue['neighbors'] == 0)\n",
    "    \n",
    "    tmp = catalogue[criteria]\n",
    "    print(f'{name}: {len(tmp)} objects')\n",
    "    \n",
    "    mean, bin_edges, binnumber = binned_statistic(tmp[x_name],\n",
    "                                                  tmp[y_name],\n",
    "                                                  statistic='mean',\n",
    "                                                  bins=bins,\n",
    "                                                  range=xlim)\n",
    "    std, _, _ = binned_statistic(tmp[x_name],\n",
    "                                  tmp[y_name],\n",
    "                                  statistic='std',\n",
    "                                  bins=bins,\n",
    "                                  range=xlim)\n",
    "\n",
    "    ax.scatter(tmp[x_name],tmp[y_name],color='tab:blue',s=1)\n",
    "    # plot the standard divation with yerr=std\n",
    "    ax.errorbar((bin_edges[1:]+bin_edges[:-1])/2,mean,fmt='-')\n",
    "    ax.text(0.65,0.85,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "\n",
    "    ax.set(xlim=xlim)\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(f'{y_name.replace(\"_\",\" \")}')\n",
    "\n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "\n",
    "\n",
    "#plt.savefig(filename,dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "\n",
    "# '[SIII]/[SII]' , 'HA/FUV'\n",
    "x_name, y_name, z_name = '[SIII]/[SII]', 'HA/FUV', 'AGE_MINCHISQ'\n",
    "xlim = [0.5,10.5]\n",
    "bins = 10\n",
    "max_sep = 2*u.arcsec\n",
    "\n",
    "sample = muse_sample & hst_sample & astrosat_sample\n",
    "\n",
    "filename = basedir/'reports'/'all_objects_FUV_over_SII_with_age.pdf'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample): \n",
    "\n",
    "    # it makes a different if we match the clusters to the nebulae or the other way around\n",
    "    catalogcoord = clusters[clusters['gal_name']==name].copy()\n",
    "    matchcoord   = nebulae[nebulae['gal_name']==name].copy()\n",
    "\n",
    "    idx, sep, _ = match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n",
    "\n",
    "    catalogue = matchcoord.copy()\n",
    "\n",
    "    for col in catalogcoord.columns:\n",
    "        if col in catalogue.columns:\n",
    "            catalogue[f'{col}2'] = catalogcoord[idx][col]\n",
    "        else:\n",
    "            catalogue[col] = catalogcoord[idx][col]\n",
    "\n",
    "    catalogue['sep'] = sep\n",
    "    catalogue = catalogue[sep.__lt__(max_sep)]\n",
    "    catalogue = catalogue[~np.isnan(catalogue[x_name]) & ~np.isnan(catalogue[y_name]) & (catalogue['AGE_MINCHISQ']<10)]\n",
    "    print(f'{name}: {len(catalogue)} objects in joined catalogue')\n",
    "\n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    #catalogue = catalogue[catalogue['HA6562_FLUX']>np.nanpercentile(catalogue['HA6562_FLUX'],50)]\n",
    "    #catalogue = catalogue[catalogue['FUV_FLUX_CORR']>3*catalogue['FUV_FLUX_CORR_ERR']]\n",
    "    #catalogue = catalogue[catalogue['SII6716_FLUX_CORR']>3*catalogue['SII6716_FLUX_CORR_ERR']]\n",
    "    #catalogue = catalogue[catalogue['SIII9068_FLUX_CORR']>3*catalogue['SIII9068_FLUX_CORR_ERR']]\n",
    "\n",
    "    r,p = spearmanr(catalogue['[SIII]/[SII]'],catalogue['HA/FUV'])\n",
    "    print(f'{name}: rho={r:.2f}, {len(catalogue)} objects')\n",
    "\n",
    "    sc = ax.scatter(catalogue['[SIII]/[SII]'],catalogue['HA/FUV'],\n",
    "                    c=catalogue[z_name],vmin=0, vmax=10,cmap=plt.cm.RdBu_r,\n",
    "                    s=3,marker='.')\n",
    "    \n",
    "    ax.text(0.05,0.9,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.7,0.15,r'$\\rho$'+f'={r:.2f}',transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.55,0.05,f'{len(catalogue):.0f} objects', transform=ax.transAxes,fontsize=7)\n",
    "    \n",
    "    ax.set(xscale='log',yscale='log',xlim=[1e-2,1],ylim=[2,2e2])\n",
    "    # https://stackoverflow.com/questions/21920233/matplotlib-log-scale-tick-label-number-formatting/33213196\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel('[SIII]/[SII]')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(r'H$\\alpha$ / FUV')\n",
    "\n",
    "fig.colorbar(sc,ax=axes.ravel().tolist(),label=f'{z_name.replace(\"_\",\" \")}')\n",
    "        \n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "\n",
    "\n",
    "plt.savefig(filename,dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Age from escape fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def age_from_escape(fesc,Qobserved,mass,cluster):\n",
    "    '''a different approach to get ages\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fsec : float / array\n",
    "        the escape fraction of the cloud\n",
    "        \n",
    "    Qobserved : float /array\n",
    "        the observed number of ionizing photons (from MUSE Halpha)\n",
    "        \n",
    "    mass : float / array\n",
    "        the mass of the association from the SED fitting\n",
    "    \n",
    "    cluster : object\n",
    "        the cluster model (starburst99)\n",
    "    '''\n",
    "    \n",
    "    # make sure the input is in arrays\n",
    "    fesc = np.atleast_1d(fesc)\n",
    "    Qobserved = np.atleast_1d(Qobserved)\n",
    "    mass = np.atleast_1d(mass)\n",
    "    \n",
    "    \n",
    "    # calculate the original flux based on the escape fraction\n",
    "    Qpredicted = Qobserved / (1-fesc)\n",
    "    # scale to the same mass as the model\n",
    "    Qpredicted = Qpredicted / mass * cluster.mass\n",
    "    \n",
    "    # the number of ionizing photons from the starburst99 model\n",
    "    HI_rate = cluster.quanta['HI_rate'].value\n",
    "    time = cluster.quanta['Time']\n",
    "    # the age that minimizes the difference between observation and model\n",
    "    age = time[np.argmin(np.abs(HI_rate[:,None] - Qpredicted),axis=0)].to(u.Myr).value\n",
    "    \n",
    "    return age\n",
    "\n",
    "# we need the distance to the galaxy to convert fluxes to luminosities\n",
    "catalogue['distance'] = np.nan\n",
    "for gal_name in catalogue['gal_name']:\n",
    "    distance = Distance(distmod=sample_table.loc[gal_name]['(m-M)'])\n",
    "    catalogue['distance'][catalogue['gal_name']==gal_name] = distance\n",
    "    \n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(catalogue['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "catalogue['L(FUV)'] = (catalogue['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2/u.angstrom *4*np.pi*(catalogue['distance']*u.Mpc)**2).to(u.erg/u.s/u.angstrom)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "\n",
    "fesc = 1/(1+1/catalogue['dig/hii'])\n",
    "age = age_from_escape(fesc,catalogue['Qobserved'],catalogue['mass'],cluster)\n",
    "catalogue['age_new'] = age    \n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The escape fraction is defined as\n",
    "\\begin{equation}\n",
    "f_{\\mathrm{esc}} = \\frac{Q_p-Q_o}{Q_p} \n",
    "\\end{equation}\n",
    "we assume that $Q_o=f_\\mathrm{HII}$ is the flux from the \\HII region and $Q_p=f_\\mathrm{HII}+f_\\mathrm{dig} =f_\\mathrm{HII} \\cdot(f_\\mathrm{dig} / f_\\mathrm{HII}+1)$. Putting this into the definition of the escape fraction yields\n",
    "\\begin{align}\n",
    "f_{\\mathrm{esc}} &= \\frac{f_\\mathrm{HII}\\cdot(f_\\mathrm{dig} / f_\\mathrm{HII}+1) - f_\\mathrm{HII}}{f_\\mathrm{HII}\\cdot(f_\\mathrm{dig} / f_\\mathrm{HII}+1)} \\nonumber\\\\\n",
    "&= \\frac{f_\\mathrm{dig} / f_\\mathrm{HII}}{f_\\mathrm{dig} / f_\\mathrm{HII}+1} \\nonumber \\\\\n",
    "&= \\frac{1}{1+f_\\mathrm{HII}/f_\\mathrm{dig}}\n",
    "\\end{align}\n",
    "we correct $H\\alpha$ for the lost flux\n",
    "\\begin{equation}\n",
    "H\\alpha_\\mathrm{corr} = H\\alpha_\\mathrm{obs} / (1-f_\\mathrm{esc})\n",
    "\\end{equation}\n",
    "We create a starburst99 model with the mass of the association and select the age that minimises the difference between the observed and predicted H$\\alpha$ flux. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "mask = (catalogue['mass']>1e4) #& (catalogue['overlap_asc']>0.9) & (catalogue['overlap_neb']>0.1) \n",
    "tmp = catalogue[mask]\n",
    "\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "FUV  = cluster.FUV['FUV']\n",
    "\n",
    "#c=np.log10(tmp['HA6562_FLUX_CORR']),vmin=3.5,vmax=7.7\n",
    "# we need to correct Halpha with the escape fraction\n",
    "sc = ax1.scatter(age[mask],tmp['HA/FUV_corr'])\n",
    "x,mean,std = bin_stat(age[mask],tmp['HA/FUV_corr'],[0,15],nbins=10)\n",
    "ax1.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "ax1.plot(cluster.FUV['Time'].to(u.Myr).value,Ha/FUV,color='black')\n",
    "ax1.set(xlim=[0,15],ylim=[0,150],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "ax1.set_title('age from fesc')\n",
    "#plt.colorbar(sc,ax=ax1)\n",
    "\n",
    "ax2.scatter(tmp['age'],tmp['HA/FUV'])\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],[0,15],nbins=10)\n",
    "ax2.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "\n",
    "ax2.plot(cluster.FUV['Time'].to(u.Myr).value,Ha/FUV,color='black')\n",
    "ax2.set(xlim=[0,15],ylim=[0,140],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "ax2.set_title('age from SED')\n",
    "\n",
    "'''\n",
    "ax3.scatter(tmp['age_new'],tmp['eq_width'])\n",
    "x,mean,std = bin_stat(tmp['age_new'],tmp['eq_width'],[0,15],nbins=10)\n",
    "ax3.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "ax3.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['eq_width_H_A']/10,color='black')\n",
    "ax3.set(xlim=[0,15],ylim=[0,400],xlabel='age / Myr',ylabel=r'EW(H$\\alpha)$')\n",
    "ax3.set_title('age from fesc')\n",
    "\n",
    "ax4.scatter(tmp['age'],tmp['eq_width'])\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['eq_width'],[0,15],nbins=10)\n",
    "ax4.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "ax4.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['eq_width_H_A']/10,color='black')\n",
    "ax4.set(xlim=[0,15],ylim=[0,400],xlabel='age / Myr',ylabel=r'EW(H$\\alpha)$')\n",
    "ax4.set_title('age from SED')\n",
    "'''\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'age_from_fesc=digghii.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(6,3))\n",
    "mask = (catalogue['mass']>1e4) & (catalogue['overlap_asc']>0.9) & (catalogue['overlap_neb']>0.1)  & (catalogue['age']<10)\n",
    "tmp = catalogue[mask]\n",
    "\n",
    "\n",
    "ax1.hist(fesc[mask],bins=np.arange(0,1,0.05))\n",
    "#ax1.hist(fesc2[mask],bins=np.arange(0,1,0.05),alpha=0.7)\n",
    "\n",
    "ax1.set(xlim=[0,1],xlabel=r'$f_\\mathrm{esc}$')\n",
    "ax1.set_title(r'$f_\\mathrm{esc}$ from $f_\\mathrm{dig}/f_\\mathrm{HII}$')\n",
    "\n",
    "ax2.hist(tmp['fesc'],bins=np.arange(0,1,0.05))\n",
    "ax2.set(xlim=[0,1],xlabel=r'$f_\\mathrm{esc}$')\n",
    "ax2.set_title('$f_\\mathrm{esc}$ from SED')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'fesc_hist.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=4,nrows=3,figsize=(8,6))\n",
    "\n",
    "sample = np.unique(catalogue[~np.isnan(catalogue['HA/FUV_corr']) & (catalogue['mass']>1e4)]['gal_name'])\n",
    "\n",
    "axes_iter = iter(axes.flatten())\n",
    "cluster.measure_FUV()\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "FUV  = cluster.FUV['FUV']\n",
    "    \n",
    "for i,gal_name in enumerate(sample):\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    \n",
    "    mask = (catalogue['gal_name']==gal_name) & (catalogue['mass']>1e4)\n",
    "    tmp = catalogue[mask]\n",
    "    \n",
    "    ax.scatter(tmp['age_new'],tmp['HA/FUV_corr'])\n",
    "    x,mean,std = bin_stat(age[mask],tmp['HA/FUV'],[0,15],nbins=10)\n",
    "    #ax1.errorbar(x,mean,yerr=std,fmt='-',color='grey')\n",
    "\n",
    "    ax.plot(cluster.FUV['Time'].to(u.Myr).value,9e4*Ha/FUV,color='black')\n",
    "    ax.set(xlim=[0,15],ylim=[0,150],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "    ax.set_title(gal_name)\n",
    "for i in range(2):\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.plot import corner\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<10)\n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "columns  = ['age_new','HA/FUV_corr','eq_width','logq_D91','Delta_met_scal']\n",
    "limits   = {'HA/FUV_corr':(-5,105),'age_new':(3,9),'fesc':(0,1.1),'Delta_met_scal':(-0.1,0.1),\n",
    "            'logq_D91':(6,8),'density':(0,150),'eq_width':(0,200),\n",
    "            'dig/hii':(0.1,0.9),'met_scal':(8.3,8.7)}\n",
    "labels   = {'HA/FUV_corr':r'H$\\alpha$ / FUV','age_new':'age / Myr','fesc':'fesc','eq_width':r'EW(H$\\alpha$)',\n",
    "            'Delta_met_scal':r'$\\Delta$(O/H)','logq_D91':r'$\\log q$','density':'density / cm-3',\n",
    "            'met_scal':'12+logO/H','dig/hii':r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$'}\n",
    "\n",
    "filename = basedir/'reports'/f'corner_age_from_fesc.png'\n",
    "\n",
    "corner(tmp,columns,limits,labels=labels,nbins=5,\n",
    "       filename=filename,vmin=1000,vmax=1e6,\n",
    "       figsize=two_column,aspect_ratio=1,s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_closest_point_on_curve(point,curve,conversion_factor=1,plot=True):\n",
    "    '''The the closest point on a 2D curve\n",
    "    \n",
    "    assumes that both axis have the same unit (or use conversion_factor)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    point : (float,float) / (array/array)\n",
    "        \n",
    "    curve : (array,array)\n",
    "        \n",
    "    conversion_factor : float\n",
    "        if x and y have different units: y = x*conversion_factor\n",
    "    '''\n",
    "    \n",
    "    x_point,y_point=point\n",
    "    x_curve,y_curve=curve\n",
    "    \n",
    "    x_point = np.atleast_1d(x_point)\n",
    "    y_point = np.atleast_1d(y_point)\n",
    "    \n",
    "    distance = np.sqrt((x_curve[:,None]-x_point)**2 + conversion_factor*(y_curve[:,None]-y_point)**2)\n",
    "    idx = np.argmin(distance,axis=0)\n",
    "\n",
    "    if plot:\n",
    "        fig,ax=plt.subplots(figsize=(4,4))\n",
    "        ax.plot(x_curve,y_curve,color='black')\n",
    "        #ax.plot(x_curve,distance,color='grey')\n",
    "        for xp,yp,i in zip(x_point,y_point,idx):\n",
    "            ax.plot([xp,x_curve[i]],[yp,y_curve[i]])\n",
    "            ax.scatter(xp,yp)\n",
    "        #ax.set_aspect('equal')\n",
    "        plt.show()\n",
    "        \n",
    "    return x_curve[idx],y_curve[idx]\n",
    "\n",
    "\n",
    "find_closest_point_on_curve(([3,7,12],[8,14,8]),\n",
    "                            (np.linspace(0,15),2+15/(1+np.exp(np.linspace(0,15)-5))),\n",
    "                            conversion_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "divider  = 'age'\n",
    "columns  = ['density','temperature','logq_D91','Delta_met_scal']\n",
    "limits   = {'Delta_met_scal':(-0.1,0.1),'logq_D91':(6,8),\n",
    "            'density':(0,120),'temperature':(0,1.2e4)}\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']>0.9) \n",
    "criteria &= ~np.isnan(catalogue[divider])\n",
    "\n",
    "fig,axes=plt.subplots(ncols=len(columns),figsize=(two_column,two_column/len(columns)))\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    tmp = catalogue[criteria & ~np.isnan(catalogue[col])]\n",
    "    \n",
    "    ax.scatter(tmp['age_new'],tmp[col])\n",
    "    ax.set(ylim=limits[col],xlabel='age',ylabel=col.replace('_',''))\n",
    "\n",
    "    \n",
    "axes[0].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "divider  = 'age'\n",
    "columns  = ['density','temperature','logq_D91','Delta_met_scal']\n",
    "limits   = {'Delta_met_scal':(-0.1,0.1),'logq_D91':(6,8),\n",
    "            'density':(0,120),'temperature':(0,1.2e4)}\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']>0.9) \n",
    "criteria &= ~np.isnan(catalogue[divider])\n",
    "\n",
    "# calculate the 33% and 66% percentile of the sample\n",
    "p1,p2 = np.percentile(tmp[divider],[33,66])\n",
    "\n",
    "fig,axes=plt.subplots(ncols=len(columns),figsize=(two_column,two_column/len(columns)))\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    tmp = catalogue[criteria & ~np.isnan(catalogue[col])]\n",
    "    print(f'{col}: {len(tmp)} objects')\n",
    "    \n",
    "    tmp_young = tmp[tmp[divider]<=p1]\n",
    "    ax.hist(tmp_young[col],histtype='step',label='young',color='tab:blue')\n",
    "    \n",
    "    tmp_middle = tmp[(tmp[divider]>p1) & (tmp[divider]<=p2)]\n",
    "    ax.hist(tmp_middle[col],histtype='step',label='middle',color='tab:purple')    \n",
    "    \n",
    "    tmp_old = tmp[tmp[divider]>p2]\n",
    "    ax.hist(tmp_old[col],histtype='step',label='old',color='tab:red')\n",
    "    \n",
    "    ax.set(xlim=limits[col])\n",
    "    ax.set_title(col.replace('_',''))\n",
    "\n",
    "    \n",
    "axes[0].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Age trend or escape fraction\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\mathrm{esc}} = \\frac{Q_p-Q_o}{Q_p} \n",
    "\\end{equation}\n",
    "\n",
    "first we assume a constant escape fraction of ~70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time']\n",
    "HaFUV_pred=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "HaFUV_obs = 0.3*HaFUV_pred\n",
    "\n",
    "fesc = 1-HaFUV_obs/HaFUV_pred\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age/1e6,HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,80],ylabel='Ha/FUV',xlabel='age / Myr')\n",
    "\n",
    "ax2.scatter(age/1e6,HaFUV_obs)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],ylabel='Ha/FUV observed',xlabel='age / Myr')\n",
    "\n",
    "ax3.scatter(age/1e6,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_obs,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV observed',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "next we assume that we still observe the theoretical trend, but with the original flux being constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time'].value/1e6\n",
    "HaFUV_obs=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "HaFUV_pred = 80\n",
    "\n",
    "fesc = 1-HaFUV_obs/HaFUV_pred\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age,0*age+HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,90],ylabel='Ha/FUV',xlabel='age / Myr')\n",
    "\n",
    "ax2.scatter(age,HaFUV_obs)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],ylabel='Ha/FUV observed',xlabel='age / Myr')\n",
    "\n",
    "ax3.scatter(age,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_obs,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "finally we assume an escape fraction that is increasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time'].value/1e6\n",
    "HaFUV_pred=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "fesc = 0.7/(1+np.exp(-1*(age-3)))\n",
    "\n",
    "HaFUV_obs = (1-fesc)*HaFUV_pred\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age,HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV')\n",
    "\n",
    "ax2.scatter(age,HaFUV_obs)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV observed')\n",
    "\n",
    "\n",
    "ax3.scatter(age,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_obs,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "what if we don't observe any trends but there should be? How is the escape fraction behaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "age=cluster.ewidth['Time'].value/1e6\n",
    "HaFUV_pred=(cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV']).value\n",
    "\n",
    "HaFUV_obs = 30\n",
    "fesc = 1-HaFUV_obs/HaFUV_pred\n",
    "\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(ncols=4,figsize=(2*two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(age,HaFUV_pred)\n",
    "ax1.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV')\n",
    "\n",
    "ax2.scatter(age,HaFUV_obs+0*age)\n",
    "ax2.set(xlim=[0,10],ylim=[0,80],xlabel='age / Myr',ylabel='Ha/FUV observed')\n",
    "\n",
    "\n",
    "ax3.scatter(age,fesc)\n",
    "ax3.set(xlim=[0,10],ylim=[0,1],xlabel='age / Myr',ylabel='fesc')\n",
    "\n",
    "ax4.scatter(HaFUV_pred,fesc)\n",
    "ax4.set(xlim=[0,80],ylim=[0,1],xlabel='Ha/FUV',ylabel='fesc')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ages of the association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column*2/3))\n",
    "axes = iter(axes.flatten())\n",
    "\n",
    "version = 'v1p2'\n",
    "for HSTband in ['nuv','v']:\n",
    "    for scalepc in [16,32,64]:\n",
    "        folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "        lst = []\n",
    "        for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "            gal_name = file.stem.split('_')[0]\n",
    "            tbl = Table(fits.getdata(file,ext=1))\n",
    "            tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "            lst.append(tbl)\n",
    "        assoc_tmp = vstack(lst)\n",
    "        \n",
    "        with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "            associations = Table(hdul[1].data)\n",
    "        associations = join(associations,assoc_tmp,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "        \n",
    "        ax = next(axes)\n",
    "\n",
    "        tmp=associations[associations['1to1']]\n",
    "        ax.hist(tmp['reg_dolflux_Age_MinChiSq'],bins=np.arange(0,20))\n",
    "        ax.set_title(f'{HSTband}, {scalepc}pc, {np.mean(tmp[\"reg_dolflux_Age_MinChiSq\"]):.1f} Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPASS\n",
    "\n",
    "https://flexiblelearning.auckland.ac.nz/bpass/8/files/bpassv2_1_manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_bpass(binary='bin',imf='imf135',upper_mass='300',metallicity='z014'):\n",
    "    '''read the output from BPASS\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    binary : str (`bin` or `sin`)\n",
    "        use binarys or single stars\n",
    "        \n",
    "    imf : str\n",
    "        the slope of the IMF (the model)\n",
    "        \n",
    "    upper_mass : str\n",
    "        mass of the most massive stars\n",
    "    \n",
    "    metallicity : str\n",
    "    '''\n",
    "    \n",
    "    filename = basedir/'..'/'BPASS'/f'bpass_v2.2.1_{imf}_{upper_mass}'/binary/f'ionizing-{binary}-{imf}_{upper_mass}.{metallicity}.dat'\n",
    "\n",
    "    if not filename.is_file():\n",
    "        print('file does not exist')\n",
    "        return filename\n",
    "    \n",
    "    units = [u.LogUnit(u.year),u.LogUnit(1/u.s),u.LogUnit(u.erg/u.s),u.LogUnit(u.erg/u.s/u.A),u.LogUnit(u.erg/u.s/u.A)]\n",
    "    names = ['log_age','log_Q','log_Halpha','log_FUV','log_NUV']\n",
    "    bpass = ascii.read(filename,names=names)\n",
    "    bpass = QTable(bpass)\n",
    "\n",
    "    for unit, col in zip(units,bpass.columns):\n",
    "        bpass[col].unit = unit \n",
    "    for col in list(bpass.columns):\n",
    "        if col.startswith('log'):\n",
    "            bpass[col] = bpass[col].physical\n",
    "            bpass.rename_column(col,col[4:])\n",
    "    return bpass\n",
    "\n",
    "bpass = read_bpass(metallicity='z020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(bpass['age'],bpass['Halpha']/bpass['FUV'])\n",
    "ax.set(xlim=[1e6,1e7],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "HI_rate = bpass['Q'].value\n",
    "time = bpass['age']\n",
    "for row in tqdm(catalogue):\n",
    "    idx = np.argmin(np.abs(time-row['age']*u.Myr))\n",
    "    row['Qpredicted'] = HI_rate[idx] * row['mass'] / 1e6\n",
    "    \n",
    "catalogue['distance'] = np.nan\n",
    "for gal_name in catalogue['gal_name']:\n",
    "    distance = Distance(distmod=sample_table.loc[gal_name]['(m-M)'])\n",
    "    catalogue['distance'][catalogue['gal_name']==gal_name] = distance\n",
    "    \n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(catalogue['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for objects with negative fesc we redo the analysis with age-age_err\n",
    "Qpredict_new = []\n",
    "for row in tqdm(catalogue):\n",
    "    if row['fesc']<0:\n",
    "        idx = np.argmin(np.abs(time-(row['age']-3*row['age_err'])*u.Myr))\n",
    "        row['Qpredicted'] = ( HI_rate[idx] * row['mass'] / 1e6 )\n",
    "\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= fesc>0\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "print(f'fesc={np.nanmean(tmp[tmp[\"fesc\"]>0][\"fesc\"]):.2f} (from {np.sum(criteria)} objects)')\n",
    "print(f\"{np.sum(tmp['fesc']<0)} of {len(tmp)} ({np.sum(tmp['fesc']<0)/len(tmp)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different stellar models/population synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpass_z014 = read_bpass(metallicity='z014')\n",
    "bpass_z008 = read_bpass(metallicity='z008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(nrows=3,figsize=(two_column,two_column),sharex=True)\n",
    "\n",
    "ax1.plot(bpass['age']/1e6,bpass['Halpha'],label='BPASSv014')\n",
    "#ax1.plot(bpass_z008['age']/1e6,bpass_z008['Halpha'],label='BPASSv008')\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A'],label='GENEVAv40')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],ylabel=r'$\\mathrm{H}\\alpha \\,/\\, \\mathrm{erg}\\ \\mathrm{s}^{-1}$',\n",
    "        yscale='log',ylim=[1e39,2e41])\n",
    "\n",
    "ax2.plot(bpass['age']/1e6,bpass['FUV'],label='BPASSv014')\n",
    "#ax2.plot(bpass_z008['age']/1e6,bpass_z008['FUV'],label='BPASSv008')\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.FUV['FUV'],label='GENEVAv40')\n",
    "#ax2.legend()\n",
    "ax2.set(xlim=[0,10],ylabel=r'$\\mathrm{FUV}\\,/\\, \\mathrm{erg}\\ \\mathrm{s}^{-1}\\ \\mathrm{\\AA}^{-1}$',\n",
    "       yscale='log',ylim=[8e37,3e39])\n",
    "\n",
    "ax3.plot(bpass['age']/1e6,bpass['Halpha']/bpass['FUV'],label='BPASSv014')\n",
    "#ax3.plot(bpass_z008['age']/1e6,bpass_z008['Halpha']/bpass_z008['FUV'],label='BPASSv008')\n",
    "ax3.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],label='GENEVAv40')\n",
    "#ax3.legend()\n",
    "ax3.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'$\\mathrm{H}\\alpha/\\mathrm{FUV}$')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define the sample\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']>0.9) \n",
    "tmp = catalogue[criteria].copy()\n",
    "tmp = tmp[['age','mass','EBV_balmer','density','temperature','met_scal','logq_D91','fesc']]\n",
    "\n",
    "x = tmp.to_pandas()\n",
    "# remove all columns with NaN\n",
    "tmp = tmp[np.all(~np.isnan(x),axis=1).values]\n",
    "x = x[np.all(~np.isnan(x),axis=1)]\n",
    "x = StandardScaler().fit_transform(x)\n",
    "#x = x[...,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalised_data = pd.DataFrame(x,columns=[f'feature{i}' for i in range(x.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_hiiregion = PCA(n_components=2)\n",
    "principalComponents_hiiregion = pca_hiiregion.fit_transform(x)\n",
    "principal_hiiregion_Df = pd.DataFrame(data = principalComponents_hiiregion\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "idx = tmp['age']<3\n",
    "ax.scatter(principal_hiiregion_Df.loc[idx,'principal component 1'],\n",
    "           principal_hiiregion_Df.loc[idx,'principal component 2'],color='tab:blue')\n",
    "\n",
    "idx = tmp['age']>3\n",
    "ax.scatter(principal_hiiregion_Df.loc[idx,'principal component 1'],\n",
    "           principal_hiiregion_Df.loc[idx,'principal component 2'],color='tab:red')\n",
    "\n",
    "ax.set(xlabel='PCA-1',ylabel='PCA-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot regions (MUSE & HST over WFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "sample = set([x.stem.split('_')[0].upper() for x in (data_ext/'HST'/'white_light').iterdir()])\n",
    "sample = np.unique(sample_table['name'])\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows),facecolor='black')\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "ax=next(axes_iter)\n",
    "# plot the PHANGS logo in the top left axis\n",
    "image = mpl.image.imread(str(basedir/'references'/'Logo_white.png'))\n",
    "img = np.sum(image,axis=2)\n",
    "img = np.pad(img,[(120,36),(25,25)],mode='constant')\n",
    "ax.imshow(img,cmap=plt.cm.gray)\n",
    "ax.axis('off')\n",
    "\n",
    "hst_sample = []\n",
    "for name in sorted(sample):\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "    \n",
    "    if filename.is_file():\n",
    "        with fits.open(filename) as hdul:\n",
    "            hst_whitelight = NDData(hdul[0].data,mask=hdul[0].data==0,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "            hst_whitelight.data[hst_whitelight.data==0] = np.nan\n",
    "        hst_sample.append(name)\n",
    "\n",
    "    \n",
    "    filename = data_ext / 'MUSE' / 'DR2.1' / 'MUSEDAP' / f'{name}_MAPS.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "        \n",
    "    filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "\n",
    "        WFI = NDData(data=hdul[0].data,\n",
    "                     meta=hdul[0].header,\n",
    "                     wcs=WCS(hdul[0].header))\n",
    "        \n",
    "\n",
    "    reg_muse_pix, reg_muse_sky = find_sky_region(Halpha.mask.astype(int),wcs=Halpha.wcs)\n",
    "    if name in hst_sample:\n",
    "        reg_hst_pix, reg_hst_sky = find_sky_region(hst_whitelight.mask.astype(int),wcs=hst_whitelight.wcs)\n",
    "    \n",
    "    WFI_cutout = Cutout2D(WFI.data,sample_table.loc[name]['SkyCoord'],size=8*u.arcmin,wcs=WFI.wcs)\n",
    "    \n",
    "    # project from muse to hst coordinates\n",
    "    reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "    if name in hst_sample:\n",
    "        reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "    ax = next(axes_iter)\n",
    "\n",
    "    # plot image\n",
    "    norm = simple_norm(WFI_cutout.data,clip=False,percent=99)\n",
    "    ax.imshow(WFI_cutout.data,norm=norm,cmap=plt.cm.gray,origin='lower')\n",
    "\n",
    "    reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE',lw=0.5)\n",
    "    if name in hst_sample:\n",
    "        reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST',lw=0.5)\n",
    "    t = ax.text(0.05,0.91,name, transform=ax.transAxes,color='black',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i in range(nrows*ncols-len(sample)-1):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=-0.01,hspace=0.05)\n",
    "plt.savefig(basedir/'reports'/'all_objects.jpg',facecolor='black',dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(len(muse_sample)/ncols))\n",
    "\n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for name in sorted(muse_sample):\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    catalogue_file = basedir/'..'/'PNLF'/'data'/'catalogues'/f'{name}_nebulae.txt'\n",
    "    catalogue = ascii.read(catalogue_file,format='fixed_width_two_line',delimiter_pad=' ',position_char='=')\n",
    "    catalogue['SkyCoord'] = SkyCoord(catalogue['RaDec'])\n",
    "    catalogue=catalogue[catalogue['type']=='PN']\n",
    "    \n",
    "    filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "\n",
    "        WFI = NDData(data=hdul[0].data,\n",
    "                     meta=hdul[0].header,\n",
    "                     wcs=WCS(hdul[0].header))\n",
    "        \n",
    "    \n",
    "    WFI_cutout = Cutout2D(WFI.data,sample_table.loc[name]['SkyCoord'],size=5*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "    ax = next(axes_iter)\n",
    "\n",
    "    # plot image\n",
    "    norm = simple_norm(WFI_cutout.data,clip=False,percent=99)\n",
    "    ax.imshow(WFI_cutout.data,norm=norm,cmap=plt.cm.gray,origin='lower')\n",
    "    \n",
    "    x,y = catalogue['SkyCoord'].to_pixel(WFI_cutout.wcs)\n",
    "    ax.scatter(x,y,marker='o',ec='tab:red',fc='none',s=1,lw=0.2)\n",
    "    \n",
    "    t = ax.text(0.05,0.91,name, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i in range(nrows*ncols-len(muse_sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=-0.01,hspace=0.05)\n",
    "plt.savefig(basedir/'reports'/'all_objects_PN.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(len(muse_sample)/ncols))\n",
    "\n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for name in sorted(muse_sample):\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    catalogue = filter_table(nebulae,gal_name=name)\n",
    "    #catalogue['SkyCoord'] = SkyCoord(catalogue['RaDec'])\n",
    "    \n",
    "    filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        WFI = NDData(data=hdul[0].data,\n",
    "                     meta=hdul[0].header,\n",
    "                     wcs=WCS(hdul[0].header))\n",
    "        \n",
    "    \n",
    "    WFI_cutout = Cutout2D(WFI.data,sample_table.loc[name]['SkyCoord'],size=5*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "    ax = next(axes_iter)\n",
    "\n",
    "    # plot image\n",
    "    norm = simple_norm(WFI_cutout.data,clip=False,percent=99)\n",
    "    ax.imshow(WFI_cutout.data,norm=norm,cmap=plt.cm.gray,origin='lower')\n",
    "    \n",
    "    x,y = catalogue['SkyCoord'].to_pixel(WFI_cutout.wcs)\n",
    "    ax.scatter(x,y,marker='o',ec='tab:blue',fc='none',s=1,lw=0.2)\n",
    "    \n",
    "    t = ax.text(0.05,0.91,name, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i in range(nrows*ncols-len(muse_sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=-0.01,hspace=0.05)\n",
    "plt.savefig(basedir/'reports'/'all_objects_nebulae.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between catalogues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which version of the association catalogue to use\n",
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 32\n",
    "gal_name = 'NGC0628'\n",
    "\n",
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_nebulae.yml') as f:\n",
    "    nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = nebulae[['gal_name','region_ID']]\n",
    "overlap.add_index(['gal_name','region_ID'])\n",
    "overlap['nuv12pc'] = 0\n",
    "\n",
    "for k,v in nebulae_dict.items():\n",
    "    overlap[(overlap['gal_name']==gal_name) & (overlap['region_ID']==k)]['nuv12pc'][0] = len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap[(overlap['gal_name']==gal_name) & (overlap['region_ID']==k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap.loc[['NGC0628',12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a LaTeX table for the journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['RA_asc'],catalogue['DEC_asc'] = zip(*[x.split(' ') for x in catalogue['SkyCoord_asc'].to_string(style='hmsdms',precision=2)])\n",
    "catalogue['RA_neb'],catalogue['DEC_neb'] = zip(*[x.split(' ') for x in catalogue['SkyCoord_neb'].to_string(style='hmsdms',precision=2)])\n",
    "catalogue['logmass'] = np.log10(catalogue['mass'])\n",
    "catalogue['logmass_err'] = catalogue['mass_err'] / catalogue['mass']\n",
    "catalogue['dots'] = '$\\hdots$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one large table\n",
    "columns = ['gal_name','region_ID','RA_neb','DEC_neb','[SIII]/[SII]','[SIII]/[SII]_err','Delta_met_scal','met_scal_err','HA/FUV','HA/FUV_err','eq_width','EBV_balmer','EBV_balmer_err','overlap_neb','assoc_ID','RA_asc','DEC_asc','age','age_err','mass','mass_err','EBV_stars','EBV_stars_err','overlap_asc']\n",
    "# create LaTeX table for a single galaxy (for paper)\n",
    "tmp = catalogue[columns]\n",
    "\n",
    "\n",
    "for col in columns:\n",
    "    if not col.startswith('RA') and not col.startswith('DEC') and col!='gal_name':\n",
    "        tmp[col].info.format = '%.2f'\n",
    "\n",
    "ascii.write(tmp[-10:],sys.stdout,Writer=ascii.Latex,overwrite=True,exclude_names=['x','y','fwhm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or two separate tables\n",
    "# for the associations\n",
    "columns = ['gal_name','assoc_ID','RA_asc','DEC_asc','age','age_err','logmass','logmass_err','EBV_stars','EBV_stars_err','overlap_asc','dots']\n",
    "# create LaTeX table for a single galaxy (for paper)\n",
    "tmp = catalogue[columns]\n",
    "\n",
    "\n",
    "for col in columns:\n",
    "    if not col.startswith('RA') and not col.startswith('DEC') and col!='gal_name' and col!='dots':\n",
    "        tmp[col].info.format = '%.2f'\n",
    "tmp['assoc_ID'].info.format = '%.0f'\n",
    "        \n",
    "ascii.write(tmp[:10],sys.stdout,Writer=ascii.Latex,overwrite=True,exclude_names=['x','y','fwhm'])\n",
    "\n",
    "\n",
    "# and now for the HII regions\n",
    "columns = ['dots','region_ID','RA_neb','DEC_neb','[SIII]/[SII]','[SIII]/[SII]_err','Delta_met_scal','met_scal_err','HA/FUV','HA/FUV_err','eq_width','EBV_balmer','EBV_balmer_err','overlap_neb']\n",
    "# create LaTeX table for a single galaxy (for paper)\n",
    "tmp = catalogue[columns]\n",
    "\n",
    "for col in columns:\n",
    "    if not col.startswith('RA') and not col.startswith('DEC') and col!='gal_name' and col!='dots':\n",
    "        tmp[col].info.format = '%.2f'\n",
    "tmp['region_ID'].info.format = '%.0f'\n",
    "\n",
    "ascii.write(tmp[:10],sys.stdout,Writer=ascii.Latex,overwrite=True,exclude_names=['x','y','fwhm'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure DIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_dig(data,mask,label,position,factor=1,max_iter=10,size=32,plot=False):\n",
    "    '''measure the diffuse ionized gas around an HII-region'''\n",
    "    \n",
    "    cutout_mask = Cutout2D(mask.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    cutout_data = Cutout2D(data.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    \n",
    "    area_mask  = np.sum(cutout_mask.data==label)\n",
    "    input_mask = cutout_mask.data==label\n",
    "    \n",
    "    n_iter = 0\n",
    "    while True:\n",
    "        n_iter+=1\n",
    "        boundaries = find_boundaries(input_mask,mode='outer')\n",
    "        input_mask |=boundaries\n",
    "        area_boundary = np.sum(input_mask & np.isnan(cutout_mask.data)) \n",
    "        if area_boundary > factor*area_mask or n_iter>max_iter: break\n",
    "            \n",
    "    if plot:\n",
    "        fig,ax=plt.subplots(figsize=(5,5))\n",
    "        ax.imshow(cutout_mask.data,origin='lower')\n",
    "        mask = np.zeros((*cutout_mask.shape,4))\n",
    "        mask[input_mask & np.isnan(cutout_mask.data),:] = (1,0,0,0.5)\n",
    "        ax.imshow(mask,origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "    #if np.sum(boundaries & np.isnan(cutout_mask.data))==0:\n",
    "    #    print(f'no boundaries for {label}')\n",
    "    dig = cutout_data.data[input_mask & np.isnan(cutout_mask.data)]\n",
    "\n",
    "    return np.median(dig),np.mean(dig),np.sum(dig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old and young populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regions import read_ds9\n",
    "\n",
    "tmp = catalogue[catalogue['gal_name']=='NGC1365']\n",
    "\n",
    "# filter image with uncertainties\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'NGC1365_uvis_f275w_exp_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "\n",
    "associations,associations_mask = read_associations(folder=data_ext/'HST',target='NGC1365',scalepc=32)\n",
    "reg_young, reg_old = read_ds9(basedir/'data'/'tmp'/'young_old.reg')\n",
    "\n",
    "# the catalogue\n",
    "young_sample = tmp[reg_young.contains(tmp['SkyCoord_asc'],wcs=associations_mask.wcs)]\n",
    "young_sample = young_sample[young_sample['age']<20]\n",
    "old_sample = tmp[reg_old.contains(tmp['SkyCoord_asc'],wcs=associations_mask.wcs)]\n",
    "old_sample = old_sample[old_sample['age']>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the maps\n",
    "young = np.isin(associations_mask.data,associations[associations['age']<10]['assoc_ID'])\n",
    "old   = np.isin(associations_mask.data,associations[associations['age']>10]['assoc_ID'])\n",
    "\n",
    "young = young.astype(float)\n",
    "young[young==0] = np.nan\n",
    "\n",
    "old = old.astype(float)\n",
    "old[old==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(projection=F275.wcs)\n",
    "\n",
    "norm = simple_norm(F275.data,clip=False,percent=99)\n",
    "ax.imshow(F275.data,norm=norm,origin='lower',cmap=plt.cm.Greys,alpha=0.5)\n",
    "ax.imshow(old,vmin=0,vmax=1,cmap=plt.cm.Reds,alpha=0.8)\n",
    "ax.imshow(young,vmin=0,vmax=1,cmap=plt.cm.Blues,alpha=0.8)\n",
    "\n",
    "reg_young_pix = reg_young.to_pixel(F275.wcs)\n",
    "reg_old_pix = reg_old.to_pixel(F275.wcs)\n",
    "\n",
    "ax.add_artist(reg_young_pix.as_artist())\n",
    "ax.add_artist(reg_old_pix.as_artist())\n",
    "\n",
    "\n",
    "ax.set(xlim=[2000,5000],ylim=[3000,5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,10]\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "sc = ax.scatter(young_sample['EBV_stars'],young_sample['EBV_balmer'],color='g',label='upper arm')\n",
    "sc = ax.scatter(old_sample['EBV_stars'],old_sample['EBV_balmer'],color='m',label='lower arm')\n",
    "ax.legend()\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "#fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel='E(B-V) stars',ylabel='E(B-V) Balmer')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deproject radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import project\n",
    "\n",
    "def r25(name,table):\n",
    "    '''calculate deprojected r25\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # get the pixel position of the centre\n",
    "    with fits.open(data_ext/'MUSE_DR2.1'/'MUSEDAP'/f'{name}_MAPS.fits') as hdul:\n",
    "        wcs = WCS(hdul['FLUX'].header)\n",
    "    centre = sample_table.loc[name]['SkyCoord']\n",
    "    x_cen,y_cen = centre.to_pixel(wcs)\n",
    "    \n",
    "    pa  = sample_table.loc[name]['posang']\n",
    "    inc = sample_table.loc[name]['Inclination']\n",
    "    r25 = sample_table.loc[name]['r25']*u.arcmin\n",
    "    \n",
    "    # deproject\n",
    "    x_depr,y_depr = project(table['x_neb']-x_cen,table['y_neb']-y_cen,pa,inc)\n",
    "    skycoord_depr = SkyCoord.from_pixel(x_depr+x_cen,y_depr+y_cen,wcs)\n",
    "    \n",
    "    # separation to centre\n",
    "    sep = skycoord_depr.separation(centre)\n",
    "    \n",
    "    return (sep/r25).decompose()\n",
    "\n",
    "name = 'NGC0628'\n",
    "\n",
    "# the catalogue with the positions\n",
    "tmp = nebulae[nebulae['gal_name']==name]\n",
    "\n",
    "r25(name,tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at units of FUV and Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speclite.filters import FilterResponse, load_filters, plot_filters\n",
    "\n",
    "response_curve = ascii.read(basedir/'data'/'external'/'astrosat_response_curve.txt',\n",
    "                                     names=['wavelength','EA','Filter'])\n",
    "\n",
    "F148W_mask = response_curve['Filter']=='F148W'\n",
    "F148W_lam = response_curve['wavelength'][F148W_mask]*u.angstrom\n",
    "F148W_res = response_curve['EA'][F148W_mask] / max(response_curve['EA'][F148W_mask])\n",
    "F148W = FilterResponse(F148W_lam,F148W_res,meta=dict(group_name='Astrosat',band_name='F148W'))\n",
    "\n",
    "F154W_mask = response_curve['Filter']=='F154W'\n",
    "F154W_lam  = response_curve['wavelength'][F154W_mask]*u.angstrom\n",
    "F154W_res  = response_curve['EA'][F154W_mask] / max(response_curve['EA'][F154W_mask])\n",
    "F154W = FilterResponse(F154W_lam,F154W_res,meta=dict(group_name='Astrosat',band_name='F154W'))\n",
    "\n",
    "astrosat_filter = load_filters('Astrosat-F148W', 'Astrosat-F154W')\n",
    "plot_filters(astrosat_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = 1e6 * u.yr\n",
    "\n",
    "# find closest availalbe age\n",
    "ages=cluster.spectrum['Time']\n",
    "age =ages[np.argmin(np.abs(ages-age))]\n",
    "\n",
    "wavelength = cluster.spectrum[cluster.spectrum['Time']==age]['Wavelength']\n",
    "spectrum   = cluster.spectrum[cluster.spectrum['Time']==age]['Total']\n",
    "\n",
    "ages=cluster.uvline['Time']\n",
    "age =ages[np.argmin(np.abs(ages-age))]\n",
    "wavelength_uv = cluster.uvline[cluster.uvline['Time']==age]['Wavelength']\n",
    "spectrum_uv   = cluster.uvline[cluster.uvline['Time']==age]['lum']\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(wavelength,spectrum,color='tab:red')\n",
    "ax.plot(wavelength_uv,spectrum_uv,color='tab:blue')\n",
    "ax.set(xlim=[1000,2000],yscale='log',ylim=[1e38,1e40],\n",
    "       xlabel='wavelength / AA',ylabel='erg / s / AA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different methods to calculate FUV from starburst99\n",
    "age = np.unique(cluster.uvline['Time'])[4]\n",
    "\n",
    "wavelength = cluster.uvline['Wavelength'][cluster.uvline['Time']==age]\n",
    "spectrum   = cluster.uvline['lum'][cluster.uvline['Time']==age]\n",
    "print(f'uvline,   filter: {np.mean(np.interp(wavelength,F148W_lam,F148W_res)*spectrum):.2g}')\n",
    "\n",
    "mask = (wavelength>1250*u.angstrom) & (wavelength<1800*u.angstrom)\n",
    "print(f'uvline,   range:  {np.mean(spectrum[mask]):.2g}')\n",
    "\n",
    "wavelength = cluster.spectrum['Wavelength'][cluster.spectrum['Time']==age]\n",
    "spectrum   = cluster.spectrum['Total'][cluster.spectrum['Time']==age]\n",
    "print(f'spectrum, filter: {np.mean(np.interp(wavelength,F148W_lam,F148W_res)*spectrum):.2g}')\n",
    "\n",
    "mask = (wavelength>1250*u.angstrom) & (wavelength<1800*u.angstrom)\n",
    "print(f'spectrum, range:  {np.mean(spectrum[mask]):.2g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "for stellar_model in [41,42,43,44,45]:\n",
    "    cluster = Cluster(stellar_model=stellar_model)\n",
    "    cluster.measure_FUV()\n",
    "    _,_,metallicity,_ = find_model(stellar_model)\n",
    "    \n",
    "    ax.plot(cluster.ewidth['Time']/1e6,\n",
    "            cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],\n",
    "            label=f'Z={metallicity}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "#ax.set_title('Geneva Tracks with Rotation')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'HaFUV_starburst99_padova.pdf',dpi=400)\n",
    "plt.savefig(basedir/'reports'/'HaFUV_starburst99_padova.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,0.75*two_column/2))\n",
    "\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'])\n",
    "ax1.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['eq_width_H_A'])\n",
    "ax2.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'EW(H$\\alpha$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'HaFUV.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = 1e6 * u.yr\n",
    "\n",
    "# find closest availalbe age\n",
    "ages=cluster.spectrum['Time']\n",
    "age =ages[np.argmin(np.abs(ages-age))]\n",
    "\n",
    "wavelength = cluster.spectrum[cluster.spectrum['Time']==age]['Wavelength']\n",
    "spectrum   = cluster.spectrum[cluster.spectrum['Time']==age]['Total']\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(wavelength,spectrum)\n",
    "ax.set(xlim=[500,8000],yscale='log',ylim=[1e35,1e40])\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(response_curve['lam'],response_curve['r'],color='grey')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D density histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic_2d\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "x = np.array([0.5,0.3,0.5,3.5])\n",
    "y = np.array([0.5,0.6,2.5,1.5])\n",
    "z = np.array([7.2,7.8,6.5,7.9])\n",
    "\n",
    "xlim,ylim=[0,4],[0,4]\n",
    "\n",
    "def hist_scatter(x,y,z,xlim,ylim):\n",
    "    \n",
    "    hist , x_e, y_e    = np.histogram2d(x,y, bins=4,range =[xlim,ylim], density=True)\n",
    "    mean , x_e, y_e, _ = binned_statistic_2d(x,y,z,bins=4,range = [xlim,ylim])\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ),mean,np.vstack([x,y]).T,method=\"nearest\",bounds_error=False)\n",
    "\n",
    "    fig,ax=plt.subplots()\n",
    "    cmap = plt.cm.get_cmap('gray_r',4)\n",
    "    im = ax.imshow(mean.T,origin='lower',cmap=cmap,extent=[*xlim,*ylim],vmin=0,vmax=2)\n",
    "    sc=ax.scatter(x,y,c=z,cmap=plt.cm.viridis,vmin=6,vmax=8)\n",
    "    fig.colorbar(sc)\n",
    "    ax.set(xlim=xlim,ylim=ylim)\n",
    "    plt.show()\n",
    "    \n",
    "hist_scatter(x,y,z,xlim,ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure NUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import InverseVariance\n",
    "\n",
    "gal_name = 'NGC2835'\n",
    "\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "error_file = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_err-drc-wht.fits'\n",
    "\n",
    "if not filename.is_file():\n",
    "    print(f'no NUV data for {gal_name}')\n",
    "else:\n",
    "    with fits.open(filename) as hdul:\n",
    "        F275 = NDData(hdul[0].data,\n",
    "                        mask=hdul[0].data==0,\n",
    "                        meta=hdul[0].header,\n",
    "                        wcs=WCS(hdul[0].header))\n",
    "        with fits.open(error_file) as hdul:\n",
    "            F275.uncertainty = InverseVariance(hdul[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "HSTband = 'nuv'\n",
    "\n",
    "NUV_fluxes = {}\n",
    "NUV_fluxes_err = {}\n",
    "for scalepc in [8,16,32,64]:\n",
    "    print(f'working on {scalepc}pc')\n",
    "    associations, associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                        target=gal_name.lower(),scalepc=scalepc,\n",
    "                                                        HSTband='nuv',version='v1p2',data='all')\n",
    "\n",
    "    # measure flux in mask and covert to physical units\n",
    "    std_err_map = np.sqrt(1/F275.uncertainty.array)\n",
    "\n",
    "    NUV = [np.sum(F275.data[associations_mask.data==assoc_ID]) for assoc_ID in associations['assoc_ID']]\n",
    "    NUV_err = [np.sqrt(np.sum(std_err_map[associations_mask.data==assoc_ID]**2)) for assoc_ID in associations['assoc_ID']]\n",
    "\n",
    "    NUV_mJy = 1e3*np.array(NUV)* F275.meta['PHOTFNU']*u.mJy\n",
    "    NUV_mJy_err = 1e3*np.array(NUV_err)* F275.meta['PHOTFNU']*u.mJy\n",
    "\n",
    "    NUV_flam = np.array(NUV)* F275.meta['PHOTFLAM']\n",
    "    NUV_flam_err = np.array(NUV_err)* F275.meta['PHOTFLAM']\n",
    "    \n",
    "    NUV_fluxes[scalepc] = NUV_mJy\n",
    "    NUV_fluxes_err[scalepc] = NUV_mJy_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(associations['NUV_dolflux_mjy'],NUV_mJy,s=1)\n",
    "ax1.plot([5e-5,1],[5e-5,1],color='black')\n",
    "ax1.set(xlim=[5e-5,1e0],ylim=[5e-5,1e0],xscale='log',yscale='log',\n",
    "        xlabel='NUV / mJy from catalogue',ylabel='NUV / mJy from image')\n",
    "ax1.set_title(r'using \\texttt{PHOTFNU}')\n",
    "\n",
    "\n",
    "# we are missing a factor 100 in the untis here\n",
    "ax2.scatter(associations['NUV_FLUX'],NUV_flam,s=1)\n",
    "ax2.plot([5e-19,5e-13],[5e-19,5e-13],color='black')\n",
    "ax2.set(xlim=[5e-19,5e-15],ylim=[5e-19,5e-15],xscale='log',yscale='log',\n",
    "        xlabel=r'NUV / erg s$^{-1}$ cm$^{-2}$ \\AA$^{-1}$ from catalogue',ylabel=r'NUV / erg s$^{-1}$ cm$^{-2}$ \\AA$^{-1}$ from image')\n",
    "ax2.set_title(r'using \\texttt{PHOTFLAM}')\n",
    "\n",
    "#fig.suptitle(f'{gal_name}, NUV (F275W) for {scalepc}pc associations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/f'remeasure_NUV_{gal_name}_{scalepc}pc.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(F275.meta['PHOTFNU']*u.Jy).to(u.erg/u.s/u.cm**2/u.Angstrom,equivalencies=u.spectral_density(2704*u.AA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F275.meta['PHOTFLAM']*u.erg/u.s/u.cm**2/u.Angstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(ncols=4,figsize=(1.5*two_column,two_column/4*1.5))\n",
    "\n",
    "for scalepc,ax in zip([8,16,32,64],axes):\n",
    "    \n",
    "    associations = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                     target=gal_name.lower(),scalepc=scalepc,\n",
    "                                     HSTband='nuv',version='v1p2',data='catalogue')\n",
    "    \n",
    "    dif = np.mean((associations['NUV_dolflux_mjy']-1.67*NUV_fluxes[scalepc].value)/associations['NUV_dolflux_mjy'])\n",
    "    ax.scatter(associations['NUV_dolflux_mjy'],1.67*NUV_fluxes[scalepc],s=1)\n",
    "    ax.plot([5e-5,1],[5e-5,1],color='black')\n",
    "    ax.set(xlim=[5e-5,1e0],ylim=[5e-5,1e0],xscale='log',yscale='log',\n",
    "            xlabel='NUV / mJy from catalogue',ylabel='NUV / mJy from image')\n",
    "    ax.set_title(f'{scalepc}pc ({100*dif:.2f}\\% smaller)')\n",
    "\n",
    "\n",
    "#fig.suptitle(f'{gal_name}, NUV (F275W) for {scalepc}pc associations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/f'remeasure_NUV_all_resolutions_{gal_name}.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now measure FUV inside the nebula mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "from astropy.nddata import NDData, StdDevUncertainty, InverseVariance\n",
    "\n",
    "HSTbands_wave = {'NUV':2704*u.AA,'U':3355*u.AA,'B':4325*u.AA,'V':5308*u.AA,'I':8024*u.AA}\n",
    "freq_to_wave = lambda band: u.mJy.to(u.erg/u.s/u.cm**2/u.Angstrom,equivalencies=u.spectral_density(HSTbands_wave[band]))\n",
    "\n",
    "extinction_model = O94(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    " \n",
    "    return ext,ext_err\n",
    "\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae catalogue' / 'Nebulae_catalogue_v2.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['NUV_FLUX'] = np.nan\n",
    "nebulae['NUV_FLUX_ERR'] = np.nan\n",
    "nebulae['NUV_FLUX_CORR'] = np.nan\n",
    "nebulae['NUV_FLUX_CORR_ERR'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_name = 'NGC1365'\n",
    "\n",
    "print(f'start with {gal_name}')\n",
    "p = {x:sample_table.loc[gal_name][x] for x in sample_table.columns}\n",
    "\n",
    "filename = data_ext / 'MUSE'/ 'DR2.1' / 'MUSEDAP' / f'{gal_name}_MAPS.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "filename = data_ext / 'Products' / 'Nebulae catalogue' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "print(f'read in nebulae catalogue')\n",
    "\n",
    "# NUV image\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "error_file = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_err-drc-wht.fits'\n",
    "\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                    mask=hdul[0].data==0,\n",
    "                    meta=hdul[0].header,\n",
    "                    wcs=WCS(hdul[0].header))\n",
    "    with fits.open(error_file) as hdul:\n",
    "        F275.uncertainty = InverseVariance(hdul[0].data)\n",
    "print(f'read in HST data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.regions import Regions\n",
    "from reproject import reproject_interp\n",
    "\n",
    "muse_regions = Regions(mask=nebulae_mask.data,projection=nebulae_mask.meta,bkg=-1)\n",
    "hst_regions = muse_regions.reproject(F275.meta)\n",
    "print('regions reprojected')\n",
    "\n",
    "muse_reproj, footprint = reproject_interp((nebulae_mask.mask,nebulae_mask.wcs),F275.meta)\n",
    "mean,median,std=sigma_clipped_stats(F275.data[footprint.astype(bool)])\n",
    "print('measuring sigma_clipped_stats')\n",
    "\n",
    "tmp = nebulae[nebulae['gal_name']==gal_name]\n",
    "\n",
    "std_err_map = np.sqrt(1/F275.uncertainty.array)\n",
    "\n",
    "flux = np.array([np.sum(F275.data[hst_regions.mask==ID]) for ID in tmp['region_ID']])\n",
    "err  = np.array([np.sqrt(np.sum(std_err_map[hst_regions.mask==ID]**2)) for ID in tmp['region_ID']])\n",
    "\n",
    "# convert counts to physical units\n",
    "flux = np.array(flux) * F275.meta['PHOTFLAM']\n",
    "err  = np.array(err) * F275.meta['PHOTFLAM']\n",
    "print('measuring flux')\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. NUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(2704*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(0.5*tmp['EBV'],tmp['EBV_ERR'],wavelength=2704*u.angstrom)\n",
    "\n",
    "nebulae['NUV_FLUX'][nebulae['gal_name']==gal_name] = 1e20*flux / extinction_mw\n",
    "nebulae['NUV_FLUX_ERR'][nebulae['gal_name']==gal_name] = 1e20*err / extinction_mw\n",
    "\n",
    "nebulae['NUV_FLUX_CORR'][nebulae['gal_name']==gal_name] = 1e20*flux / extinction_mw / ext_int \n",
    "nebulae['NUV_FLUX_CORR_ERR'][nebulae['gal_name']==gal_name] =  nebulae['NUV_FLUX_CORR'][nebulae['gal_name']==gal_name] *np.sqrt((err/flux)**2 + (ext_int_err/ext_int)**2)  \n",
    "\n",
    "print('extinction correction and write to catalogue\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors of FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/'old'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv_old = Table(hdul[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "tmp = fuv_old[fuv_old['gal_name']=='NGC3627']\n",
    "ax1.hist(tmp['FUV_FLUX'],bins=np.logspace(2,5,12))\n",
    "ax1.set(xscale='log')\n",
    "\n",
    "tmp = fuv[fuv['gal_name']=='NGC3351']\n",
    "ax2.hist(tmp['FUV_FLUX'],bins=np.logspace(2,5,12))\n",
    "ax2.set(xscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [1e2,1e6]\n",
    "\n",
    "fuv_flux_old = fuv_old['FUV_FLUX_CORR'][fuv['gal_name']!='NGC3351']\n",
    "fuv_flux_new = fuv['FUV_FLUX_CORR'][fuv['gal_name']!='NGC3351']\n",
    "ax.scatter(fuv_old['FUV_FLUX_CORR'][fuv['gal_name']=='NGC3351'],fuv['FUV_FLUX'][fuv['gal_name']=='NGC3351'],alpha=0.6,label='NGC3351',s=0.8)\n",
    "ax.scatter(fuv_flux_old,fuv_flux_new,alpha=0.6,label='not NGC3351',s=0.8)\n",
    "print(f'difference {100*np.nanmean((fuv_flux_old-fuv_flux_new)/fuv_flux_old):.2f}%')\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim,\n",
    "       xlabel='FUV old',ylabel='FUV new')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(-1,2)\n",
    "ax.hist(fuv['FUV_FLUX_CORR']/fuv['FUV_FLUX_CORR_ERR'],bins=bins)\n",
    "ax.set(xscale='log',xlim=[0.1,1e2],xlabel=r'FUV/ $\\delta$FUV')\n",
    "mean_ston = np.nanmean(fuv['FUV_FLUX_CORR']/fuv['FUV_FLUX_CORR_ERR'])\n",
    "ax.set_title(f'mean S/N={mean_ston:.2f}')\n",
    "plt.savefig(basedir/'reports'/'benchmarks'/'FUV_StoN.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuv['FUV_FLUX_ERR_NEW'] = np.nan\n",
    "fuv['IntTime'] = np.nan\n",
    "fuv['CTSTOFLUX'] = np.nan\n",
    "for gal_name in np.unique(fuv['gal_name']):\n",
    "    astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F148W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F154W_flux_reproj.fits'\n",
    "        if not astro_file.is_file():\n",
    "            print(f'no astrosat file for {gal_name}')\n",
    "            continue\n",
    "            \n",
    "    with fits.open(astro_file) as hdul:\n",
    "        for row in hdul[0].header['COMMENT']:\n",
    "            if row.startswith('CTSTOFLUX'):\n",
    "                _,CTSTOFLUX = row.split(':')\n",
    "                CTSTOFLUX = float(CTSTOFLUX)\n",
    "            if row.startswith('IntTime'):\n",
    "                _,IntTime = row.split(':')\n",
    "                IntTime = float(IntTime)\n",
    "        header=hdul[0].header\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.nanmean(100*(fuv['FUV_FLUX_ERR']-fuv['FUV_FLUX_ERR_NEW'])/fuv['FUV_FLUX_ERR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [1e2,2e3]\n",
    "\n",
    "for gal_name in np.unique(fuv['gal_name']):\n",
    "    tmp = fuv[fuv['gal_name']==gal_name]\n",
    "    ax.scatter(tmp['FUV_FLUX_ERR'],tmp['FUV_FLUX_ERR_NEW'],c=tmp['IntTime']/60,vmin=0,vmax=100)\n",
    "\n",
    "ax.plot(lim,lim,color='black')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [5e2,5e8]\n",
    "\n",
    "ax.scatter(nebulae['HA6562_FLUX_CORR'],nebulae['HA_conv_FLUX_CORR'])\n",
    "\n",
    "ax.plot(lim,lim,color='black')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim,\n",
    "       xlabel=r'H$\\alpha$ catalogue',ylabel=r'H$\\alpha$ from convolved image')\n",
    "plt.savefig(basedir/'reports'/'benchmarks'/'Halpha_convolved.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [5e-3,5e5]\n",
    "\n",
    "ax.scatter(nebulae['HA6562_FLUX_ERR'],nebulae['HA_conv_FLUX_ERR'])\n",
    "\n",
    "ax.plot(lim,lim,color='black')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim,\n",
    "       xlabel=r'H$\\alpha$ Error catalogue',ylabel=r'H$\\alpha$ Error from convolved image')\n",
    "#plt.savefig(basedir/'reports'/'benchmarks'/'Halpha_convolved.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "sc = ax.scatter(fuv['FUV_FLUX'],fuv['FUV_FLUX']/fuv['FUV_FLUX_ERR'],c=fuv['IntTime']/60)\n",
    "ax.set(xscale='log',xlim=[7e1,2e6],yscale='log',ylim=[1,100],\n",
    "       xlabel='FUV flux',ylabel='S/N FUV')\n",
    "fig.colorbar(sc,label='Integration time / min')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.regions import Regions\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae catalogue'/ 'Nebulae_catalogue_v2.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "\n",
    "nebulae['FUV_FLUX'] = np.nan\n",
    "nebulae['FUV_FLUX_ERR'] = np.nan\n",
    "nebulae['FUV_FLUX_CORR'] = np.nan\n",
    "nebulae['FUV_FLUX_CORR_ERR'] = np.nan\n",
    "\n",
    "nebulae['HA_conv_FLUX'] = np.nan\n",
    "nebulae['HA_conv_FLUX_ERR'] = np.nan\n",
    "nebulae['HA_conv_FLUX_CORR'] = np.nan\n",
    "nebulae['HA_conv_FLUX_CORR_ERR'] = np.nan\n",
    "\n",
    "\n",
    "gal_name = 'NGC1566'\n",
    "        \n",
    "print(f'start with {gal_name}')\n",
    "\n",
    "print(f'read in nebulae catalogue')\n",
    "filename = next((data_ext/'MUSE'/'DR2.1'/'copt').glob(f'{gal_name}*.fits'))\n",
    "copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "filename = data_ext / 'Products' / 'Nebulae catalogue' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "print(f'read in astrosat data')\n",
    "astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F148W_flux_reproj.fits'\n",
    "if not astro_file.is_file():\n",
    "    astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        print(f'no astrosat file for {gal_name}')\n",
    "\n",
    "with fits.open(astro_file) as hdul:\n",
    "    d = hdul[0].data\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    for row in hdul[0].header['COMMENT']:\n",
    "        if row.startswith('CTSTOFLUX'):\n",
    "            _,CTSTOFLUX = row.split(':')\n",
    "            CTSTOFLUX = float(CTSTOFLUX)\n",
    "        if row.startswith('IntTime'):\n",
    "            _,IntTime = row.split(':')\n",
    "            IntTime = float(IntTime)\n",
    "\n",
    "\n",
    "print('reproject regions')\n",
    "muse_regions = Regions(mask=nebulae_mask.data,projection=nebulae_mask.meta,bkg=-1)\n",
    "astrosat_regions = muse_regions.reproject(astrosat.meta)\n",
    "\n",
    "tmp = nebulae[nebulae['gal_name']==gal_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "position = tmp[1]['SkyCoord']\n",
    "size = 5*u.arcsec\n",
    "Cutout_Halpha = Cutout2D(Halpha.data,position,size=size,wcs=Halpha.wcs)\n",
    "Cutout_FUV = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)\n",
    "Cutout_mask = Cutout2D(astrosat_regions.mask,position,size=size,wcs=astrosat.wcs)\n",
    "Cutout_mask_neb = Cutout2D(muse_regions.mask,position,size=size,wcs=Halpha.wcs)\n",
    "\n",
    "norm = simple_norm(Cutout_Halpha.data,clip=False,percent=99)\n",
    "ax1.imshow(Cutout_Halpha.data,norm=norm,origin='lower',cmap=plt.cm.Greens)\n",
    "ax1.axis('off')\n",
    "\n",
    "contours = []\n",
    "region_ID = np.unique(Cutout_mask_neb.data[~np.isnan(Cutout_mask_neb.data)])\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(Cutout_mask_neb.data)\n",
    "    blank_mask[Cutout_mask_neb.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "for coords in contours:\n",
    "    ax1.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label='association')\n",
    "\n",
    "norm = simple_norm(Cutout_FUV.data,clip=False,percent=99)\n",
    "ax2.imshow(Cutout_FUV.data,norm=norm,origin='lower',cmap=plt.cm.Blues)\n",
    "ax2.axis('off')\n",
    "\n",
    "ax2.imshow(Cutout_mask.data,origin='lower',cmap=plt.cm.gray,alpha=0.6)\n",
    "\n",
    "contours = []\n",
    "region_ID = np.unique(Cutout_mask.data[~np.isnan(Cutout_mask.data)])\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(Cutout_mask.data)\n",
    "    blank_mask[Cutout_mask.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "for coords in contours:\n",
    "    ax2.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label='association')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.5*np.log10(np.sqrt(2*np.pi))+2.5*np.log10(std) + 2.5/np.log(10)*(x-mu)**2/(2*std**2)-13.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5e-17,1.5e-17)\n",
    "plt.plot(-2.5*np.log10(x)-13.74,norm.pdf(x,1e-17,1e-18),color='blue')\n",
    "plt.xlim((28,29.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of the sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import Distance\n",
    "\n",
    "sample = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample['SkyCoord'] = SkyCoord(sample['R.A.'],sample['Dec.'])\n",
    "sample['d/Mpc'] = Distance(distmod=sample['(m-M)'])\n",
    "\n",
    "ra = sample['SkyCoord'].ra\n",
    "ra = ra.wrap_at(180*u.degree)\n",
    "dec = sample['SkyCoord'].dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpl.use('pdf')\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111, projection=\"mollweide\")\n",
    "ax.scatter(ra.radian,dec.radian,marker='.')\n",
    "#ax.set_xticklabels(['14h','16h','18h','20h','22h','0h','2h','4h','6h','8h','10h'])\n",
    "ax.grid(False)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "d_max = np.max(sample['d/Mpc'])\n",
    "width0 = 0.3 # in radian\n",
    "for x,y,name,distance in zip(ra,dec,sample['name'],sample['d/Mpc']):\n",
    "            \n",
    "    filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{name}*.fits'))\n",
    "    copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "        \n",
    "    img_width,img_height = Halpha.data.shape\n",
    "    width  = width0 * distance / d_max\n",
    "    height = width/img_width*img_height\n",
    "    bounds = [x.radian-width/2,y.radian-height/2,width,height]\n",
    "\n",
    "    ax_img = ax.inset_axes(bounds,transform=ax.transData)\n",
    "    norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "    ax_img.imshow(Halpha.data,cmap=plt.cm.gray_r,norm=norm,origin='lower')\n",
    "    ax_img.axis('off')\n",
    "    ax_img.set_title(name)\n",
    "        \n",
    "    #ax.annotate(s,(x.radian,y.radian),xycoords='data',size='x-small')\n",
    "\n",
    "fig.savefig(\"sky_map.pdf\",dpi=600)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import resolution_from_wcs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "450.85px",
    "left": "38px",
    "top": "110.133px",
    "width": "317.9px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
