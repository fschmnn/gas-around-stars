{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions Multi <a class=\"tocSkip\">\n",
    "\n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues.\n",
    "   \n",
    "This notebook useses multiple galaxies at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from astrotools.packages import *\n",
    "from astrotools.constants import tab10, single_column, two_column, thesis_width\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('pymuse')\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter(\"%(asctime)-15s %(message)s\",datefmt='%H:%M:%S')\n",
    "handler.setFormatter(fmt)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# first we need to specify the path to the raw data\n",
    "basedir = Path('..')\n",
    "data_ext = Path('a:')/'Archive' #basedir / 'data' / 'raw' \n",
    "\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])\n",
    "sample_table['power_index'] = 2.3\n",
    "sample_table['power_index'][sample_table['AO'].mask]=2.8\n",
    "sample_table['distance'] = Distance(distmod=sample_table['(m-M)'])\n",
    "\n",
    "sample_table_v1p6 = Table.read(basedir/'data'/'external'/'phangs_sample_table_v1p6.fits')\n",
    "sample_table_v1p6 = sample_table_v1p6[sample_table_v1p6['survey_muse_status']=='released'].copy()\n",
    "sample_table_v1p6['name'] = [x.upper() for x in sample_table_v1p6['name']]\n",
    "sample_table_v1p6['dist_err'] = sample_table_v1p6['dist']*sample_table_v1p6['dist_unc']*np.log(10)\n",
    "sample_table_v1p6.add_index('name')\n",
    "# we are interested in props_mstar, dist, dist_err, dist_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which version of the association catalogue to use\n",
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The nebulae catalogue\n",
    "\n",
    "707 objects are removed as stars or too close to the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original catalogue from Francesco\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v3.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "del nebulae[['EW_HA','EW_HA_ERR']]\n",
    "nebulae.rename_columns(['cen_x','cen_y','cen_ra','cen_dec','region_area',\n",
    "                          'EBV','EBV_ERR','SkyCoord'],\n",
    "                         ['x_neb','y_neb','ra_neb','dec_neb','area_neb',\n",
    "                          'EBV_balmer','EBV_balmer_err','SkyCoord_neb'])\n",
    "    \n",
    "# some additional properties \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_dig.fits') as hdul:\n",
    "    dig = Table(hdul[1].data)\n",
    "dig['dig/hii'] = dig['dig_median'] / dig['hii_median']\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_FUV_bkg.fits') as hdul:\n",
    "    fuv = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_EW.fits') as hdul:\n",
    "    eq_width = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_density_refit.fits') as hdul:\n",
    "    density = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_refitNII.fits') as hdul:\n",
    "    refitNII = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_in_frame.fits') as hdul:\n",
    "    in_frame = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_CO.fits') as hdul:\n",
    "    CO = Table(hdul[1].data)\n",
    "    \n",
    "nebulae = join(nebulae,fuv,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,eq_width,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,dig,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,density,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,refitNII,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,in_frame,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,CO,keys=['gal_name','region_ID'])\n",
    "\n",
    "# this will rais a few errors that we just ignore\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "    SII_ERR = np.sqrt(nebulae['SII6716_FLUX_CORR_ERR']**2+nebulae['SII6730_FLUX_CORR_ERR']**2)\n",
    "    SIII = (1+2.47)*nebulae['SIII9068_FLUX_CORR']\n",
    "    SIII_ERR = 3.47 * nebulae['SIII9068_FLUX_CORR_ERR']\n",
    "    \n",
    "    nebulae['[SIII]/[SII]'][SII>0] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae['[SIII]/[SII]_ERR'] = nebulae['[SIII]/[SII]'] * np.sqrt((SII_ERR/SII)**2+(SIII_ERR/SIII)**2)\n",
    "    \n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX']/nebulae['FUV_FLUX']\n",
    "    nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['FUV_FLUX_ERR']/nebulae['FUV_FLUX'])**2+(nebulae['HA6562_FLUX_ERR']/nebulae['HA6562_FLUX'])**2)\n",
    "    nebulae['HA/FUV_corr'] = nebulae['HA6562_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    nebulae['HA/FUV_corr_err'] = nebulae['HA/FUV_corr']*np.sqrt((nebulae['FUV_FLUX_CORR_ERR']/nebulae['FUV_FLUX_CORR'])**2+(nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2)\n",
    "\n",
    "# calculate luminosity based on distance from sample table\n",
    "nebulae['distance'] = np.nan\n",
    "nebulae['distance_err'] = np.nan\n",
    "for gal_name in np.unique(nebulae['gal_name']):\n",
    "    nebulae['distance'][nebulae['gal_name']==gal_name] = sample_table_v1p6.loc[gal_name]['dist']\n",
    "    nebulae['distance_err'][nebulae['gal_name']==gal_name] = sample_table_v1p6.loc[gal_name]['dist_err']\n",
    "nebulae['HA6562_LUM_CORR'] = (nebulae['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(nebulae['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "nebulae['HA6562_LUM_CORR_ERR'] = nebulae['HA6562_LUM_CORR'] * np.sqrt((nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2+(nebulae['distance_err']/nebulae['distance'])**2)\n",
    "\n",
    "# the nebulae catalogue with additional information\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*{scalepc}pc_nebulae.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    name=gal_name.lower()\n",
    "    lst.append(tbl)\n",
    "nebulae_tmp = vstack(lst)\n",
    "\n",
    "nebulae = join(nebulae,nebulae_tmp,keys=['gal_name','region_ID'],join_type='outer')\n",
    "nebulae = nebulae[nebulae['flag_star']==0]\n",
    "nebulae = nebulae[nebulae['flag_edge']==0]\n",
    "HIIregion_mask = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "\n",
    "print(f'{len(nebulae)} nebulae in initial catalogue (all galaxies)')\n",
    "nebulae = nebulae[HIIregion_mask]\n",
    "print(f'we use {len(lst)} galaxies with {np.sum(~nebulae[\"overlap_neb\"].mask & (nebulae[\"in_frame\"]))} HII regions')\n",
    "# only use HII regions and only the galaxies with associations\n",
    "#nebulae = nebulae[HIIregion_mask & ~nebulae[\"overlap_neb\"].mask]\n",
    "print(f'{np.sum((nebulae[\"overlap_neb\"]>0) & (nebulae[\"in_frame\"]))} HII regions ({100*np.sum((nebulae[\"overlap_neb\"]>0) & (nebulae[\"in_frame\"]))/np.sum(nebulae[\"in_frame\"]):.1f}%) overlap with association')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those files hold the merged association catalogues\n",
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "    associations = Table(hdul[1].data)\n",
    "    \n",
    "associations['SkyCoord'] = SkyCoord(associations['reg_ra']*u.degree,associations['reg_dec']*u.degree)\n",
    "associations.rename_columns(['reg_ra','reg_dec','reg_x','reg_y',\n",
    "                             'reg_dolflux_Age_MinChiSq','reg_dolflux_Mass_MinChiSq','reg_dolflux_Ebv_MinChiSq',\n",
    "                             'reg_dolflux_Age_MinChiSq_err','reg_dolflux_Mass_MinChiSq_err','reg_dolflux_Ebv_MinChiSq_err',\n",
    "                             'SkyCoord'],\n",
    "                            ['ra_asc','dec_asc','x_asc','y_asc','age','mass',\n",
    "                             'EBV_stars','age_err','mass_err','EBV_stars_err','SkyCoord_asc'])\n",
    "\n",
    "#with fits.open(basedir/'data'/'interim'/f'association_CO_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "#    assoc_CO = Table(hdul[1].data)\n",
    "#associations = join(associations,assoc_CO,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "# Halpha measured in the association masks\n",
    "#with fits.open(basedir/'data'/'interim'/f'phangshst_associations_nuv_ws32pc_v1p1_Halpha.fits') as hdul:\n",
    "#    assoc_Halpha = Table(hdul[1].data)\n",
    "\n",
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}_in_frame.fits') as hdul:\n",
    "    in_frame = Table(hdul[1].data)\n",
    "associations = join(associations,in_frame,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "# the association catalogue matched with the nebuale catalogue\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    #print(f'reading {gal_name}')\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    lst.append(tbl)\n",
    "assoc_tmp = vstack(lst)\n",
    "# combine both catalogues\n",
    "associations = join(associations,assoc_tmp,keys=['gal_name','assoc_ID'])\n",
    "print(f'{len(lst)} galaxies in sample ({np.sum(associations[\"in_frame\"])} associations)')\n",
    "print(f'{np.sum((associations[\"overlap_asc\"]>0) & (associations[\"in_frame\"]))} associations ({100*np.sum((associations[\"overlap_asc\"]>0) & (associations[\"in_frame\"]))/np.sum(associations[\"in_frame\"]):.1f} %) overlap with an HII region')\n",
    "\n",
    "#criteria = np.abs(associations['age']-associations['age_16'])>associations['age_err']\n",
    "#criteria |= np.abs(associations['age']-associations['age_64'])>associations['age_err']\n",
    "#associations['uniform_age'] = ~criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The joined catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also recreate the table from the association an nebulae catalogues\n",
    "catalogue = join(nebulae[~nebulae['assoc_ID'].mask],associations,keys=['gal_name','assoc_ID','region_ID'])\n",
    "\n",
    "# add dig and calculate galactic radius\n",
    "catalogue['galactic_radius'] = np.nan\n",
    "for gal_name in np.unique(catalogue['gal_name']):\n",
    "    centre = sample_table.loc[gal_name]['SkyCoord']\n",
    "    catalogue['galactic_radius'][catalogue['gal_name']==gal_name] = catalogue[catalogue['gal_name']==gal_name]['SkyCoord_neb'].separation(centre).to(u.arcmin)\n",
    "#catalogue['HA/NUV'] = catalogue['HA6562_FLUX_CORR']/catalogue['NUV_FLUX']/1e20\n",
    "    \n",
    "#del catalogue[['SkyCoord_asc','SkyCoord_neb']]\n",
    "#hdu = fits.BinTableHDU(catalogue,name='matched catalogue')\n",
    "#hdu.writeto(basedir/'data'/'interim'/f'matched_catalogue_{HSTband}_ws{scalepc}pc_{version}.fits',overwrite=True)\n",
    "    \n",
    "print(f'{len(catalogue)} objects in final catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_sample      = set(np.unique(associations['gal_name']))\n",
    "astrosat_sample = set(np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name']))\n",
    "muse_sample     = set(sample_table['name'])\n",
    "complete_sample = hst_sample & astrosat_sample & muse_sample\n",
    "\n",
    "\n",
    "print(f'nebulae: {len(nebulae[np.isin(nebulae[\"gal_name\"],list(hst_sample))])}')\n",
    "print(f'associations: {len(associations)} from {len(hst_sample)} galaxies')\n",
    "print(f'FUV: {len(astrosat_sample)} galaxies')\n",
    "print(f'match: {len(catalogue)}')\n",
    "print(f'contained: {len(catalogue[catalogue[\"overlap\"]==\"contained\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_overlap = np.sum(catalogue['overlap']=='contained')\n",
    "N_mass = np.sum(catalogue['mass']>1e4)\n",
    "N_age = np.sum(catalogue['age']<=8)\n",
    "N_final = np.sum((catalogue['age']<=8) & (catalogue['mass']>1e4) & (catalogue['overlap']=='contained'))\n",
    "                 \n",
    "print(f'overlap: {N_overlap}')\n",
    "print(f'mass: {N_mass}')\n",
    "print(f'age: {N_age}')\n",
    "print(f'all: {N_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phangs_sample_table_v1p6 = Table.read(basedir/'data'/'external'/'phangs_sample_table_v1p6.fits')\n",
    "phangs_sample_table_v1p6 = phangs_sample_table_v1p6[np.isin(list(phangs_sample_table_v1p6['name']),[x.lower() for x in sample_table['name']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phangs_sample_table_v1p6[['dist','dist_unc','dist_label','dist_ref']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Table to showcase the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sample table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table',\n",
    "'col_align':'lrrrrc',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Galaxy sample',\n",
    "'units': {'R.A.':'(J2000)','Dec.':'(J2000)','$i$':'deg','PA':'deg','Distance':'$\\si{\\mega\\parsec}$',\n",
    "          'r25':'arcmin'},\n",
    "'preamble': '\\\\centering',\n",
    "'tablefoot': f'\\\\label{{tbl:sample}}'\n",
    "            }\n",
    "\n",
    "astrosat_sample = np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name'])\n",
    "\n",
    "tbl = sample_table[['name','(m-M)']]\n",
    "tbl.sort('name')\n",
    "tbl.add_column(Distance(distmod=tbl['(m-M)']).to(u.Mpc),index=1,name='Distance')\n",
    "tbl['Distance'].info.format = '%.2f' \n",
    "del tbl['(m-M)']\n",
    "tbl[r'$N_{\\HII}$'] = [np.sum((nebulae['gal_name']==name) & (nebulae['in_frame'])) for name in tbl['name']]\n",
    "tbl[r'$N_\\mathrm{asc}$'] = [np.sum((associations['gal_name']==name) & (associations['in_frame'])) for name in tbl['name']]\n",
    "tbl[r'$N_\\mathrm{match}$'] = [np.sum(catalogue['gal_name']==name) for name in tbl['name']]\n",
    "tbl.add_column([f'\\\\galaxyname{{{row[\"name\"][:-4]}}}{{{row[\"name\"][-4:]}}}' for row in tbl],index=0,name='Name')\n",
    "\n",
    "tbl['\\\\textit{AstroSat}'] = ['\\checkmark' if x else '' for x in np.isin(tbl['name'],astrosat_sample)]\n",
    "del tbl['name']\n",
    "\n",
    "tbl.add_row(['',None,np.sum(nebulae['in_frame']),np.sum(associations['in_frame']),len(catalogue),''])\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict,exclude_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "overlap between the two catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Overlap statistics',\n",
    "'preamble': '\\\\centering',\n",
    "'col_align':'lrrrr',\n",
    "'tablefoot': f'\\\\label{{tbl:overlap_statistics}}'\n",
    "            }\n",
    "\n",
    "# we only use nebulae in the 18 galaxies with HST associations\n",
    "tmp_neb = nebulae[(~nebulae['Nassoc'].mask) & (nebulae['in_frame'])]\n",
    "\n",
    "Nasc = []\n",
    "Nneb = []\n",
    "Nasc_per = []\n",
    "Nneb_per = []\n",
    "index = []\n",
    "for i in range(5):\n",
    "    index.append(i)\n",
    "    \n",
    "    Nneb.append(np.sum((tmp_neb[\"Nassoc\"]==i)))\n",
    "    Nneb_per.append(f'({100*np.sum((tmp_neb[\"Nassoc\"]==i))/len(tmp_neb):.1f}\\%)')\n",
    "    \n",
    "    Nasc.append(np.sum((associations[\"Nnebulae\"]==i) & (associations['in_frame'])))\n",
    "    Nasc_per.append(f'({100*np.sum((associations[\"Nnebulae\"]==i) & associations[\"in_frame\"])/np.sum(associations[\"in_frame\"]):.1f}\\%)')\n",
    "\n",
    "    # add the last row with the sum\n",
    "Nneb.append(np.sum(tmp_neb[\"Nassoc\"]>i))\n",
    "Nneb_per.append(f'({100*np.sum((tmp_neb[\"Nassoc\"]>i))/len(tmp_neb):.1f}\\%)')\n",
    "Nasc.append(np.sum(associations[\"Nnebulae\"]>i))\n",
    "Nasc_per.append(f'({100*np.sum((associations[\"Nnebulae\"]>i) & associations[\"in_frame\"])/np.sum(associations[\"in_frame\"]):.1f}\\%)')\n",
    "index.append(f'>{i+1}')\n",
    "\n",
    "tbl = Table([index,Nneb,Nneb_per,Nasc,Nasc_per],names=[' ','1','Nebulae','2','Associations'])\n",
    "\n",
    "print('WARNING: this table also includes non HII region in the overlap of the associations')\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "correct `Nnebulae` (we need to exclude non HII regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Nnebulae = []\n",
    "for gal_name in sample_table['name']:\n",
    "    with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "        associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    tmp_neb = nebulae[nebulae['gal_name']==gal_name]\n",
    "    tmp_asc = associations[associations['gal_name']==gal_name]\n",
    "    \n",
    "    for assoc_ID in tmp_asc['assoc_ID']:\n",
    "        Nnebulae.append(np.sum(np.isin(associations_dict[assoc_ID],tmp_neb['region_ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Overlap statistics',\n",
    "'preamble': '\\\\centering',\n",
    "'col_align':'lrrrr',\n",
    "'tablefoot': f'\\\\label{{tbl:overlap_statistics}}'\n",
    "            }\n",
    "\n",
    "# we only use nebulae in the 18 galaxies with HST associations\n",
    "tmp_neb = nebulae[(~nebulae['Nassoc'].mask) & (nebulae['in_frame'])]\n",
    "\n",
    "Nasc = []\n",
    "Nneb = []\n",
    "Nasc_per = []\n",
    "Nneb_per = []\n",
    "index = []\n",
    "for i in range(5):\n",
    "    index.append(i)\n",
    "    \n",
    "    Nneb.append(np.sum((tmp_neb[\"Nassoc\"]==i)))\n",
    "    Nneb_per.append(f'({100*np.sum((tmp_neb[\"Nassoc\"]==i))/len(tmp_neb):.1f}\\%)')\n",
    "    \n",
    "    Nasc.append(np.sum((np.array(Nnebulae)==i) & (associations['in_frame'])))\n",
    "    Nasc_per.append(f'({100*np.sum((np.array(Nnebulae)==i) & associations[\"in_frame\"])/np.sum(associations[\"in_frame\"]):.1f}\\%)')\n",
    "\n",
    "    # add the last row with the sum\n",
    "Nneb.append(np.sum(tmp_neb[\"Nassoc\"]>i))\n",
    "Nneb_per.append(f'({100*np.sum((tmp_neb[\"Nassoc\"]>i))/len(tmp_neb):.1f}\\%)')\n",
    "Nasc.append(np.sum(np.array(Nnebulae)>i))\n",
    "Nasc_per.append(f'({100*np.sum((np.array(Nnebulae)>i) & associations[\"in_frame\"])/np.sum(associations[\"in_frame\"]):.1f}\\%)')\n",
    "index.append(f'>{i+1}')\n",
    "\n",
    "tbl = Table([index,Nneb,Nneb_per,Nasc,Nasc_per],names=[' ','1','Nebulae','2','Associations'])\n",
    "\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"massive enough (>1e4 Msun): {np.sum(catalogue['mass']>1e4)}\")\n",
    "print(f\"young enough (<8 Myr): {np.sum(catalogue['age']<=8)}\")\n",
    "print(f\"contained: {np.sum(catalogue['overlap']=='contained')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample_table['res_arc'] = [0.72,0.73,0.74,0.63,0.82,0.49,0.65,0.8,0.64,0.72,0.85,0.74,0.77,0.58,0.58,0.64,0.44,0.73,0.79]\n",
    "sample_table['res_pc'] = sample_table['distance']*sample_table['res_arc']*u.arcsec.to(u.radian)*1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the old sample table that contains additional properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latexdict = {'tabletype': 'table*',\n",
    "'header_start': '\\\\toprule',\n",
    "'header_end': '\\\\midrule',\n",
    "'data_end': '\\\\bottomrule',\n",
    "'caption': f'Galaxy sample',\n",
    "'units': {'R.A.':'(J2000)','Dec.':'(J2000)','$i$':'deg','PA':'deg','Distance':'$\\si{\\mega\\parsec}$',\n",
    "          'r25':'arcmin'},\n",
    "'preamble': '\\\\centering',\n",
    "'tablefoot': f'\\\\label{{tbl:sample}}'\n",
    "            }\n",
    "\n",
    "tbl = sample_table[['name','Type','R.A.','Dec.','Inclination','posang','r25','mass','SFR','PSF','(m-M)']]\n",
    "\n",
    "tbl['R.A.'] = [row.replace('h','x').replace('m','y').replace('.','z').replace('s','') for row in tbl['R.A.']]\n",
    "tbl['R.A.'] = [row.replace('x','$^\\mathrm{h}$').replace('y','$^\\mathrm{m}$').replace('z','$^\\mathrm{s}\\kern -3pt.$') for row in tbl['R.A.']]\n",
    "\n",
    "tbl['Dec.'] = [row.replace('d','x').replace('m','y').replace('.','z').replace('s','') for row in tbl['Dec.']]\n",
    "tbl['Dec.'] = [row.replace('-','$-$').replace('+','$+$').replace('x','$^\\mathrm{d}$').replace('y','$^\\mathrm{m}$').replace('z','$^\\mathrm{s}\\kern -3pt.$') for row in tbl['Dec.']]\n",
    "tbl.add_column(Distance(distmod=tbl['(m-M)']).to(u.Mpc),index=4,name='Distance')\n",
    "tbl['Distance'].info.format = '%.2f' \n",
    "tbl['Nneb'] = [np.sum((nebulae['gal_name']==name) & (nebulae['in_frame'])) for name in tbl['name']]\n",
    "tbl['Nasc'] = [np.sum((associations['gal_name']==name) & (associations['in_frame'])) for name in tbl['name']]\n",
    "\n",
    "#tbl['name'] = [f'\\\\galaxyname{{{row[\"name\"][:-4]}}}{{{row[\"name\"][-4:]}}}' for row in tbl]\n",
    "tbl.rename_columns(['Inclination','posang','r25'],['$i$','PA','$r_{25}$'])\n",
    "#tbl = join(tbl,t,keys='name')\n",
    "\n",
    "#with open(basedir / 'data' / 'interim' /'sample.tex','w',newline='\\n') as f:\n",
    "#    ascii.write(tbl,f,Writer=ascii.Latex, latexdict=latexdict,overwrite=True,exclude_names=['(m-M)','Sitelle'])\n",
    "ascii.write(tbl,sys.stdout, Writer = ascii.Latex,latexdict=latexdict,exclude_names=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "table to showcase the different versions of the association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# show all available options and the number of nebulae/associatoins\n",
    "version = 'v1p2'\n",
    "bands = []\n",
    "scale = []\n",
    "Ngal  = []\n",
    "Nneb  = []\n",
    "Nneb_per = []\n",
    "Nasc  = []\n",
    "Nasc_per = []\n",
    "N1to1 = []\n",
    "\n",
    "for HSTband in ['nuv','v']:\n",
    "    for scalepc in [16,32,64]:\n",
    "        folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "        lst = []\n",
    "        for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "            gal_name = file.stem.split('_')[0]\n",
    "            tbl = Table(fits.getdata(file,ext=1))\n",
    "            tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "            lst.append(tbl)\n",
    "        assoc_tmp = vstack(lst)\n",
    "        assoc_in_frame = Table.read(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}_in_frame.fits')\n",
    "        \n",
    "        # missing = set(sample_table['name']) - set(np.unique(assoc_tmp['gal_name']))\n",
    "        #print(f'{HSTband:>3}, {scalepc:>2}pc: {len(lst)} galaxies, {len(assoc_tmp):>5} associations ({np.sum(assoc_tmp[\"1to1\"]):>4} 1to1)')\n",
    "        \n",
    "        folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "        lst = []\n",
    "        for file in folder.glob(f'*{scalepc}pc_nebulae.fits'):\n",
    "            gal_name = file.stem.split('_')[0]\n",
    "            tbl = Table(fits.getdata(file,ext=1))\n",
    "            tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "            name=gal_name.lower()\n",
    "            lst.append(tbl)\n",
    "        nebulae_tmp = vstack(lst) \n",
    "        print(f'{HSTband:>3}, {scalepc:>2}pc: {len(lst)} galaxies, {np.sum(nebulae_tmp[\"overlap_neb\"]>0)} HII regions overlap ({np.sum(nebulae_tmp[\"1to1\"]):>4} 1to1)')\n",
    "\n",
    "        bands.append(HSTband)\n",
    "        scale.append(scalepc)\n",
    "        Ngal.append(len(lst))\n",
    "        Nneb.append(np.sum(nebulae_tmp[\"overlap_neb\"]>0))\n",
    "        Nneb_per.append(np.sum(nebulae_tmp[\"overlap_neb\"]>0)/np.sum(nebulae['in_frame']))\n",
    "        Nasc.append(np.sum(assoc_tmp['overlap_asc']>0))\n",
    "        Nasc_per.append(np.sum(assoc_tmp['overlap_asc']>0)/np.sum(assoc_in_frame['in_frame']))\n",
    "        N1to1.append(np.sum(assoc_tmp[\"1to1\"]>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Table([bands,scale,Nneb,Nasc,Nasc_per,N1to1],names=['Band','scale','Nneb','Nasc','asc_per','N1to1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "maybe later, a table that shows the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "astrosat_sample = np.unique(nebulae[~np.isnan(nebulae['FUV_FLUX'])]['gal_name'])\n",
    "muse_sample     = sample_table['name']\n",
    "hst_sample      = associations['gal_name']\n",
    "sitelle_sample  = ['NGC0628','NGC2835','NGC3351','NGC3627','NGC4535']\n",
    "\n",
    "t = Table({\n",
    "    'name':muse_sample,\n",
    "    'MUSE':np.isin(muse_sample,muse_sample),\n",
    "    'HST':np.isin(muse_sample,hst_sample),\n",
    "    'Astrosat':np.isin(muse_sample,astrosat_sample),\n",
    "    'Sitelle':np.isin(muse_sample,sitelle_sample)}\n",
    "     )\n",
    "\n",
    "for col in t.columns[1:]:\n",
    "    t[col] = ['\\checkmark' if x else '' for x in t[col] ]\n",
    "ascii.write(t,sys.stdout, Writer = ascii.Latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catalogue for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "columns = ['gal_name','region_ID','assoc_ID',\n",
    "           'ra_neb','dec_neb','ra_asc','dec_asc',\n",
    "           'age','age_err','mass','mass_err',\n",
    "           'EBV_stars','EBV_stars_err','overlap_asc','overlap',\n",
    "           'logq_D91','logq_D91_err','Delta_met_scal','density','density_err',\n",
    "           'HA6562_LUM_CORR','HA6562_LUM_CORR_ERR','HA/FUV_corr','HA/FUV_corr_err',\n",
    "           'EW_HA','EW_HA_ERR','continuum_Halpha','continuum_Halpha_error',\n",
    "           'continuum_Halpha_bkg','continuum_Halpha_bkg_error',\n",
    "           'EBV_balmer','EBV_balmer_err','overlap_neb','neighbors','GMC_sep']\n",
    "\n",
    "\n",
    "doc = f'''matched catalogue for Scheuermann et al. (submitted)\n",
    "S/N cuts are not applied yet.\n",
    "The code for this project can be found here.\n",
    "https://github.com/fschmnn/cluster/\n",
    "last update: {date.today().strftime(\"%b %d, %Y\")}\n",
    "'''\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "for i,comment in enumerate(doc.split('\\n')):\n",
    "    if i==0:\n",
    "        primary_hdu.header['COMMENT'] = comment\n",
    "    else:\n",
    "        primary_hdu.header[''] = comment\n",
    "table_hdu   = fits.BinTableHDU(catalogue[columns])\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto('matched_catalogue.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot the sample (cutouts)\n",
    "\n",
    "plot all objects in the merged catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from cluster.plot import single_cutout\n",
    "\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria]\n",
    "\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "size=10*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_{HSTband}_{scalepc}pc'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                # nebulae mask\n",
    "                filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            ax = single_cutout(ax,\n",
    "                             position = row['SkyCoord_neb'],\n",
    "                             image = F275,\n",
    "                             mask1 = nebulae_mask,\n",
    "                             mask2 = associations_mask,\n",
    "                             label = f\"{row['gal_name']}: {row['region_ID']:.0f}/{row['assoc_ID']:.0f}\",\n",
    "                             size  = 4*u.arcsecond)\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "three color composit with CO emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "from pnlf.plot import create_RGB\n",
    "\n",
    "\n",
    "criteria = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_asc'] == 1)\n",
    "criteria &= (catalogue['age'] < 10)\n",
    "#tmp = catalogue[criteria]\n",
    "\n",
    "print(f'{len(tmp)} objects')\n",
    "\n",
    "size=5*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_rgb_{HSTband}_{scalepc}pc'\n",
    "\n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for j in range(Npages):\n",
    "        print(f'working on page {j+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[j*Npage:(j+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                print(f'reading files for {gal_name}')\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                \n",
    "                filename = data_ext/'MUSE'/'DR2.1'/'MUSEDAP'/ f'{gal_name}_MAPS.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                                    meta=hdul['HA6562_FLUX'].header,\n",
    "                                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    \n",
    "                # nebulae mask\n",
    "                filename = data_ext/'Products'/ 'Nebulae catalogue'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'HST'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "            \n",
    "                with fits.open(data_ext/'ALMA'/'v4p0'/f'{gal_name.lower()}_12m+7m+tp_co21_broad_tpeak.fits') as hdul:\n",
    "                    CO = NDData(data=hdul[0].data,\n",
    "                                meta=hdul[0].header,\n",
    "                                wcs=WCS(hdul[0].header))\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            \n",
    "            position = row['SkyCoord_neb']\n",
    "            \n",
    "            label = f\"{row['gal_name']}: {row['region_ID']:.0f}/{row['assoc_ID']:.0f}\"\n",
    "\n",
    "            cutout_F275 = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "            norm = simple_norm(cutout_F275.data,stretch='linear',clip=False,percent=99.9)\n",
    "\n",
    "            cutout_CO, _  = reproject_interp(CO,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "            cutout_Halpha, _  = reproject_interp(Halpha,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "\n",
    "            rgb = create_RGB(cutout_CO,cutout_Halpha,cutout_F275.data,\n",
    "                             percentile=[98,98,99.8],weights=[0.7,0.6,1])\n",
    "            #ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "            ax.imshow(rgb,origin='lower')\n",
    "\n",
    "            # plot the nebulae catalogue\n",
    "            cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape,order='nearest-neighbor')    \n",
    "            region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_mask)\n",
    "                blank_mask[cutout_mask==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=0.8,label='HII-region')\n",
    "\n",
    "            # 32 pc\n",
    "            cutout_32 = Cutout2D(associations_mask.data,position,size=size,wcs=associations_mask.wcs)\n",
    "            region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_32.data)\n",
    "                blank_mask[cutout_32.data==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='blue',lw=0.8,label='32pc assoc.')\n",
    "            t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "            t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if j == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Ha/FUV vs ionization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "with log q from Diaz+91\n",
    "\n",
    "$$\n",
    "\\log u = (-1.684±0.076)\\cdot \\log([SII]/[SIII])-(2.986 ±0.027)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, spearmanr\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "bins = np.linspace(*np.nanpercentile(catalogue['logq_D91'],[1,99]),10)\n",
    "xlim = [5.7,7.8]\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = 'logq_D91','EW_HA','HA/FUV_corr','Delta_met_scal'\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "table = nebulae.copy()\n",
    "table = table[table['logq_D91']>3*table['logq_D91_err']]\n",
    "#table = table[table['HA/FUV_corr']>3*table['HA/FUV_corr_err']]\n",
    "\n",
    "print(f'{len(table)} objects in sample')\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "sample_table.sort('mass')\n",
    "\n",
    "rho1,rho2,rho3 = [], [], []\n",
    "for i,gal_name in enumerate(sample_table['name']):\n",
    "    #print(gal_name)\n",
    "    tmp = table[table['gal_name']==gal_name]\n",
    "    \n",
    "    if np.any(np.isfinite(tmp['EW_HA'])):\n",
    "        t = tmp[tmp['EW_HA']>3*tmp['EW_HA_ERR']]\n",
    "        x,mean,std = bin_stat(t['logq_D91'],t['EW_HA'],bins=bins,statistic='median')\n",
    "        ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "        rho1.append(spearmanr(tmp['logq_D91'],tmp['EW_HA'],nan_policy='omit')[0])\n",
    "\n",
    "    if np.any(np.isfinite(tmp['HA/FUV_corr'])):\n",
    "        t = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err'])]\n",
    "        x,mean,std = bin_stat(t['logq_D91'],t[y2_name],bins=bins,statistic='median')\n",
    "        ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "        try:\n",
    "            rho2.append(spearmanr(t['logq_D91'],t['HA/FUV_corr'],nan_policy='omit')[0])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if np.any(np.isfinite(tmp['Delta_met_scal'])):\n",
    "        x,mean,std = bin_stat(tmp['logq_D91'],tmp['Delta_met_scal'],bins=bins,statistic='median')\n",
    "        ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "        rho3.append(spearmanr(tmp['logq_D91'],tmp['Delta_met_scal'],nan_policy='omit')[0])\n",
    "\n",
    "# plot contours\n",
    "for ax,y_name,y_name_err in zip([ax1,ax2,ax3],\n",
    "                     ['EW_HA','HA/FUV_corr','Delta_met_scal'],\n",
    "                     ['EW_HA_ERR','HA/FUV_corr_err',None]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    if y_name_err:\n",
    "        x,y = x[table[y_name]>3*table[y_name_err]], y[table[y_name]>3*table[y_name_err]]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "    print(f'{y_name}: {spearmanr(x,y,nan_policy=\"omit\")[0]:.2f}')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "    \n",
    "\n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],bins=bins,statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "\n",
    "ax1.set(xlim=xlim, yscale='log',ylim=[3,5e2],\n",
    "        xlabel=r'$\\log q$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "ax2.set(xlim=xlim, yscale='log',ylim=[1,1e2],\n",
    "        xlabel=r'$\\log q$',ylabel=r'H$\\alpha$/FUV')\n",
    "ax3.set(xlim=xlim,ylim=[-0.15,0.15],\n",
    "        xlabel=r'$\\log q$',ylabel=r'$\\Delta$(O/H)')\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax3.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "\n",
    "# get a second x axis on top with [SIII]/[SII]\n",
    "xmin,xmax=ax1.get_xlim()\n",
    "ax1_top = ax1.twiny()\n",
    "ax1_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax1_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax1_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "ax2_top = ax3.twiny()\n",
    "ax2_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax2_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax2_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "ax3_top = ax2.twiny()\n",
    "ax3_top.set(xlim=[logq_D91_reverse(xmin),logq_D91_reverse(xmax)],xscale='log')\n",
    "ax3_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax3_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "for i, rho in enumerate([rho1,rho2,rho3]):\n",
    "    print(f'plot{i}: rho={np.nanmean(rho):.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.6])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$\\log (m/\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "plt.savefig(basedir/'reports'/'nebulae_correlations.pdf',dpi=600)\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "histogram/binned stat with log bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = '[SIII]/[SII]', 'Delta_met_scal','HA/FUV_corr','eq_width'\n",
    "xlim,ylim1,ylim2,ylim3 = [1e-2,1],[-0.15,0.15],[8e-1,8e1],[3,3e2]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3.2))\n",
    "\n",
    "table = nebulae.copy()\n",
    "table = table[(table['HA/FUV_corr']>3*table['HA/FUV_corr_err']) | np.isnan(table['FUV_FLUX'])]\n",
    "table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "sample_table.sort('mass')\n",
    "\n",
    "# the density histogram\n",
    "\n",
    "nbins=20\n",
    "\n",
    "x,y = table[x_name],table[y1_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim1,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax1.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y2_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim2),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax2.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y3_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim3),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax3.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "# the bins for the binned mean\n",
    "#bins = np.logspace(-1.9,-0.2,10)\n",
    "bins = np.logspace(-2.4,-0.4,12)\n",
    "\n",
    "for i,gal_name in enumerate(sample_table['name']):\n",
    "    #print(gal_name)\n",
    "    \n",
    "    tmp = table[table['gal_name']==gal_name]\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],bins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],bins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],bins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "    \n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],bins=bins,statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "\n",
    "ax1.set(xscale='log',xlim=xlim,ylim=ylim1,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "ax2.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim2,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "ax3.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim3,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax1.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.73])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$\\log (m/\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the same plot but now binned by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "x_name,y1_name,y2_name,y3_name = '[SIII]/[SII]', 'Delta_met_scal','HA/FUV','eq_width'\n",
    "xlim,ylim1,ylim2,ylim3 = [1e-2,1],[-0.15,0.15],[8e-1,8e1],[3,3e2]\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3.2))\n",
    "\n",
    "table = catalogue.copy()\n",
    "#table = table[(table['FUV_FLUX_CORR']>3*table['FUV_FLUX_CORR_ERR']) | np.isnan(table['FUV_FLUX_CORR'])]\n",
    "table = table[table['[SIII]/[SII]']>3*table['[SIII]/[SII]_err']]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=0,vmax=20)\n",
    "\n",
    "\n",
    "nbins=20\n",
    "\n",
    "x,y = table[x_name],table[y1_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim1,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax1.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y2_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim2),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax2.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "x,y = table[x_name],table[y3_name]\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim3),nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "ax3.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "# the bins for the binned mean\n",
    "#bins = np.logspace(-1.9,-0.2,10)\n",
    "bins = np.logspace(-2.4,-0.4,12)\n",
    "\n",
    "\n",
    "tmp = catalogue #[catalogue['mass']>1e4]\n",
    "\n",
    "for age in [(0,2),(2,5),(5,10),(10,20)]:\n",
    "    age_mean = np.mean(age)\n",
    "    tmp = table[(table['age']>age[0]) & (table['age']<age[1])]\n",
    "    print(f'{age_mean}: {len(tmp)} objects')\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y1_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y2_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "    x,mean,std = bin_stat(tmp[x_name],tmp[y3_name],[None,None],nbins=bins,statistic='median')\n",
    "    ax3.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_mean)),label=age_mean)\n",
    "\n",
    "# plot contours\n",
    "\n",
    "for ax,y_name in zip([ax1,ax2,ax3],[y1_name,y2_name,y3_name]):\n",
    "    \n",
    "    x,y = table[x_name],table[y_name]\n",
    "    #ax1.scatter(x,y,s=0.5,color='black')\n",
    "\n",
    "    # just ignore nan values\n",
    "    x = x[~np.isnan(y) & np.isfinite(y)]\n",
    "    y = y[~np.isnan(y) & np.isfinite(y)]\n",
    "    \n",
    "    x,mean,std = bin_stat(table[x_name],table[y_name],[None,None],nbins=bins,statistic='median')\n",
    "    #ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')\n",
    "\n",
    "    \n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['mass'],cmap=cmap,vmin=0,vmax=12)\n",
    "\n",
    "ax1.set(xscale='log',xlim=xlim,ylim=ylim1,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "ax2.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim2,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "ax3.set(xscale='log',xlim=xlim, yscale='log',ylim=ylim3,\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$')\n",
    "\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax1.yaxis.set_major_locator(mpl.ticker.MultipleLocator(0.1))\n",
    "ax1.yaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "ax3.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax3.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))   \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85,wspace=0.4)\n",
    "cbar_ax = fig.add_axes([0.87, 0.21, 0.02, 0.73])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'age / Myr',ticks=np.arange(0,20,4))\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'nebulae_correlations_age_bin.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "a dedicated subplot for each galaxie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, pearsonr, spearmanr\n",
    "\n",
    "sample = set(astrosat_sample) & set(muse_sample)\n",
    "filename = basedir/'reports'/'all_objects_HaFUV_over_SII'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "    \n",
    "#vmin,vmax = np.min(catalogue['HA6562_FLUX_CORR']),np.max(catalogue['HA6562_FLUX_CORR'])\n",
    "vmin,vmax = 1e-16,1e-14\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample):  \n",
    "    \n",
    "    tmp = nebulae[(nebulae['gal_name']==name)]\n",
    "        \n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    tmp = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err']) | np.isnan(tmp['FUV_FLUX'])]\n",
    "    tmp = tmp[tmp['[SIII]/[SII]']>3*tmp['[SIII]/[SII]_err']]\n",
    "\n",
    "    r,p = spearmanr(tmp['[SIII]/[SII]'],tmp['HA/FUV_corr'])\n",
    "    print(f'{name}: rho={r:.2f}, {len(tmp)} objects')\n",
    "\n",
    "    sc = ax.scatter(tmp['[SIII]/[SII]'],tmp['HA/FUV_corr'],\n",
    "               c=1e-20*tmp['HA6562_FLUX_CORR'],vmin=vmin,vmax=vmax,\n",
    "               cmap=plt.cm.plasma,\n",
    "               norm=mpl.colors.LogNorm(),\n",
    "               s=1,marker='.')\n",
    "    \n",
    "    #q = 98\n",
    "    #bins = np.logspace(*np.log10(np.percentile(tmp['[SIII]/[SII]'],[100-q,q])),10)\n",
    "    #x,mean,std = bin_stat(tmp['[SIII]/[SII]'],tmp['HA/FUV'],[None,None],nbins=bins)\n",
    "    #ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label=gal_name)\n",
    "    \n",
    "    ax.text(0.05,0.9,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.75,0.15,r'$\\rho$'+f'={r:.2f}',transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.62,0.05,f'{len(tmp):.0f} objects', transform=ax.transAxes,fontsize=7)\n",
    "    \n",
    "    ax.set(xscale='log',yscale='log',xlim=[1e-2,1],ylim=[2,4e2])\n",
    "    # https://stackoverflow.com/questions/21920233/matplotlib-log-scale-tick-label-number-formatting/33213196\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(r'H$\\alpha$ / FUV')\n",
    "\n",
    "        \n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    \n",
    "    if i==0:\n",
    "        #ax.remove()\n",
    "        ax.axis('off')\n",
    "        cbar = fig.colorbar(sc, ax=ax,\n",
    "                            label=r'$\\mathrm{H}\\alpha$ / (erg s$^{-1}$ cm$^{-2}$ Hz$^{-1}$)',\n",
    "                            orientation='horizontal',\n",
    "                           )\n",
    "    else:\n",
    "        ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "\n",
    "\n",
    "#plt.savefig(filename.with_suffix('.png'),dpi=600)\n",
    "#plt.savefig(filename.with_suffix('.pdf'),dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = nebulae[(nebulae['gal_name']=='NGC4254')]\n",
    "#tmp = tmp[(tmp['HA/FUV_corr']>3*tmp['HA/FUV_corr_err']) | np.isnan(tmp['FUV_FLUX'])]\n",
    "tmp = tmp[tmp['[SIII]/[SII]']>3*tmp['[SIII]/[SII]_err']]\n",
    "\n",
    "tmp[['HA/FUV_corr','HA/FUV_corr_err']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### age histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0.5,10.5,1)\n",
    "\n",
    "tmp = associations[(associations['mass']>1e4)] #& (associations['age']<=10)]\n",
    "print(len(tmp))\n",
    "ages_con = tmp[tmp['overlap']=='contained']['age']\n",
    "ages_par = tmp[tmp['overlap']=='partial']['age']\n",
    "ages_iso = tmp[(tmp['overlap']=='isolated') & (tmp['in_frame'])]['age']\n",
    "\n",
    "print(f'ages: con={np.median(ages_con):.2f}, par={np.median(ages_par):.2f}, iso={np.median(ages_iso):.2f}')\n",
    "\n",
    "for overlap,ax in zip(['contained','partial','isolated'],[ax1,ax2,ax3]):\n",
    "    sample = tmp[tmp['overlap']==overlap]\n",
    "    n = np.sum(sample['age']<=2)\n",
    "    #  r'$2 \\leq \\mathrm{age / Myr}$: '+\n",
    "    text =f'young: {n:>5} ({100*n/len(sample):.0f}\\%)\\n'\n",
    "    n = np.sum((sample['age']>2) & (sample['age']<=10))\n",
    "    # r'$2 < \\mathrm{age / Myr} \\leq 10$: '+\n",
    "    text += f'intermediate: {n:>5} ({100*n/len(sample):.0f}\\%)\\n'\n",
    "    n = np.sum(sample['age']>10)\n",
    "    #r'$\\mathrm{age / Myr} < 10$: '+\n",
    "    text += f'old: {n:>5} ({100*n/len(sample):.0f}\\%)'\n",
    "    ax.text(0.95,0.8,text, transform=ax.transAxes,\n",
    "             color='black',fontsize=8,horizontalalignment='right')\n",
    "    \n",
    "n1,_,_=ax1.hist(ages_con,bins=bins,histtype='step',label='contained')\n",
    "n2,_,_=ax2.hist(ages_par,bins=bins,histtype='step',label='partially')\n",
    "n3,_,_=ax3.hist(ages_iso,bins=bins,histtype='step',label='isolated')\n",
    "\n",
    "percentiels = np.nanpercentile(ages_con,[15.85,50,84.15])\n",
    "minus,plus  = np.ediff1d(percentiels)\n",
    "ax1.set_title(f'contained (${percentiels[1]:.1f}^{{+{plus:.1f}}}_{{-{minus:.1f}}}$ Myr)')\n",
    "percentiels = np.nanpercentile(ages_par,[15.85,50,84.15])\n",
    "minus,plus  = np.ediff1d(percentiels)\n",
    "ax2.set_title(f'partial (${percentiels[1]:.1f}^{{+{plus:.1f}}}_{{-{minus:.1f}}}$ Myr)')\n",
    "percentiels = np.nanpercentile(ages_iso,[15.85,50,84.15])\n",
    "minus,plus  = np.ediff1d(percentiels)\n",
    "ax3.set_title(f'isolated (${percentiels[1]:.1f}^{{+{plus:.1f}}}_{{-{minus:.1f}}}$ Myr)')\n",
    "\n",
    "ymax = np.max([n1,n2,n3])\n",
    "ymax = 1.1*np.round(ymax,-int(np.log10(ymax))+1)\n",
    "ymax = 1150\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,ymax],xlim=[0,10],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_contained.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "np.std(np.median(np.random.choice(ages_con,size=(N,len(ages_con)),replace=True),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "tmp = associations[(associations['mass']>1e4)]\n",
    "idx,sep,_=match_coordinates_sky(tmp['SkyCoord_asc'],nebulae['SkyCoord_neb'])\n",
    "\n",
    "ages1 = tmp[(sep<0.4*u.arcsec)]['age']\n",
    "ages2 = tmp[(sep>0.4*u.arcsec) & (sep<0.8*u.arcsec)]['age']\n",
    "ages3 = tmp[(sep>0.8*u.arcsec)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'$s<0.4\"$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'$0.4\"<s<0.8\"$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'$0.8\"<s$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,1100],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_sep.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = catalogue[(catalogue['mass']>=1e4) ]  # & (catalogue['age']<10)]\n",
    "print(len(tmp))\n",
    "\n",
    "cont = tmp['continuum_Halpha']-tmp['continuum_Halpha_bkg']\n",
    "cont_err = np.sqrt(tmp['continuum_Halpha_error']**2+tmp['continuum_Halpha_bkg_error']**2)\n",
    "tmp['EW_HA'][cont/cont_err<5] = np.nan\n",
    "tmp['EW_HA_CORR'][cont/cont_err<5] = np.nan\n",
    "\n",
    "p1,p2=np.nanpercentile(tmp['EW_HA'],[33,66])\n",
    "\n",
    "ages1 = tmp[tmp['EW_HA']<p1][\"age\"]\n",
    "ages2 = tmp[(tmp['EW_HA']>p1) & (tmp['EW_HA']<p2)][\"age\"]\n",
    "ages3 = tmp[(tmp['EW_HA']>p2)][\"age\"]\n",
    "\n",
    "print(f'median age: 1={np.median(ages1):.2f}, 2={np.median(ages2):.2f}, 3={np.median(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0.5,10.5,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "\n",
    "percentiels = np.nanpercentile(ages1,[15.85,50,84.15])\n",
    "minus,plus  = np.ediff1d(percentiels)\n",
    "ax1.set_title(r'1$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' (${percentiels[1]:.1f}^{{+{plus:.1f}}}_{{-{minus:.1f}}}$ Myr)')\n",
    "percentiels = np.nanpercentile(ages2,[15.85,50,84.15])\n",
    "minus,plus  = np.ediff1d(percentiels)\n",
    "ax2.set_title(r'2$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' (${percentiels[1]:.1f}^{{+{plus:.1f}}}_{{-{minus:.1f}}}$ Myr)')\n",
    "percentiels = np.nanpercentile(ages3,[15.85,50,84.15])\n",
    "minus,plus  = np.ediff1d(percentiels)\n",
    "ax3.set_title(r'3$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' (${percentiels[1]:.1f}^{{+{plus:.1f}}}_{{-{minus:.1f}}}$ Myr)')\n",
    "\n",
    "\n",
    "for sample,ax in zip([ages1,ages2,ages3],[ax1,ax2,ax3]):\n",
    "    \n",
    "    n = np.sum(sample<=2)\n",
    "    #  r'$2 \\leq \\mathrm{age / Myr}$: '+\n",
    "    text =f'young: {n:>5} ({100*n/len(sample):.0f}\\%)\\n'\n",
    "    n = np.sum((sample>2) & (sample<=10))\n",
    "    # r'$2 < \\mathrm{age / Myr} \\leq 10$: '+\n",
    "    text += f'intermediate: {n:>5} ({100*n/len(sample):.0f}\\%)\\n'\n",
    "    n = np.sum(sample>10)\n",
    "    #r'$\\mathrm{age / Myr} < 10$: '+\n",
    "    text += f'old: {n:>5} ({100*n/len(sample):.0f}\\%)'\n",
    "    ax.text(0.95,0.8,text, transform=ax.transAxes,\n",
    "             color='black',fontsize=8,horizontalalignment='right')\n",
    "\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,180],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/f'age_hist_eq_width.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.sum(~np.isnan(tmp['EW_HA']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = catalogue[(catalogue['mass']>1e4) ]  # & (catalogue['age']<10)]\n",
    "print(len(tmp))\n",
    "\n",
    "p1,p2=np.nanpercentile(tmp['HA/FUV_corr'],[33,66])\n",
    "\n",
    "ages1 = tmp[tmp['HA/FUV_corr']<p1]\n",
    "ages2 = tmp[(tmp['HA/FUV_corr']>p1) & (tmp['HA/FUV_corr']<p2)]\n",
    "ages3 = tmp[(tmp['HA/FUV_corr']>p2)]\n",
    "\n",
    "print(f'median age: 1={np.median(ages1[\"age\"]):.2f}, 2={np.median(ages2[\"age\"]):.2f}, 3={np.median(ages3[\"age\"]):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0.5,10.5,1)\n",
    "\n",
    "ax1.hist(ages1[\"age\"],bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2[\"age\"],bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3[\"age\"],bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'1$^\\mathrm{st}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.median(ages1[\"age\"]):.2f} Myr)')\n",
    "ax2.set_title(r'2$^\\mathrm{nd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$' +f' ({np.median(ages2[\"age\"]):.2f} Myr)')\n",
    "ax3.set_title(r'3$^\\mathrm{rd}$ percentile in $\\mathrm{EW}(\\mathrm{H}\\alpha)$'+f' ({np.median(ages3[\"age\"]):.2f} Myr)')\n",
    "\n",
    "for sample,ax in zip([ages1,ages2,ages3],[ax1,ax2,ax3]):\n",
    "    \n",
    "    n = np.sum(sample['age']<=2)\n",
    "    #  r'$2 \\leq \\mathrm{age / Myr}$: '+\n",
    "    text =f'young: {n:>5} ({100*n/len(sample):.0f}\\%)\\n'\n",
    "    n = np.sum((sample['age']>2) & (sample['age']<=10))\n",
    "    # r'$2 < \\mathrm{age / Myr} \\leq 10$: '+\n",
    "    text += f'intermediate: {n:>5} ({100*n/len(sample):.0f}\\%)\\n'\n",
    "    n = np.sum(sample['age']>10)\n",
    "    #r'$\\mathrm{age / Myr} < 10$: '+\n",
    "    text += f'old: {n:>5} ({100*n/len(sample):.0f}\\%)'\n",
    "    ax.text(0.95,0.8,text, transform=ax.transAxes,\n",
    "             color='black',fontsize=8,horizontalalignment='right')\n",
    "\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,180],xlabel='age / Myr')\n",
    "#plt.savefig(basedir/'reports'/f'age_hist_HaFUV.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass-to-Halpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use age bins instead of galaxy mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import spearmanr, binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(nrows=2,figsize=(single_column,single_column*(1+1/1.618+0.1)),\n",
    "                          gridspec_kw={'height_ratios': [1, 1.618]})\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<10)\n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "#tmp = catalogue.copy()\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[2e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "vmin,vmax=0,10\n",
    "nbins = 8\n",
    "bins = np.logspace(2,5.4,nbins)\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis',5)\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "age_bins = [0,2,4,6,8,10]\n",
    "for i in range(len(age_bins)-1):\n",
    "\n",
    "    sub = tmp[(tmp['age']>=age_bins[i]) & (tmp['age']<age_bins[i+1])]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    x,mean,std = bin_stat(x,y,bins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_bins[i]+1)))\n",
    "\n",
    "rho = spearmanr(tmp['mass'],tmp['HA6562_LUM_CORR'],nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "#x,mean,std = bin_stat(x,y,[None,None],nbins=bins,statistic='median')\n",
    "#ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')    \n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['distance'],cmap=cmap,vmin=0,vmax=12)\n",
    "\n",
    "# ----------------------- ax2 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>0) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "#tmp = catalogue.copy()\n",
    "print(f'ax1: {len(tmp)} objects')\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax2.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=vmin,vmax=vmax,cmap=cmap,rasterized=True)\n",
    "EBV_balmer_err = np.mean(tmp['EBV_balmer_err'])\n",
    "EBV_stars_err = np.mean(tmp['EBV_stars_err'])\n",
    "ax2.errorbar(0.58,0.08,xerr=EBV_stars_err,yerr=EBV_balmer_err,fmt='ko',ms=0)\n",
    "\n",
    "ax2.plot([0,1],[0,2],color='black')\n",
    "ax2.plot([0,2],[0,2],color='black')\n",
    "ax2.set(xlim=[0,0.7],ylim=[0,0.7],xlabel=r'$E(B-V)$ stellar',ylabel=r'$E(B-V)$ Balmer')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(top=0.89)\n",
    "cax = fig.add_axes([0.125, 0.9, 0.78, 0.02])\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal',ticklocation='top')\n",
    "#cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar.set_ticks([0,2,4,6,8,10])\n",
    "cbar.set_ticklabels([0,2,4,6,8,'10'])\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'mass_HA_EBV.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax1=plt.subplots(nrows=1,figsize=(single_column,single_column/1.618))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<=8) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "vmin,vmax = 0,12\n",
    "nbins = 8\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('viridis',6)\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "rho = []\n",
    "age_bins = [0,2,4,6,8,10,12]\n",
    "for i in range(len(age_bins)-1):\n",
    "\n",
    "    sub = tmp[(tmp['age']>=age_bins[i]) & (tmp['age']<age_bins[i+1])]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    x,mean,std = bin_stat(x,y,bins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_bins[i]+1)))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=19*[1],cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "#divider = make_axes_locatable(ax1)\n",
    "#cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "#cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "#cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar = fig.colorbar(sc,label='age / Myr')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'tmp_mass_HA.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from scipy.stats import spearmanr, gaussian_kde, binned_statistic, binned_statistic_2d\n",
    "\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "#criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.2))\n",
    "\n",
    "x,y,z = tmp['mass'],tmp['HA6562_LUM_CORR'],tmp['age']\n",
    "\n",
    "\n",
    "xlim =[1e2,1e5]\n",
    "ylim = [5e35,8e39]\n",
    "nbins = 25\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.logspace(*np.log10(ylim),nbins)]\n",
    "\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins)\n",
    "stat, x_e, y_e,_ = binned_statistic_2d(x,y,z,bins=bins)\n",
    "#stat[hist<5] = np.nan\n",
    "img = ax.pcolormesh(x_e,y_e,stat.T,cmap=plt.cm.viridis,vmin=0,vmax=10)\n",
    "\n",
    "#sc = ax.scatter(x,y,c=z,vmin=0,vmax=10)\n",
    "x_stat,mean,std = bin_stat(x,y,bins=bins[0],statistic='median')\n",
    "ax.errorbar(x_stat,mean,fmt='o-',ms=1.5,color='white')\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax.text(0.05,0.93,label,transform=ax.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "fig.colorbar(img,label='age / Myr')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'mass_HA_age.png',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.stats import spearmanr, binned_statistic\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(nrows=2,figsize=(single_column,single_column*2))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e2) \n",
    "criteria &= (catalogue['age']<=8) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "nbins = 8\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "rho = []\n",
    "for k,gal_name in enumerate(np.unique(sample_table['name'])):\n",
    "\n",
    "    sub = tmp[tmp['gal_name']==gal_name]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    if len(x)>5:\n",
    "        rho.append(spearmanr(x,y,nan_policy='omit')[0]) \n",
    "    x,mean,std = bin_stat(x,y,range=xlim,bins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "#x,mean,std = bin_stat(x,y,[None,None],nbins=bins,statistic='median')\n",
    "#ax.errorbar(x,mean,fmt='o-',ms=2.5,color='black')    \n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=sample_table['distance'],cmap=cmap,vmin=9.4,vmax=11)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='log M / M$_\\odot$',cax=cax,orientation='horizontal',\n",
    "                    ticks=[9.5,10,10.5,11])\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "# ----------------------- ax2 ------------------------------------------\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'ax1: {len(tmp)} objects')\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis',10)\n",
    "\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax2.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=cmap)\n",
    "EBV_balmer_err = np.mean(tmp['EBV_balmer_err'])\n",
    "EBV_stars_err = np.mean(tmp['EBV_stars_err'])\n",
    "ax2.errorbar(0.5,0.1,xerr=EBV_stars_err,yerr=EBV_balmer_err,fmt='ko',ms=0)\n",
    "\n",
    "ax2.plot([0,1],[0,2],color='black')\n",
    "ax2.plot([0,2],[0,2],color='black')\n",
    "ax2.set(xlim=[0,0.7],ylim=[0,0.7],xlabel=r'$E(B-V)$ stars',ylabel=r'$E(B-V)$ Balmer')\n",
    "\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar.set_ticks([0,2,4,6,8,10])\n",
    "cbar.set_ticklabels([0,2,4,6,8,'10+'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'mass_HA_EBV.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Halpha luminosity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "tmp = nebulae[(nebulae['overlap_neb']==0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='unmatched',zorder=0)\n",
    "print(f\"isolated: {np.mean(tmp['HA6562_LUM_CORR']):.2g}\")\n",
    "\n",
    "tmp = nebulae[(nebulae['overlap_neb']>0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='matched',zorder=0)\n",
    "print(f\"overlapping: {np.mean(tmp['HA6562_LUM_CORR']):.2g}\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='linear',ylim=[1,2.2e3],ylabel='N')\n",
    "plt.savefig(basedir/'reports'/'Halpha_luminosity_function.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neb_overlap = np.sum((nebulae['overlap_neb']>0) & (nebulae['in_frame'])) \n",
    "neb_in_frame = np.sum(nebulae['in_frame'])\n",
    "print(f'{neb_overlap} of {neb_in_frame} HII regions overlap ({100*neb_overlap/neb_in_frame:.1f}%)')\n",
    "\n",
    "asc_overlap = np.sum((associations['overlap_asc']>0) & (associations['in_frame'])) \n",
    "asc_in_frame = np.sum(associations['in_frame'])\n",
    "print(f'{asc_overlap} of {asc_in_frame} associations overlap ({100*asc_overlap/asc_in_frame:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.linspace(0.5,10.5,10)\n",
    "\n",
    "tmp = associations[(associations['overlap_asc']==1) & (associations['in_frame'])]\n",
    "print(np.mean(tmp['age']))\n",
    "ax.hist(tmp['age'],bins=bins,alpha=0.6,label='contained')\n",
    "\n",
    "tmp = associations[(associations['overlap_asc']<1) & (associations['overlap_asc']>0) & (associations['in_frame'])]\n",
    "print(np.mean(tmp['age']))\n",
    "ax.hist(tmp['age'],bins=bins,alpha=0.6,label='partial')\n",
    "\n",
    "tmp = associations[(associations['overlap_asc']==0) & (associations['in_frame'])]\n",
    "print(np.mean(tmp['age']))\n",
    "ax.hist(tmp['age'],bins=bins,alpha=0.6,label='isolated')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='linear',xlim=[1,10],xlabel=r'age / Myr',\n",
    "       yscale='linear',ylabel='N')\n",
    "#plt.savefig(basedir/'reports'/'age_hist.png',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### EBV\n",
    "\n",
    "we expect \n",
    "$$\n",
    "E(B-V)_{balmer} = 2 \\cdot E(B-V)_{stars}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis',10)\n",
    "fig = plt.figure(figsize=(single_column,single_column/1.1))\n",
    "ax = fig.add_subplot()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=\"10%\", pad=0.2,)\n",
    "\n",
    "xlim = [0,10]\n",
    "#ax.errorbar(tmp['EBV_stars'],tmp['EBV_balmer'],xerr=tmp['EBV_stars_err'],yerr=tmp['EBV_balmer_err'],ms=0,fmt='ok')\n",
    "sc=ax.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=cmap)\n",
    "#x,mean,std = bin_stat(tmp['EBV_stars'],tmp['EBV_balmer'],[0,0.7],nbins=7,statistic='mean')\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='red')\n",
    "EBV_balmer_err = np.median(tmp['EBV_balmer_err'])\n",
    "EBV_stars_err = np.median(tmp['EBV_stars_err'])\n",
    "ax.errorbar(0.5,0.1,xerr=EBV_stars_err,yerr=EBV_balmer_err,fmt='ko',ms=0)\n",
    "\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel=r'$E(B-V)$ stars',ylabel=r'$E(B-V)$ Balmer')\n",
    "cbar = fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "cbar.set_ticks([0,2,4,6,8,10])\n",
    "cbar.set_ticklabels([0,2,4,6,8,'10+'])\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'EBV_Balmer_vs_Stars_{HSTband}_{scalepc}pc.pdf',dpi=600)\n",
    "#plt.savefig(basedir/'reports'/f'EBV_Balmer_vs_Stars_{HSTband}_{scalepc}pc.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "age_bins = np.array([0,2,4,6,8,10])\n",
    "\n",
    "for low,high in zip(age_bins[:-1],age_bins[1:]):\n",
    "    t = tmp[(tmp['age']>low) & (tmp['age']<high)]\n",
    "    r,p = spearmanr(t['EBV_stars'],t['EBV_balmer'])\n",
    "    print(f'{low} to {high} Myr: rho={r:.2f}, {len(t)} objects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Temperature and density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.scatter(catalogue['age'],catalogue['density'])\n",
    "x,mean,std = bin_stat(catalogue['age'],catalogue['density'],[0,10],nbins=10,statistic='median')\n",
    "ax.errorbar(x,mean,yerr=None,fmt='-',color='black')\n",
    "\n",
    "ax.set(xlim=[0,10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GMC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sep = 4\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=2)\n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "print(f'{np.sum(criteria)} objects are 2 Myr or younger')\n",
    "\n",
    "tmp1 = catalogue[criteria & (catalogue['GMC_sep']>=sep)]\n",
    "print(f'{len(tmp1)} not associated with GMC')\n",
    "print(f'EW(Ha): {np.mean(tmp1[\"eq_width\"]):.1f}')\n",
    "print(f'Ha/FUV: {np.nanmean(tmp1[\"HA/FUV_corr\"][np.isfinite(tmp1[\"HA/FUV_corr\"])]):.1f}\\n')\n",
    "\n",
    "tmp2 = catalogue[criteria & (catalogue['GMC_sep']<sep)]\n",
    "print(f'{len(tmp2)} associated with GMC')\n",
    "print(f'EW(Ha): {np.mean(tmp2[\"eq_width\"]):.1f}')\n",
    "print(f'Ha/FUV: {np.nanmean(tmp2[\"HA/FUV_corr\"][np.isfinite(tmp2[\"HA/FUV_corr\"])]):.1f}\\n')\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['overlap_asc']==1)]\n",
    "print(f\"age GMC: {np.mean(tmp[tmp['GMC_sep']<sep]['age']):.2f}\")\n",
    "print(f\"age without GMC: {np.mean(tmp[tmp['GMC_sep']>=sep]['age']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "binned stats, grouped by Ha or GMC mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(figsize=(two_column,two_column/2),ncols=2)\n",
    "\n",
    "\n",
    "tmp = catalogue.copy()\n",
    "tmp['GMC_sep_pc'] = tmp['GMC_sep'] *u.arcsec.to(u.radian) * tmp['distance']*1e6\n",
    "tmp = tmp[tmp['GMC_sep_pc']<300]\n",
    "\n",
    "#sc=ax1.scatter(tmp['age'],tmp['GMC_sep_pc'],c=np.log10(tmp['HA6562_LUM_CORR']),vmin=36,vmax=40)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',3)\n",
    "norm = mpl.colors.Normalize(vmin=36,vmax=39)\n",
    "Ha_bins = [36,37,38,39]\n",
    "for i,Ha in enumerate(Ha_bins[:-1]):\n",
    "    sub = tmp[(tmp['HA6562_LUM_CORR']>10**Ha_bins[i]) & (tmp['HA6562_LUM_CORR']<10**Ha_bins[i+1])]\n",
    "    x, y = sub['age'], sub['GMC_sep_pc']\n",
    "    corner_binned_stat(x,y,ax1,bins=np.arange(0.5,11.5,1),color=cmap(norm(Ha+0.5)))\n",
    "ax1.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'$\\delta_\\mathrm{GMC}$ / pc')\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),ax=ax1,orientation='horizontal',location='top',\n",
    "             ticks=np.arange(36,40,1),label=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "\n",
    "cmap = mpl.cm.get_cmap('viridis',3)\n",
    "norm = mpl.colors.Normalize(vmin=5,vmax=8)\n",
    "mass_bins = [5,6,7,8]\n",
    "for i,mass in enumerate(mass_bins[:-1]):\n",
    "    sub = tmp[(tmp['GMC_mass']>10**mass_bins[i]) & (tmp['GMC_mass']<10**mass_bins[i+1])]\n",
    "    x, y = sub['age'], sub['GMC_sep_pc']\n",
    "    corner_binned_stat(x,y,ax2,bins=np.arange(0.5,11.5,1),color=cmap(norm(mass+0.5)))\n",
    "ax2.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'$\\delta_\\mathrm{GMC}$ / pc')\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),ax=ax2,orientation='horizontal',location='top',\n",
    "             ticks=np.arange(5,9,1),label=r'$M_\\mathrm{GMC} / \\mathrm{M}_\\odot$')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "looks better when using EW as a proxy for the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile\n",
    "from scipy.stats import spearmanr, binned_statistic\n",
    "\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(figsize=(single_column,single_column*1.23),nrows=2,sharex=True)\n",
    "\n",
    "bins = np.logspace(0.5,2.5,6)\n",
    "\n",
    "\n",
    "tmp = catalogue.copy()\n",
    "tmp['GMC_sep_pc'] = tmp['GMC_sep'] *u.arcsec.to(u.radian) * tmp['distance']*1e6\n",
    "tmp = tmp[tmp['GMC_sep_pc']<300]\n",
    "\n",
    "#sc=ax1.scatter(tmp['age'],tmp['GMC_sep_pc'],c=np.log10(tmp['HA6562_LUM_CORR']),vmin=36,vmax=40)\n",
    "\n",
    "# the first plot binned by Halpha\n",
    "\n",
    "cmap = mpl.cm.get_cmap('autumn',3)\n",
    "norm = mpl.colors.Normalize(vmin=36,vmax=39)\n",
    "\n",
    "#corner_binned_percentile(tmp['EW_HA'],tmp['GMC_sep_pc'],ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "#corner_binned_percentile(tmp['EW_HA'],tmp['GMC_sep_pc'],ax1,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "\n",
    "Ha_bins = [36,37,38,39]\n",
    "#Ha_bins = np.nanpercentile(tmp['HA6562_LUM_CORR'],[0,20,40,60,80,100])\n",
    "for i,Ha in enumerate(Ha_bins[:-1]):\n",
    "    sub = tmp[(tmp['HA6562_LUM_CORR']>10**Ha_bins[i]) & (tmp['HA6562_LUM_CORR']<10**Ha_bins[i+1])]\n",
    "    x, y = sub['EW_HA'], sub['GMC_sep_pc']\n",
    "    x, y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    mean, edges, _ = binned_statistic(x,y,statistic='median',bins=bins)\n",
    "    std, edges, _ = binned_statistic(x,y,statistic='std',bins=bins)\n",
    "    x = (edges[1:]+edges[:-1])/2\n",
    "    ax1.errorbar(x,mean,yerr=None,fmt='o-',ms=2,color=cmap(norm(Ha+0.5)))\n",
    "    ax1.fill_between(x,y1=mean-std,y2=mean+std,color=cmap(norm(Ha+0.5)),alpha=0.4)\n",
    "\n",
    "    \n",
    "corner_binned_stat(tmp['EW_HA'],tmp['GMC_sep_pc'],ax1,bins=bins,color='black')\n",
    "\n",
    "ax1.set(xscale='log',xlim=[5,250],ylabel=r'$\\delta_\\mathrm{GMC}$ / pc',ylim=[10,300])\n",
    "cbar1 = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),ax=ax1,orientation='vertical',location='right',\n",
    "             ticks=np.arange(36,40,1),label=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "cbar1.ax.set_yticklabels([f'$10^{{{i}}}$' for i in Ha_bins])\n",
    "\n",
    "# the second plot binned by GMC mass\n",
    "\n",
    "cmap = mpl.cm.get_cmap('summer',3)\n",
    "norm = mpl.colors.Normalize(vmin=5,vmax=8)\n",
    "\n",
    "#corner_binned_percentile(tmp['EW_HA'],tmp['GMC_sep_pc'],ax2,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "#corner_binned_percentile(tmp['EW_HA'],tmp['GMC_sep_pc'],ax2,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "\n",
    "                                          \n",
    "mass_bins = [5,6,7,8]\n",
    "for i,mass in enumerate(mass_bins[:-1]):\n",
    "    sub = tmp[(tmp['GMC_mass']>10**mass_bins[i]) & (tmp['GMC_mass']<10**mass_bins[i+1])]\n",
    "    x, y = sub['EW_HA'], sub['GMC_sep_pc']\n",
    "    x, y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    mean, edges, _ = binned_statistic(x,y,statistic='median',bins=bins)\n",
    "    std, edges, _ = binned_statistic(x,y,statistic='std',bins=bins)\n",
    "    x = (edges[1:]+edges[:-1])/2\n",
    "    ax2.errorbar(x,mean,yerr=None,fmt='o-',ms=2,color=cmap(norm(mass+0.5)))\n",
    "    ax2.fill_between(x,y1=mean-std,y2=mean+std,color=cmap(norm(mass+0.5)),alpha=0.4)\n",
    "\n",
    "corner_binned_stat(tmp['EW_HA'],tmp['GMC_sep_pc'],ax2,bins=bins,color='black')\n",
    "\n",
    "ax2.set(xscale='log',xlim=[5,250],xlabel=r'EW(Ha) / \\AA',ylabel=r'$\\delta_\\mathrm{GMC}$ / pc',ylim=[10,300])\n",
    "cbar2 = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),ax=ax2,orientation='vertical',location='right',\n",
    "             ticks=np.arange(5,9,1),label=r'$M_\\mathrm{GMC}\\, /\\, \\mathrm{M}_\\odot$')\n",
    "cbar2.ax.set_yticklabels([f'$10^{{{i}}}$' for i in mass_bins])\n",
    "\n",
    "#fig.suptitle('matched HII regions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'GMC_sep_by_EW_binned_matched.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "\n",
    "tmp = nebulae.copy()\n",
    "tmp['GMC_sep_pc'] = tmp['GMC_sep'] *u.arcsec.to(u.radian) * tmp['distance']*1e6\n",
    "tmp = tmp[tmp['GMC_sep_pc']<300]\n",
    "\n",
    "ax.scatter(tmp['GMC_mass'],tmp['HA6562_LUM_CORR'])\n",
    "r,p = spearmanr(tmp['GMC_mass'],tmp['HA6562_LUM_CORR'],nan_policy='omit')\n",
    "print(f'rho={r:.2f}, p-value={p:.2g}')\n",
    "ax.set(xlim=[5e4,5e8],xscale='log',ylim=[5e35,5e40],yscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile\n",
    "from scipy.stats import spearmanr, binned_statistic\n",
    "\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(figsize=(single_column,single_column*1.23),nrows=2)\n",
    "\n",
    "\n",
    "tmp = catalogue.copy()\n",
    "tmp['GMC_sep_pc'] = tmp['GMC_sep'] *u.arcsec.to(u.radian) * tmp['distance']*1e6\n",
    "tmp = tmp[tmp['GMC_sep_pc']<300]\n",
    "\n",
    "# 'bar','disc', have very few entries\n",
    "envs = ['interarm','interbar','spiral arms']\n",
    "\n",
    "\n",
    "bins = np.logspace(36,40,8)\n",
    "for i,env in enumerate(envs):\n",
    "    sub = tmp[tmp['env_neb']==env]\n",
    "    x, y = sub['HA6562_LUM_CORR'], sub['GMC_sep_pc']\n",
    "    x, y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    mean, edges, _ = binned_statistic(x,y,statistic='median',bins=bins)\n",
    "    std, edges, _ = binned_statistic(x,y,statistic='std',bins=bins)\n",
    "    x = (edges[1:]+edges[:-1])/2\n",
    "    ax1.errorbar(x,mean,yerr=None,fmt='o-',ms=2,label=env)\n",
    "    ax1.fill_between(x,y1=mean-std,y2=mean+std,alpha=0.4)\n",
    "ax1.legend()\n",
    "corner_binned_stat(tmp['HA6562_LUM_CORR'],tmp['GMC_sep_pc'],ax1,bins=bins,color='black')\n",
    "ax1.set(xscale='log',xlabel='Halpha luminosity',xlim=[1e36,1e40],ylabel=r'$\\delta_\\mathrm{GMC}$ / pc',ylim=[10,300])\n",
    "\n",
    "                            \n",
    "bins = np.logspace(5,8,8)\n",
    "for i,env in enumerate(envs):\n",
    "    sub = tmp[tmp['env_neb']==env]\n",
    "    x, y = sub['GMC_mass'], sub['GMC_sep_pc']\n",
    "    x, y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    mean, edges, _ = binned_statistic(x,y,statistic='median',bins=bins)\n",
    "    std, edges, _ = binned_statistic(x,y,statistic='std',bins=bins)\n",
    "    x = (edges[1:]+edges[:-1])/2\n",
    "    ax2.errorbar(x,mean,yerr=None,fmt='o-',ms=2,label=env)\n",
    "    ax2.fill_between(x,y1=mean-std,y2=mean+std,alpha=0.4)\n",
    "corner_binned_stat(tmp['GMC_mass'],tmp['GMC_sep_pc'],ax2,bins=bins,color='black')\n",
    "ax2.set(xscale='log',xlim=[1e5,1e8],xlabel=r'GMC mass',ylabel=r'$\\delta_\\mathrm{GMC}$ / pc',ylim=[10,300])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'GMC_sep_by_LHa_and_GMC_mass.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.scatter(tmp['age'],tmp['HA6562_LUM_CORR'])\n",
    "\n",
    "ax.set(xlim=[0,10],ylim=[1e36,1e40],yscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for paper\n",
    "\n",
    "#### define boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define limits and labels for corner plots\n",
    "limits   = {'age':(0.5,8.5),\n",
    "            'log_age':(5.9,7),\n",
    "            'age_mw':(0,20),\n",
    "            'met_scal':(8.3,8.7),\n",
    "            'Delta_met_scal':(-0.1,0.1),\n",
    "            'density':(0,150),\n",
    "            'dig/hii':(0.2,0.9),\n",
    "            'fesc':(0,1),\n",
    "            'HA/FUV':(0,200),\n",
    "            'HA/FUV_corr':(0.8,60),\n",
    "            'log_HA':(35.7,39.7),\n",
    "            'HA6562_LUM_CORR':(5e35,5e39),\n",
    "            'EBV_balmer':(0,1),\n",
    "            'logq_D91':(5.7,8),\n",
    "            'EW_HA':(6,600),\n",
    "            'EW_HA_CORR':(60,6000),\n",
    "            'temperature':(6000,8e3),\n",
    "            'log_mass':(4,5.5),\n",
    "            'GMC_sep':(0,10),\n",
    "            'GMC_sep_pc':(0,300),\n",
    "            'RSII':(1.,1.6)}\n",
    "\n",
    "\n",
    "labels   = {'age':'age / Myr',\n",
    "            'log_age' : r'$\\log (\\mathrm{age / Myr})$',\n",
    "            'age_mw':'age (stellar pops) / Myr',\n",
    "            'met_scal':'12+logO/H',\n",
    "            'Delta_met_scal':r'$\\Delta$(O/H)',\n",
    "            'density':r'density / cm$^{-3}$','temperature':'T / K',\n",
    "            'fesc':r'$f_\\mathrm{esc}$',\n",
    "            'HA/FUV':r'H$\\alpha$ / FUV',\n",
    "            'HA/FUV_corr':r'H$\\alpha$ / FUV',\n",
    "            'HA/FUV_bkg':r'H$\\alpha$ / FUV',\n",
    "            'OIII/HB':r'$[\\mathrm{O}\\,\\textsc{iii}]/\\mathrm{H}\\beta$',\n",
    "            'log_HA':r'$\\log (L(\\mathrm{H}\\alpha) / \\mathrm{erg s}^{-1})$',\n",
    "            'HA6562_LUM_CORR':r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "            'EBV_balmer':r'$E(B-V)_\\mathrm{Balmer}$',\n",
    "            'sb_HA':r'$SB_{\\mathrm{H}\\alpha}$',\n",
    "            'EW_HA' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$',\n",
    "            'EW_HA_CORR' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)_{\\mathrm{corr}}/\\mathrm{\\AA}$',\n",
    "            'eq_width_Ha' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$',\n",
    "            'eq_width_Ha_CORR' : r'$\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA}$',\n",
    "            'log_eq_width':r'$\\log (\\mathrm{EW}(\\mathrm{H}\\alpha)/\\mathrm{\\AA})$',\n",
    "            'logq_D91':r'$\\log q$',\n",
    "            'log[SIII]/[SII]':r'$\\log([\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}])$',\n",
    "            'T_N2_REFIT':'temperature / K',\n",
    "            'log_mass':r'$\\log M/\\mathrm{M}_\\odot$',\n",
    "            'GMC_sep':r'$\\delta_\\mathrm{GMC}$ / arcsec',\n",
    "            'GMC_sep_pc':r'$\\delta_\\mathrm{GMC}$ / pc',\n",
    "             'RSII':'RSII'}\n",
    "scale = {'eq_width':'log','HA/FUV':'log'}\n",
    "\n",
    "# calculate a few additional columns\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    catalogue['log_age'] = 6+np.log10(catalogue['age'])\n",
    "    catalogue['log_HA'] = np.log10(catalogue['HA6562_LUM_CORR'])\n",
    "    area_per_pixel = ((0.2*u.arcsec).to(u.rad).value*catalogue['distance']*u.Mpc).to(u.pc)**2\n",
    "    catalogue['sb_HA_pc']=catalogue['HA6562_FLUX_CORR']/(catalogue['area_neb']*area_per_pixel.value)\n",
    "    catalogue['sb_HA_arcsec']=catalogue['HA6562_FLUX_CORR']/(catalogue['area_neb']*0.2**2)\n",
    "    catalogue['log[SIII]/[SII]'] = np.log10(catalogue['[SIII]/[SII]'])\n",
    "    catalogue['log_mass'] = np.log10(catalogue['mass'])\n",
    "    catalogue['OIII/HB'] = catalogue['OIII5006_FLUX']/catalogue['HB4861_FLUX_CORR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to determine the right limits\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.hist(nebulae['EW_HA_CORR'],bins=np.logspace(0,4,20))\n",
    "ax2.hist(nebulae['HA/FUV_corr'],bins=np.logspace(0,2,20))\n",
    "ax3.hist(nebulae['logq_D91'],bins=np.linspace(5,8,20))\n",
    "\n",
    "for ax,col in zip([ax1,ax2,ax3],['EW_HA_CORR','HA/FUV_corr','logq_D91']):\n",
    "    ax.set(xlim=limits[col],xlabel=labels[col])\n",
    "ax1.set(xscale='log')\n",
    "ax2.set(xscale='log')\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"EW: {np.log10(np.nanpercentile(nebulae['EW_HA'],[2,98]))}\")\n",
    "print(f\"HA/FUV: {np.nanpercentile(nebulae['HA/FUV_corr'],[2,98])}\")\n",
    "print(f\"logq: {np.nanpercentile(nebulae['logq_D91'],[2,98])}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the sample here so all three plots use the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "criteria  &= (catalogue['overlap_asc']==1) \n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "tmp = tmp[(tmp['age']>=tmp['age_err'])]\n",
    "\n",
    "S2N = 5\n",
    "\n",
    "tmp['logq_D91'][tmp['logq_D91']/tmp['logq_D91_err']<S2N] = np.nan\n",
    "\n",
    "cont = tmp['continuum_Halpha']-tmp['continuum_Halpha_bkg']\n",
    "cont_err = np.sqrt(tmp['continuum_Halpha_error']**2+tmp['continuum_Halpha_bkg_error']**2)\n",
    "tmp['EW_HA'][cont/cont_err<5] = np.nan\n",
    "tmp['EW_HA_CORR'][cont/cont_err<5] = np.nan\n",
    "tmp['HA/FUV_corr'][tmp['HA/FUV_corr']/tmp['HA/FUV_corr_err']<S2N] = np.nan\n",
    "tmp['HA/FUV_corr'][tmp['HA/FUV_corr']<0] = np.nan\n",
    "# make sure the 3 galaxies without AstroSat are excluded\n",
    "tmp['HA/FUV_corr'][np.isin(tmp['gal_name'],['NGC1087','NGC4303','NGC5068'])]=np.nan\n",
    "\n",
    "\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "print(f'EW:      {np.sum(~np.isnan(tmp[\"EW_HA\"]))} objects')\n",
    "print(f'EW_corr: {np.sum(~np.isnan(tmp[\"EW_HA_CORR\"]))} objects')\n",
    "print(f'HA/FUV:  {np.sum(~np.isnan(tmp[\"HA/FUV_corr\"]))} objects')\n",
    "print(f'logq:    {np.sum(~np.isnan(tmp[\"logq_D91\"]))} objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trends with age tracers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "from scipy.stats import binned_statistic\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "    \n",
    "nbins = 8\n",
    "\n",
    "columns  = ['age','EW_HA','EW_HA_CORR','HA/FUV_corr','logq_D91']\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(nrows=4,figsize=(single_column,single_column*1.618),sharex=True)\n",
    "\n",
    "for col,ax in zip(columns[1:],[ax1,ax2,ax3,ax4]):\n",
    "    ax.set(ylim=limits[col])\n",
    "    x,y = tmp['age'],np.array(tmp[col])\n",
    "    x,y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=limits['age'],n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=limits['age'],n=98,color='gray',alpha=0.2)\n",
    "\n",
    "    mean, edges, _ = binned_statistic(x,y,statistic='median',bins=nbins,range=limits['age'])\n",
    "    xp = (edges[1:]+edges[:-1])/2\n",
    "    ax.errorbar(xp,mean,fmt='o-',ms=2,color='0.3')\n",
    "    print(', '.join([f'{100*x:>5.1f}%' for x in mean/mean[0]])+f' ({col})')\n",
    "    \n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    ax.set(ylabel=labels[col])\n",
    "ax1.set(yscale='log')\n",
    "ax1.set_yticks([10,100])\n",
    "ax1.set_yticklabels(['10','100'])\n",
    "\n",
    "ax2.set(yscale='log')\n",
    "ax2.set_yticks([100,1000,10000])\n",
    "ax2.set_yticklabels(['100','1,000',r'$10,000$'])\n",
    "\n",
    "ax3.set(yscale='log')\n",
    "ax3.set_yticks([1,10])\n",
    "ax3.set_yticklabels(['1','10'])\n",
    "\n",
    "ax4.set(xlabel='age / Myr')\n",
    "ymin,ymax=limits['logq_D91']\n",
    "ax4_right = ax4.twinx()\n",
    "ax4_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "ax4_right.set_ylabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax4_right.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = basedir/'reports'/f'age_tracers.pdf'\n",
    "plt.savefig(filename,dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def correlation(x,x_err,y,y_err,name='',sample_size=1000):\n",
    "\n",
    "\n",
    "    x_sampled = np.random.normal(loc=x,scale=x_err,size=(sample_size,len(x)))\n",
    "    y_sampled = np.random.normal(loc=y,scale=y_err,size=(sample_size,len(y)))\n",
    "    \n",
    "    x_sampled[x_sampled<1]=1\n",
    "    y_sampled[y_sampled<0]=0\n",
    "\n",
    "    rho_array,p_array = [],[]\n",
    "    for xp,yp in zip(x_sampled,y_sampled):\n",
    "        rho,p = spearmanr(xp,yp,nan_policy='omit')\n",
    "        rho_array.append(rho)\n",
    "        p_array.append(p)\n",
    "    \n",
    "    print(f'{name}: {np.mean(rho_array):.3f}+-{np.std(rho_array):.3f}')\n",
    "    \n",
    "\n",
    "correlation(x=tmp['age'],x_err=tmp['age_err'],\n",
    "            y=tmp['EW_HA'],y_err=tmp['EW_HA_ERR'],name='EW')\n",
    "correlation(x=tmp['age'],x_err=tmp['age_err'],\n",
    "            y=tmp['EW_HA_CORR'],y_err=tmp['logq_D91_err'],name='EW corr')\n",
    "correlation(x=tmp['age'],x_err=tmp['age_err'],\n",
    "            y=tmp['HA/FUV_corr'],y_err=tmp['HA/FUV_corr_err'],name='Ha/FUV')\n",
    "correlation(x=tmp['age'],x_err=tmp['age_err'],\n",
    "            y=tmp['logq_D91'],y_err=tmp['logq_D91_err'],name='logq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small corner plot grouped by galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2N = 5\n",
    "tmp = nebulae.copy()\n",
    "#tmp['RSII'] = tmp['SII6716_FLUX_CORR'] / tmp['SII6730_FLUX_CORR']\n",
    "#tmp['RSII_ERR'] = tmp['RSII'] * np.sqrt( (tmp['SII6716_FLUX_CORR_ERR']/tmp['SII6716_FLUX_CORR'])**2 + (tmp['SII6730_FLUX_CORR_ERR']/tmp['SII6730_FLUX_CORR'])**2 )\n",
    "tmp['GMC_sep_pc'] = tmp['GMC_sep'] *u.arcsec.to(u.radian) * tmp['distance']*1e6\n",
    "tmp['GMC_sep_pc'][tmp['GMC_sep_pc']>300] = np.nan\n",
    "\n",
    "tmp['HA/FUV_corr'][tmp['HA/FUV_corr']/tmp['HA/FUV_corr_err']<S2N] = np.nan\n",
    "tmp['HA/FUV_corr'][tmp['HA/FUV_corr']<0] = np.nan\n",
    "tmp['logq_D91'][tmp['logq_D91']/tmp['logq_D91_err']<S2N] = np.nan\n",
    "# make sure the 3 galaxies without AstroSat are excluded\n",
    "tmp['HA/FUV_corr'][np.isin(tmp['gal_name'],['NGC1087','NGC4303','NGC5068'])]=np.nan\n",
    "\n",
    "cont = tmp['continuum_Halpha']-tmp['continuum_Halpha_bkg']\n",
    "cont_err = np.sqrt(tmp['continuum_Halpha_error']**2+tmp['continuum_Halpha_bkg_error']**2)\n",
    "\n",
    "# remove objects where we do not detect the continuum\n",
    "#tmp['EW_HA'][(tmp['EW_HA']<15*tmp['EW_HA_ERR'])] = np.nan\n",
    "#tmp['EW_HA_CORR'][(tmp['EW_HA_CORR']<15*tmp['EW_HA_CORR_ERR'])] = np.nan\n",
    "tmp['EW_HA'][cont/cont_err<5] = np.nan\n",
    "tmp['EW_HA_CORR'][cont/cont_err<5] = np.nan\n",
    "\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "print(f'EW:      {np.sum(~np.isnan(tmp[\"EW_HA\"]))} objects')\n",
    "print(f'EW_corr: {np.sum(~np.isnan(tmp[\"EW_HA_CORR\"]))} objects')\n",
    "print(f'HA/FUV:  {np.sum(~np.isnan(tmp[\"HA/FUV_corr\"]))} objects')\n",
    "print(f'logq:    {np.sum(~np.isnan(tmp[\"logq_D91\"]))} objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import spearmanr\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse           \n",
    "        \n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "def plot_function(x,y,group_by,bins,ax,xlim=None,**kwargs):\n",
    "    nbins = 6\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "    \n",
    "    rho = []\n",
    "    for gal_name in np.unique(group_by):\n",
    "        x_sub,y_sub=x[group_by==gal_name],y[group_by==gal_name]\n",
    "        corner_binned_stat(x_sub,y_sub,ax,bins=bins,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        r,p = spearmanr(x_sub,y_sub,nan_policy='omit')\n",
    "        rho.append(r)\n",
    "        if np.mean(y_sub)<1:\n",
    "            print(gal_name)\n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$'\n",
    "    t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "    #corner_spearmanr(x,y,ax,position=(0.4,0.93),pvalue=True,fontsize=7)\n",
    "    \n",
    "\n",
    "columns  = ['EW_HA','HA/FUV_corr','logq_D91']\n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(nrows=2,ncols=2,figsize=(single_column,single_column))\n",
    "ax2.remove()\n",
    "\n",
    "ax1.set(xlim=limits['EW_HA'],xscale='log',ylim=limits['HA/FUV_corr'],yscale='log',ylabel=labels['HA/FUV_corr'])\n",
    "ax1.set_xticklabels([])\n",
    "ax3.set(xlim=limits['EW_HA'],xscale='log',ylim=limits['logq_D91'],xlabel=labels['EW_HA'],ylabel=labels['logq_D91'])\n",
    "ax4.set(xlim=limits['HA/FUV_corr'],xscale='log',ylim=limits['logq_D91'],xlabel=labels['HA/FUV_corr'])\n",
    "ax4.set_yticklabels([])\n",
    "\n",
    "bins = np.logspace(0.7,2.8,7)\n",
    "plot_function(tmp['EW_HA'],tmp['HA/FUV_corr'],group_by=tmp['gal_name'],bins=bins,ax=ax1,xlim=limits['EW_HA'])\n",
    "plot_function(tmp['EW_HA'],tmp['logq_D91'],group_by=tmp['gal_name'],bins=bins,ax=ax3,xlim=limits['EW_HA'])\n",
    "bins = np.logspace(-0.2,1.8,7)\n",
    "plot_function(tmp['HA/FUV_corr'],tmp['logq_D91'],group_by=tmp['gal_name'],bins=bins,ax=ax4,xlim=limits['HA/FUV_corr'])\n",
    "\n",
    "ax1.set_yticks([1,10])\n",
    "ax1.set_yticklabels(['1','10'])\n",
    "ax3.set_xticks([10,100])\n",
    "ax3.set_xticklabels(['10','100'])\n",
    "ax4.set_xticks([1,10])\n",
    "ax4.set_xticklabels(['1','10'])\n",
    "\n",
    "ymin,ymax=limits['logq_D91']\n",
    "ax3_right = ax3.twinx()\n",
    "ax3_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "ax3_right.set_yticklabels([])\n",
    "ax4_right = ax4.twinx()\n",
    "ax4_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "ax4_right.set_ylabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax4_right.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "\n",
    "# left bottom width height\n",
    "cbar_ax = fig.add_axes(rect=[0.57,0.65,0.3,0.02])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='horizontal',ticklocation='top',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.8))\n",
    "#plt.savefig(basedir/'reports'/'nebulae_age_tracers_corner.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same plot but with corrected EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from scipy.stats import spearmanr\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse                         \n",
    "        \n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "def plot_function(x,y,group_by,bins,ax,xlim=None,**kwargs):\n",
    "    nbins = 6\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "    \n",
    "    rho = []\n",
    "    for gal_name in np.unique(group_by):\n",
    "        x_sub,y_sub=x[group_by==gal_name],y[group_by==gal_name]\n",
    "        corner_binned_stat(x_sub,y_sub,ax,bins=bins,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        r,p = spearmanr(x_sub,y_sub,nan_policy='omit')\n",
    "        rho.append(r)\n",
    "        if np.mean(y_sub)<1:\n",
    "            pass\n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$'\n",
    "    t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "    #corner_spearmanr(x,y,ax,position=(0.4,0.93),pvalue=True,fontsize=7)\n",
    "    \n",
    "\n",
    "columns  = ['EW_HA_CORR','HA/FUV_corr','logq_D91']\n",
    "\n",
    "fig, ((ax1,ax2,ax3),(ax4,ax5,ax6),(ax7,ax8,ax9)) = plt.subplots(nrows=3,ncols=3,figsize=(1.1*single_column,1.1*single_column),\n",
    "                                                                sharex='col',sharey='row')\n",
    "ax2.remove()\n",
    "ax3.remove()\n",
    "ax6.remove()\n",
    "\n",
    "ax1.set(xlim=limits['EW_HA'],xscale='log',ylim=limits['EW_HA_CORR'],yscale='log',ylabel=labels['EW_HA_CORR'])\n",
    "ax1.set_xticklabels([])\n",
    "ax4.set(xlim=limits['EW_HA'],xscale='log',ylim=limits['HA/FUV_corr'],yscale='log',ylabel=labels['HA/FUV_corr'])\n",
    "ax4.set_xticklabels([])\n",
    "ax7.set(xlim=limits['EW_HA'],xscale='log',ylim=limits['logq_D91'],xlabel=labels['EW_HA'],ylabel=labels['logq_D91'])\n",
    "\n",
    "ax5.set(xlim=limits['EW_HA_CORR'],xscale='log',ylim=limits['HA/FUV_corr'])\n",
    "ax8.set(xlim=limits['EW_HA_CORR'],xscale='log',ylim=limits['logq_D91'],xlabel=labels['EW_HA_CORR'])\n",
    "ax9.set(xlim=limits['HA/FUV_corr'],xscale='log',ylim=limits['logq_D91'],xlabel=labels['HA/FUV_corr'])\n",
    "\n",
    "\n",
    "bins = np.logspace(0.7,2.8,7)\n",
    "plot_function(tmp['EW_HA'],tmp['EW_HA_CORR'],group_by=tmp['gal_name'],bins=bins,ax=ax1,xlim=limits['EW_HA'])\n",
    "plot_function(tmp['EW_HA'],tmp['HA/FUV_corr'],group_by=tmp['gal_name'],bins=bins,ax=ax4,xlim=limits['EW_HA'])\n",
    "plot_function(tmp['EW_HA'],tmp['logq_D91'],group_by=tmp['gal_name'],bins=bins,ax=ax7,xlim=limits['EW_HA'])\n",
    "\n",
    "bins = np.logspace(1.7,3.7,7)\n",
    "plot_function(tmp['EW_HA_CORR'],tmp['HA/FUV_corr'],group_by=tmp['gal_name'],bins=bins,ax=ax5,xlim=limits['EW_HA_CORR'])\n",
    "plot_function(tmp['EW_HA_CORR'],tmp['logq_D91'],group_by=tmp['gal_name'],bins=bins,ax=ax8,xlim=limits['EW_HA_CORR'])\n",
    "\n",
    "bins = np.logspace(0.,1.8,7)\n",
    "plot_function(tmp['HA/FUV_corr'],tmp['logq_D91'],group_by=tmp['gal_name'],bins=bins,ax=ax9,xlim=limits['HA/FUV_corr'])\n",
    "\n",
    "ax4.set_yticks([1,10])\n",
    "ax4.set_yticklabels(['1','10'])\n",
    "ax9.set_xticks([1,10])\n",
    "ax9.set_xticklabels(['1','10'])\n",
    "ax7.set_xticks([10,100])\n",
    "ax7.set_xticklabels(['10','100'])\n",
    "\n",
    "\n",
    "ymin,ymax=limits['logq_D91']\n",
    "ax7_right = ax7.twinx()\n",
    "ax7_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "ax7_right.set_yticklabels([])\n",
    "ax8_right = ax8.twinx()\n",
    "ax8_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "ax8_right.set_yticklabels([])\n",
    "ax9_right = ax9.twinx()\n",
    "ax9_right.set(ylim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],yscale='log')\n",
    "ax9_right.set_ylabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "ax9_right.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.08, hspace=0.08)\n",
    "\n",
    "# left bottom width height\n",
    "cbar_ax = fig.add_axes(rect=[0.5,0.72,0.35,0.02])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='horizontal',ticklocation='top',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.8))\n",
    "plt.savefig(basedir/'reports'/'nebulae_age_tracers_corner.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### physical trends with proposed age tracers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile\n",
    "from scipy.stats import spearmanr,binned_statistic\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "def plot_function(x,y,group_by,ax,bins,**kwargs):\n",
    "    nbins = 6\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "    \n",
    "    rho = []\n",
    "    for gal_name in np.unique(group_by):\n",
    "        x_sub,y_sub=x[group_by==gal_name],y[group_by==gal_name]\n",
    "        corner_binned_stat(x_sub,y_sub,ax,bins=bins,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        r,p = spearmanr(x_sub,y_sub,nan_policy='omit')\n",
    "        #print(f'{gal_name}: {r:.2f} ({p:.3g})')\n",
    "        rho.append(r)\n",
    "        \n",
    "        #mean, edges, _ = binned_statistic(x_sub,y_sub,statistic='median',bins=bins)\n",
    "        #std, edges, _ = binned_statistic(x_sub,y_sub,statistic='std',bins=bins)\n",
    "        #x_mid = (edges[1:]+edges[:-1])/2\n",
    "        #ax.errorbar(x_mid,mean,yerr=None,fmt='o-',ms=2,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        #ax.fill_between(x_mid,y1=mean-std,y2=mean+std,alpha=0.4,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        \n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$'\n",
    "    t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "\n",
    "\n",
    "columns = ['Delta_met_scal','HA6562_LUM_CORR','density','EBV_balmer','GMC_sep_pc']\n",
    "\n",
    "fig,axes=plt.subplots(ncols=3,nrows=len(columns),figsize=(two_column,1.1*two_column/3/1.618*len(columns)),\n",
    "                      sharex='col',sharey='row')\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    ax1,ax2,ax3 = axes[i]\n",
    "    \n",
    "    bins = np.logspace(0.7,2.8,7)\n",
    "    plot_function(tmp['EW_HA'],tmp[col],tmp['gal_name'],ax1,bins=bins)\n",
    "    bins = np.logspace(-0.2,1.8,7)\n",
    "    plot_function(tmp['HA/FUV_corr'],tmp[col],tmp['gal_name'],ax2,bins=bins)\n",
    "    bins = np.linspace(5.6,8.1,7)\n",
    "    plot_function(tmp['logq_D91'],tmp[col],tmp['gal_name'],ax3,bins=bins)\n",
    "\n",
    "    ax1.set(ylabel=labels[col],ylim=limits[col])\n",
    "    if col == 'HA6562_LUM_CORR':\n",
    "        for ax in axes[i]:\n",
    "            ax.set(yscale='log',ylim=limits[col])\n",
    "    \n",
    "    ymin,ymax=limits['logq_D91']\n",
    "    ax3_top = ax3.twiny()\n",
    "    ax3_top.set(xlim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],xscale='log')\n",
    "    ax3_top.set_xticklabels([])\n",
    "    if i==0:\n",
    "        ax3_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "        #ax3_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(x)))\n",
    "        ax3_top.set_xticks([0.1,1])\n",
    "        ax3_top.set_xticklabels(['0.1','1'])\n",
    "\n",
    "ax1.set(xlabel=labels['EW_HA'],xscale='log',xlim=limits['EW_HA'])    \n",
    "ax2.set_xticks([10,100])\n",
    "ax2.set_xticklabels(['10','100'])\n",
    "ax2.set(xlabel=labels['HA/FUV_corr'],xscale='log',xlim=limits['HA/FUV_corr'])   \n",
    "ax2.set_xticks([1,10])\n",
    "ax2.set_xticklabels(['1','10'])\n",
    "ax3.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.07, hspace=0.1,top=0.88)\n",
    "cbar_ax = fig.add_axes([0.18, 0.9, 0.4, 0.02])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='horizontal',ticklocation='top',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "# add arrow to indicate age \n",
    "for ax in axes[0]:\n",
    "    ax.arrow(0.9,0.15,-0.2,0,transform=ax.transAxes,\n",
    "              head_width=0.08,head_length=0.04,color='black')\n",
    "    ax.text(0.8,0.18,'age',transform=ax.transAxes,ha='center',va='bottom',fontsize=7)\n",
    "\n",
    "plt.savefig(basedir/'reports'/'trends_with_age_tracers.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with corrected EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile\n",
    "from scipy.stats import spearmanr,binned_statistic\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "def plot_function(x,y,group_by,ax,bins,**kwargs):\n",
    "    nbins = 6\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=bins,n=98,color='gray',alpha=0.2)\n",
    "    \n",
    "    rho = []\n",
    "    for gal_name in np.unique(group_by):\n",
    "        x_sub,y_sub=x[group_by==gal_name],y[group_by==gal_name]\n",
    "        corner_binned_stat(x_sub,y_sub,ax,bins=bins,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        r,p = spearmanr(x_sub,y_sub,nan_policy='omit')\n",
    "        #print(f'{gal_name}: {r:.2f} ({p:.3g})')\n",
    "        rho.append(r)\n",
    "        \n",
    "        #mean, edges, _ = binned_statistic(x_sub,y_sub,statistic='median',bins=bins)\n",
    "        #std, edges, _ = binned_statistic(x_sub,y_sub,statistic='std',bins=bins)\n",
    "        #x_mid = (edges[1:]+edges[:-1])/2\n",
    "        #ax.errorbar(x_mid,mean,yerr=None,fmt='o-',ms=2,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        #ax.fill_between(x_mid,y1=mean-std,y2=mean+std,alpha=0.4,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "        \n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$ {p:.3g}'\n",
    "    t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "\n",
    "columns = ['Delta_met_scal','HA6562_LUM_CORR','density','EBV_balmer','GMC_sep_pc']\n",
    "\n",
    "fig,axes=plt.subplots(ncols=4,nrows=len(columns),figsize=(two_column,two_column/4*len(columns)),\n",
    "                      sharex='col',sharey='row')\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    ax1,ax2,ax3,ax4 = axes[i]\n",
    "    \n",
    "    bins = np.logspace(0.7,2.8,7)\n",
    "    plot_function(tmp['EW_HA'],tmp[col],tmp['gal_name'],ax1,bins=bins)\n",
    "    bins = np.logspace(1.7,3.7,7)\n",
    "    plot_function(tmp['EW_HA_CORR'],tmp[col],tmp['gal_name'],ax2,bins=bins)\n",
    "    bins = np.logspace(0.,1.8,7)\n",
    "    plot_function(tmp['HA/FUV_corr'],tmp[col],tmp['gal_name'],ax3,bins=bins)\n",
    "    bins = np.linspace(5.6,8.1,7)\n",
    "    plot_function(tmp['logq_D91'],tmp[col],tmp['gal_name'],ax4,bins=bins)\n",
    "\n",
    "    ax1.set(ylabel=labels[col],ylim=limits[col])\n",
    "    if col == 'HA6562_LUM_CORR':\n",
    "        for ax in axes[i]:\n",
    "            ax.set(yscale='log',ylim=limits[col])\n",
    "    \n",
    "    ymin,ymax=limits['logq_D91']\n",
    "    ax4_top = ax4.twiny()\n",
    "    ax4_top.set(xlim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],xscale='log')\n",
    "    ax4_top.set_xticklabels([])\n",
    "    if i==0:\n",
    "        ax4_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "        ax4_top.set_xticks([0.1,1])\n",
    "\n",
    "ax1.set(xlabel=labels['EW_HA'],xscale='log',xlim=limits['EW_HA'])    \n",
    "#ax2.set_xticks([100,100,1000])\n",
    "ax2.set(xlabel=labels['EW_HA_CORR'],xscale='log',xlim=limits['EW_HA_CORR'])    \n",
    "ax2.set_xticks([100,100,1000])\n",
    "ax2.set_xticklabels(['10','100','1000'])\n",
    "ax3.set(xlabel=labels['HA/FUV_corr'],xscale='log',xlim=limits['HA/FUV_corr'])   \n",
    "ax3.set_xticks([1,10])\n",
    "ax3.set_xticklabels(['1','10'])\n",
    "ax4.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])    \n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.07,top=0.88)\n",
    "cbar_ax = fig.add_axes([0.2, 0.89, 0.4, 0.02])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='horizontal',ticklocation='top',\n",
    "             label=r'$\\log (M_\\star\\,/\\,\\mathrm{M}_\\odot)$',ticks=np.arange(9.4,11.4,0.4))\n",
    "\n",
    "# add arrow to indicate age \n",
    "for ax in axes[0]:\n",
    "    ax.arrow(0.9,0.15,-0.2,0,transform=ax.transAxes,\n",
    "              head_width=0.08,head_length=0.04,color='black')\n",
    "    ax.text(0.8,0.18,'age',transform=ax.transAxes,ha='center',va='bottom',fontsize=7)\n",
    "\n",
    "plt.savefig(basedir/'reports'/'trends_with_age_tracers.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_binned_percentile\n",
    "from scipy.stats import spearmanr,binned_statistic\n",
    "from astrotools.metallicity.ionization import logq_D91, logq_D91_reverse\n",
    "\n",
    "cmap = mpl.cm.get_cmap('spring')\n",
    "norm = mpl.colors.Normalize(vmin=35,vmax=40)\n",
    "\n",
    "def plot_function(x,y,group_by,ax,bins,**kwargs):\n",
    "    nbins = 6\n",
    "    finite = np.isfinite(x) & np.isfinite(y)\n",
    "    x,y,group_by = x[finite], y[finite],group_by[finite]\n",
    "    \n",
    "    corner_scatter(x,y,ax=ax,s=0.4,alpha=0.5,cmap=cmap,c=group_by,vmin=35,vmax=40)\n",
    "    \n",
    "    r,p = spearmanr(x,y,nan_policy='omit')\n",
    "    label = r'$\\rho'+f'={r:.2f}$'\n",
    "    t = ax.text(0.08,0.9,label,transform=ax.transAxes,ha='left',va='top',fontsize=7)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "    corner_binned_stat(x,y,ax,bins=bins,color='black')\n",
    "\n",
    "\n",
    "columns = ['Delta_met_scal','HA6562_LUM_CORR','density','EBV_balmer','GMC_sep_pc']\n",
    "\n",
    "fig,axes=plt.subplots(ncols=4,nrows=len(columns),figsize=(two_column,1.1*two_column/3/1.618*len(columns)),\n",
    "                      sharex='col',sharey='row')\n",
    "\n",
    "color = 'HA6562_LUM_CORR'\n",
    "for i,col in enumerate(columns):\n",
    "    ax1,ax2,ax3,ax4 = axes[i]\n",
    "    \n",
    "    bins = np.logspace(0.7,2.8,7)\n",
    "    plot_function(tmp['EW_HA'],tmp[col],np.log10(tmp[color]),ax1,bins=bins)\n",
    "    bins = np.logspace(1.7,3.7,7)\n",
    "    plot_function(tmp['EW_HA_CORR'],tmp[col],np.log10(tmp[color]),ax2,bins=bins)\n",
    "    bins = np.logspace(0.,1.8,7)\n",
    "    plot_function(tmp['HA/FUV_corr'],tmp[col],np.log10(tmp[color]),ax3,bins=bins)\n",
    "    bins = np.linspace(5.6,8.1,7)\n",
    "    plot_function(tmp['logq_D91'],tmp[col],np.log10(tmp[color]),ax4,bins=bins)\n",
    "\n",
    "    ax1.set(ylabel=labels[col],ylim=limits[col])\n",
    "    if col == 'HA6562_LUM_CORR':\n",
    "        for ax in axes[i]:\n",
    "            ax.set(yscale='log',ylim=limits[col])\n",
    "    \n",
    "    ymin,ymax=limits['logq_D91']\n",
    "    ax4_top = ax4.twiny()\n",
    "    ax4_top.set(xlim=[logq_D91_reverse(ymin),logq_D91_reverse(ymax)],xscale='log')\n",
    "    ax4_top.set_xticklabels([])\n",
    "    if i==0:\n",
    "        ax4_top.set_xlabel(r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$')\n",
    "        #ax3_top.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(x)))\n",
    "        ax4_top.set_xticks([0.1,1])\n",
    "        ax4_top.set_xticklabels(['0.1','1'])\n",
    "\n",
    "ax1.set(xlabel=labels['EW_HA'],xscale='log',xlim=limits['EW_HA'])    \n",
    "ax2.set(xlabel=labels['EW_HA_CORR'],xscale='log',xlim=limits['EW_HA_CORR'])    \n",
    "ax3.set_xticks([10,100])\n",
    "ax3.set_xticklabels(['10','100'])\n",
    "ax3.set(xlabel=labels['HA/FUV_corr'],xscale='log',xlim=limits['HA/FUV_corr'])   \n",
    "ax3.set_xticks([1,10])\n",
    "ax3.set_xticklabels(['1','10'])\n",
    "ax4.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.07, hspace=0.1,top=0.88)\n",
    "cbar_ax = fig.add_axes([0.18, 0.9, 0.4, 0.02])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),cax=cbar_ax,orientation='horizontal',ticklocation='top',\n",
    "             label=r'Halpha Luminosity')\n",
    "\n",
    "# add arrow to indicate age \n",
    "for ax in axes[0]:\n",
    "    ax.arrow(0.9,0.15,-0.2,0,transform=ax.transAxes,\n",
    "              head_width=0.08,head_length=0.04,color='black')\n",
    "    ax.text(0.8,0.18,'age',transform=ax.transAxes,ha='center',va='bottom',fontsize=7)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'trends_with_age_tracers.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,bins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "\n",
    "tmp = nebulae.copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "tmp['log_HA'] = np.log10(tmp['HA6562_LUM_CORR'])\n",
    "\n",
    "columns  = ['EW_HA','HA/FUV_corr','log_HA','Delta_met_scal','T_N2_REFIT','density','EBV_balmer','GMC_sep']\n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=basedir/'reports'/f'corner_large.pdf',\n",
    "           figsize=2*two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,bins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=True,fontsize=7)\n",
    "    \n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['HA/FUV']>3*catalogue['HA/FUV_err']) | np.isnan(catalogue['FUV_FLUX'])\n",
    "#criteria &= catalogue['[SIII]/[SII]']>3*catalogue['[SIII]/[SII]_err']\n",
    "criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "# young objects should be associated with a GMC\n",
    "#criteria &= ((catalogue['GMC_sep']<4) | (catalogue['age']>2))\n",
    "#criteria &= catalogue['U_dolmag_vega']-catalogue['B_dolmag_vega']<-1.\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria  &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "tmp['HA/FUV_corr'][tmp['FUV_FLUX_CORR']/tmp['FUV_FLUX_CORR_ERR']<3] = np.nan\n",
    "tmp['EW_HA'][tmp['EW_HA']/tmp['EW_HA_ERR']<3] = np.nan\n",
    "tmp['logq_D91'][tmp['logq_D91']/tmp['logq_D91_err']<3] = np.nan\n",
    "tmp = tmp.filled()\n",
    "\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "columns  = ['age','eq_width_Ha','HA/FUV_corr','logq_D91']\n",
    "\n",
    "filename = None #basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=filename,histogram=False,\n",
    "           figsize=two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    x,y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,bins=nbins,color='0.3')\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "tmp = nebulae.copy()  \n",
    "tmp['log_HA'] = np.log10(tmp['HA6562_LUM_CORR'])\n",
    "\n",
    "columns = ['Delta_met_scal','log_HA','T_N2_REFIT','density','EBV_balmer','GMC_sep']\n",
    "\n",
    "fig,axes=plt.subplots(ncols=3,nrows=len(columns),figsize=(two_column,two_column/3/1.618*len(columns)),\n",
    "                      sharex='col',sharey='row')\n",
    "\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    ax1,ax2,ax3 = axes[i]\n",
    "    \n",
    "    plot_function(tmp['EW_HA'],tmp[col],ax1,xlim=limits['EW_HA'])\n",
    "    plot_function(tmp['HA/FUV_corr'],tmp[col],ax2,xlim=limits['HA/FUV_corr'])\n",
    "    plot_function(tmp['logq_D91'],tmp[col],ax3,xlim=limits['logq_D91'])\n",
    "    \n",
    "    ax1.set(ylabel=labels[col],ylim=limits[col])\n",
    "    \n",
    "ax1.set(xlabel=labels['EW_HA'],xlim=limits['EW_HA'])    \n",
    "ax2.set(xlabel=labels['HA/FUV_corr'],xlim=limits['HA/FUV_corr'])    \n",
    "ax3.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])    \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "or binned by galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "limits['log_HA'] = (35.,40)\n",
    "limits['GMC_sep'] = (0.,20)\n",
    "limits['T_N2_REFIT'] = (6000,12000)\n",
    "\n",
    "tmp = nebulae.copy()  \n",
    "tmp['log_HA'] = np.log10(tmp['HA6562_LUM_CORR'])\n",
    "\n",
    "columns = ['Delta_met_scal','log_HA','T_N2_REFIT','density','EBV_balmer','GMC_sep']\n",
    "\n",
    "fig,axes=plt.subplots(ncols=3,nrows=len(columns),figsize=(two_column,two_column/3/1.618*len(columns)),\n",
    "                      sharex='col',sharey='row')\n",
    "\n",
    "nbins = 7\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    ax1,ax2,ax3 = axes[i]\n",
    "    \n",
    "    for xcol,ax in zip(['EW_HA','HA/FUV_corr','logq_D91'],[ax1,ax2,ax3]):\n",
    "        \n",
    "        x,y = tmp[xcol],tmp[col]\n",
    "        x,y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "        corner_binned_percentile(x,y,ax,nbins=nbins,range=limits[xcol],n=68,color='gray',alpha=0.6)\n",
    "        corner_binned_percentile(x,y,ax,nbins=nbins,range=limits[xcol],n=98,color='gray',alpha=0.2)\n",
    "        corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "        \n",
    "        for gal_name in np.unique(tmp['gal_name']):\n",
    "            x,y = tmp[tmp['gal_name']==gal_name][xcol],tmp[tmp['gal_name']==gal_name][col]\n",
    "            x,y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "            corner_binned_stat(x,y,ax,bins=nbins,color=cmap(norm(sample_table.loc[gal_name]['mass'])))\n",
    "\n",
    "        corner_binned_stat(x,y,ax,bins=nbins,color='0.3')\n",
    "        \n",
    "    \n",
    "    ax1.set(ylabel=labels[col],ylim=limits[col])\n",
    "    \n",
    "ax1.set(xlabel=labels['EW_HA'],xlim=limits['EW_HA'])    \n",
    "ax2.set(xlabel=labels['HA/FUV_corr'],xlim=limits['HA/FUV_corr'])    \n",
    "ax3.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])    \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "histograms binned by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>=1e4) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= ((catalogue['GMC_sep']<2) | (catalogue['age']>2))\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4,figsize=(1.5*two_column,1.5*two_column/3))\n",
    "\n",
    "style = {'alpha':0.5}\n",
    "nbins = 10\n",
    "age_bins = [0,2,8]\n",
    "for i in range(len(age_bins)-1):\n",
    "    sub = tmp[(tmp['age']>age_bins[i]) & (tmp['age']<=age_bins[i+1])]\n",
    "    label = f'{age_bins[i]}\\> age \\>= {age_bins[i+1]} Myr: '\n",
    "    \n",
    "    ax1.hist(sub['eq_width'],bins=np.linspace(*limits['eq_width'],nbins),**style,\n",
    "             label=label+f\"{np.nanmean(sub['eq_width']):.1f}\")\n",
    "    ax2.hist(sub['HA/FUV_corr'],bins=np.linspace(*limits['HA/FUV_corr'],nbins),**style,\n",
    "            label=label+f\"{np.nanmedian(sub['HA/FUV_corr']):.1f}\")\n",
    "    ax3.hist(sub['Delta_met_scal'],bins=np.linspace(*limits['Delta_met_scal'],nbins),**style,\n",
    "            label=label+f\"{np.nanmean(sub['Delta_met_scal']):.2f}\")\n",
    "    ax4.hist(sub['logq_D91'],bins=np.linspace(*limits['logq_D91'],nbins),**style,\n",
    "             label=label+f\"{np.nanmean(sub['logq_D91']):.1f}\")\n",
    "    \n",
    "ax1.set(xlabel=labels['eq_width'],xlim=limits['eq_width'])\n",
    "ax2.set(xlabel=labels['HA/FUV_corr'],xlim=limits['HA/FUV_corr'])\n",
    "ax3.set(xlabel=labels['Delta_met_scal'],xlim=limits['Delta_met_scal'])\n",
    "ax4.set(xlabel=labels['logq_D91'],xlim=limits['logq_D91'])\n",
    "ax1.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax2.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax3.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "ax4.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower left\", mode=\"expand\", ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Quantify changes in numbers\n",
    "\n",
    "we compare the change of the EW(Ha) and Ha/FUV to the predicted numbers from Starburst99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "criteria  &= (catalogue['overlap_asc']==1) \n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "EW_HA_lst, edges, _ = binned_statistic(list(tmp['age']),list(tmp['EW_HA']),statistic='median',bins=8,range=[0.5,8.5])\n",
    "HaFUV_lst, edges, _ = binned_statistic(list(tmp['age']),list(tmp['HA/FUV_corr']),statistic='median',bins=8,range=[0.5,8.5])\n",
    "xp = (edges[1:]+edges[:-1])/2\n",
    "\n",
    "\n",
    "print('    age  EW(Ha)  Ha/FUV')\n",
    "for age,EW_HA,HaFUV in zip(xp,EW_HA_lst,HaFUV_lst):\n",
    "    print(f'{age} Myr {EW_HA/EW_HA_lst[0]:>5.2f} % {HaFUV/HaFUV_lst[0]:>5.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log q vs age vs pressure\n",
    "\n",
    "see also Dopita+2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e3) \n",
    "#criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "#criteria  &= (catalogue['overlap_asc']==1) \n",
    "#criteria  &= (catalogue['logq_D91']>3*catalogue['logq_D91_err']) \n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',6)\n",
    "norm = mpl.colors.Normalize(vmin=6.5,vmax=9.5)\n",
    "rho = []\n",
    "n_bins = np.arange(6.5,10,0.5)\n",
    "for i in range(len(n_bins)-1):\n",
    "    sub = tmp[(tmp['LOG_SIGMA_STMASS_CHAB']>=n_bins[i]) & (tmp['LOG_SIGMA_STMASS_CHAB']<n_bins[i+1])]\n",
    "    x,mean,std = bin_stat(sub['age'],sub['logq_D91'],bins=np.linspace(0.5,10.5,5),statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(n_bins[i]+0.25)))\n",
    "    \n",
    "ax.set(xlim=[0,10],ylim=[6.1,7.5],xlabel='age / Myr',ylabel=r'$\\log q$ (Diaz+91)')\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),label=r'$\\Sigma_M$ / M$_\\odot$ kpc$^{-2}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by metallicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.utils import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e3) \n",
    "#criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "#criteria  &= (catalogue['overlap_asc']==1) \n",
    "#criteria  &= (catalogue['logq_D91']>3*catalogue['logq_D91_err']) \n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',3)\n",
    "norm = mpl.colors.Normalize(vmin=8.2,vmax=8.8)\n",
    "rho = []\n",
    "n_bins = [8.2,8.4,8.6,8.8]\n",
    "for i in range(len(n_bins)-1):\n",
    "    sub = tmp[(tmp['met_scal']>=n_bins[i]) & (tmp['met_scal']<n_bins[i+1])]\n",
    "    x,mean,std = bin_stat(sub['age'],sub['logq_D91'],bins=np.linspace(0.5,10.5,5),statistic='median')\n",
    "    ax.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(n_bins[i]+0.1)))\n",
    "    \n",
    "sc = ax.scatter(tmp['age'],tmp['logq_D91'],c=tmp['met_scal'],cmap=cmap,vmin=8.2,vmax=8.8)\n",
    "ax.set(xlim=[0,10],ylim=[6.1,7.5],xlabel='age / Myr',ylabel=r'$\\log q$ (Diaz+91)')\n",
    "fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),label=r'$12+\\log (\\mathrm{O}/\\mathrm{H})$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',6)\n",
    "norm = mpl.colors.Normalize(vmin=6.5,vmax=9.5)\n",
    "n_bins = np.arange(6.5,10,1)\n",
    "for i in range(len(n_bins)-1):\n",
    "    sub = tmp[(tmp['LOG_SIGMA_STMASS_CHAB']>=n_bins[i]) & (tmp['LOG_SIGMA_STMASS_CHAB']<n_bins[i+1])]\n",
    "    ax.hist(sub['logq_D91'],color=cmap(norm(n_bins[i]+0.25)),alpha=0.6,label=n_bins[i]+0.25)\n",
    "    \n",
    "sc = ax.scatter([-1],[-1],c=[8],cmap=cmap,vmin=6.5,vmax=9.5)\n",
    "ax.set(xlim=[6.1,7.5],xlabel=r'$\\log q$ (Diaz+91)')\n",
    "#fig.colorbar(sc,label=r'$\\Sigma_M$ / M$_\\odot$ kpc$^{-2}$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with starburst99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_CSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'continuous_SF')\n",
    "cluster_SSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'single_burst_SF')\n",
    "cluster_SSF.measure_FUV()\n",
    "cluster_CSF.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Halpha/FUV\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e4) & (catalogue['overlap_asc']==1)]\n",
    "ax1.scatter(tmp['age'],tmp['HA/FUV_corr'],color='gray')\n",
    "\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_CSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = np.interp(cluster_CSF.ewidth['Time'],cluster_CSF.FUV['Time'],cluster_CSF.FUV['FUV'])\n",
    "ax1.plot(time,Ha/FUV,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_SSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster_SSF.FUV['FUV'].copy()\n",
    "ax1.plot(time,Ha/FUV,label='single burst')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],ylim=[0,100],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "\n",
    "ax2.scatter(tmp['age'],tmp['eq_width'],color='gray')\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_CSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,0.1*eq_width,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_SSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,0.1*eq_width,label='singel burst')\n",
    "ax2.legend()\n",
    "ax2.set(xlim=[0,10],ylim=[0,400],yscale='linear',xlabel='age / Myr',ylabel=r'EW(H$\\alpha$)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the distribution of EW observed vs model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_solar = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster_subsolar = Cluster(stellar_model='GENEVAv40',metallicity=0.004)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = catalogue.copy()\n",
    "\n",
    "eq_width_model_solar = cluster_solar.ewidth['eq_width_H_A']\n",
    "eq_width_model_subsolar = cluster_subsolar.ewidth['eq_width_H_A']\n",
    "age_model = cluster_solar.ewidth['Time']\n",
    "\n",
    "tmp['eq_width_model_solar'] = np.nan\n",
    "tmp['eq_width_model_subsolar'] = np.nan\n",
    "for row in tmp:\n",
    "    idx = np.argmin(np.abs(age_model-row['age']*u.Myr))\n",
    "    row['eq_width_model_solar'] = eq_width_model_solar[idx].value\n",
    "    row['eq_width_model_subsolar'] = eq_width_model_subsolar[idx].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(0,4,15)\n",
    "\n",
    "ax.hist(tmp['eq_width'],bins=bins,label='observed',alpha=0.6)\n",
    "ax.hist(tmp['eq_width_model_solar'],bins=bins,label='model solar',alpha=0.6)\n",
    "#ax.hist(tmp['eq_width_model_subsolar'],bins=bins,label='model subsolar',alpha=0.6)\n",
    "ax.legend()\n",
    "ax.set(xscale='log',xlim=[1,1e4],xlabel=r'$\\mathrm{EW}(\\mathrm{H}\\alpha)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other things like violine plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "from astrotools.plot.corner import corner_scatter, corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_density_histogram, corner_gaussian_kde_scatter,\\\n",
    "                                corner_binned_stat2d, corner_binned_stat2d_histogram,\\\n",
    "                                corner_binned_percentile,corner_violin, corner_spearmanr\n",
    "                             \n",
    "def plot_function(x,y,ax,xlim=None,**kwargs):\n",
    "    nbins = 7\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=xlim,n=98,color='gray',alpha=0.2)\n",
    "    corner_binned_stat(x,y,ax,nbins=nbins,color='0.3')\n",
    "    #corner_density_histogram(x,y,ax,nbins=10,cmap=plt.cm.gray_r)\n",
    "    #corner_scatter(x,y,ax,s=0.5)\n",
    "    #corner_gaussian_kde_scatter(x,y,ax,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    \n",
    "filename = basedir/'reports'/f'corner_{HSTband}_{scalepc}pc.pdf'\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "#criteria &= (catalogue['HA/FUV']>3*catalogue['HA/FUV_err']) | np.isnan(catalogue['FUV_FLUX'])\n",
    "#criteria &= catalogue['[SIII]/[SII]']>3*catalogue['[SIII]/[SII]_err']\n",
    "criteria &= (catalogue['age']<=8) #& (catalogue['age']>2)\n",
    "# young objects should be associated with a GMC\n",
    "#criteria &= ((catalogue['GMC_sep']<4) | (catalogue['age']>2))\n",
    "#criteria &= catalogue['U_dolmag_vega']-catalogue['B_dolmag_vega']<-1.\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['log_age'] = 6+np.log10(tmp['age'])\n",
    "    tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "    tmp['log[SIII]/[SII]'] = np.log10(tmp['[SIII]/[SII]'])\n",
    "\n",
    "columns  = ['logq_D91','Delta_met_scal','HA/FUV_corr','log_eq_width']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=plot_function,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=None,\n",
    "           figsize=two_column,aspect_ratio=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "\n",
    "x = tmp['r_R25']\n",
    "y = tmp['log_eq_width']\n",
    "\n",
    "ax.scatter(x,y,c='gray')\n",
    "corner_binned_stat(x,y,ax)\n",
    "corner_spearmanr(x,y,ax,pvalue=True)\n",
    "\n",
    "ax.set(xlim=[0,1],ylim=[0,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use a violin plot to show the distribution of points in each age bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "x_label = 'age'\n",
    "y_label = 'eq_width'\n",
    "\n",
    "x,y = tmp[x_label], tmp[y_label]\n",
    "x,y = x[~np.isnan(y)],y[~np.isnan(y)]\n",
    "x,y = x[(x>=limits[x_label][0]) & (x<=limits[x_label][1]) & (y>=limits[y_label][0]) & (y<=limits[y_label][1])], \\\n",
    "      y[(x>=limits[x_label][0]) & (x<=limits[x_label][1]) & (y>=limits[y_label][0]) & (y<=limits[y_label][1])]\n",
    "\n",
    "positions = np.arange(0,10)\n",
    "\n",
    "#binned_data = corner_violin(x,y,ax,positions,showmedians=True)\n",
    "\n",
    "bins = (positions[1:]+positions[:-1])/2\n",
    "binned_data = [y[(bins[i]<x) & (x<bins[i+1])] for i in range(len(bins)-1)]\n",
    "ax.violinplot(binned_data,positions=positions[1:-1],showmeans=True)\n",
    "\n",
    "ax.set(xlabel=x_label,ylabel=y_label.replace('_',''))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter age with color-color-diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[-0.33 , -1.662],[-0.307, -1.577],[-0.25 , -1.468],[-0.218, -1.394],[-0.186, -1.321],\n",
    "                   [-0.138, -1.235],[-0.076, -1.213],[-0.007, -1.206],[ 0.065, -1.197],[ 0.135, -1.187],\n",
    "                   [ 0.203, -1.198],[ 0.276, -1.201],[ 0.347, -1.209],[ 0.418, -1.219],[ 0.488, -1.233],\n",
    "                   [ 0.559, -1.251],[ 0.63 , -1.27 ],[ 0.706, -1.264],[ 0.771, -1.287],[ 0.648, -1.119],\n",
    "                   [ 0.626, -1.026],[ 0.565, -0.989],[ 0.559, -0.87 ],[ 0.502, -0.807],[ 0.481, -0.706],\n",
    "                   [ 0.457, -0.546],[ 0.505, -0.449],[ 0.487, -0.384],[ 0.488, -0.328],[ 0.468, -0.239],\n",
    "                   [ 0.517, -0.109],[ 0.555, -0.015],[ 0.6  ,  0.041],[ 0.65 ,  0.132],[ 0.724,  0.174],\n",
    "                   [ 0.864,  0.169],[ 0.948,  0.166],[ 1.02 ,  0.152],[ 1.138,  0.182],[ 1.203,  0.194],\n",
    "                   [ 1.253,  0.282],[ 1.331,  0.378],[ 1.354,  0.469]])\n",
    "\n",
    "x,y=points.T\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(2.3*single_column,1*single_column))\n",
    "\n",
    "tmp = associations\n",
    "#ax.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],s=0.5)\n",
    "tmp = catalogue\n",
    "sc=ax1.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['GMC_sep'],vmin=0,vmax=10,\n",
    "           s=0.5)\n",
    "ax1.plot(x,y,color='red')\n",
    "ax1.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "ax1.invert_yaxis()\n",
    "plt.colorbar(sc,label='GMC sep / arcsec',ax=ax1)\n",
    "\n",
    "sc=ax2.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['age'],vmin=0,vmax=20,cmap=plt.cm.plasma,\n",
    "           s=0.5)\n",
    "ax2.plot(x,y,color='red')\n",
    "ax2.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "ax2.invert_yaxis()\n",
    "fig.colorbar(sc,label='age / Myr',ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('UBVI_matched_associations.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "bins=np.arange(0,150,10)\n",
    "tmp = catalogue[(catalogue['age']==1) & (catalogue['overlap_asc']==1)]\n",
    "mask = tmp['GMC_sep']<4\n",
    "\n",
    "ax.hist(tmp['eq_width'][mask],bins=bins,label=f'nearby GMC: {np.mean(tmp[\"eq_width\"][mask]):.2f} AA',histtype='step',alpha=0.8)\n",
    "ax.hist(tmp['eq_width'][~mask],bins=bins,label=f'no GMC {np.mean(tmp[\"eq_width\"][~mask]):.2f} AA',histtype='step',alpha=0.8)\n",
    "ax.set(xlabel=r'EW(H$\\alpha$)')\n",
    "ax.set_title('young associations (age 1 Myr)')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "\n",
    "# define limits and labels for corner plots\n",
    "limits   = {'age':(0,20),\n",
    "            'UB':(-2.0,0),\n",
    "            'CO':(0,200),\n",
    "            'GMC_sep':(0,10)}\n",
    "\n",
    "labels   = {'age':'age / Myr',\n",
    "            'UB':r'$U-B$',\n",
    "            'CO':r'CO flux',\n",
    "            'GMC_sep':r'GMC sep / arcsec'}\n",
    "\n",
    "                             \n",
    "tmp = catalogue.copy()\n",
    "#tmp = tmp[tmp['GMC_sep']<5]\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['UB'] = tmp['U_dolmag_vega']-tmp['B_dolmag_vega']\n",
    "\n",
    "columns  = ['age','GMC_sep','CO','UB']\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(tmp,columns,function=None,\n",
    "           limits=limits,labels=labels,\n",
    "           filename=None,\n",
    "           figsize=two_column,aspect_ratio=1,s=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the old nebulae with high EW might come from Wolf Rayet Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the old nebulae with high EW might come from Wolf Rayet Stars\n",
    "WR_candidates = catalogue[(catalogue['age']>6) & (catalogue['age']<10) & (catalogue['eq_width']>80) & (catalogue['mass']>5e3)]\n",
    "print(f'{len(WR_candidates)} WR candidates')\n",
    "groups = WR_candidates.group_by('gal_name')\n",
    "spectra = []\n",
    "for key, group in zip(groups.groups.keys, groups.groups):\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spectra'/f'{key[\"gal_name\"]}_VorSpectra.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        tbl = Table(hdul[1].data)\n",
    "        spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    tbl['region_ID'] = np.arange(len(tbl))\n",
    "    tbl.add_index('region_ID')\n",
    "    for region_ID in group['region_ID']:\n",
    "        spectra.append((key['gal_name'],region_ID,spectral_axis,tbl.loc[region_ID]['SPEC']))\n",
    "        \n",
    "        hdu = fits.BinTableHDU(WR_candidates[['gal_name','region_ID','assoc_ID','x_neb','y_neb','age','mass','eq_width']],\n",
    "                       name='WR_candidates')\n",
    "hdu.writeto(basedir/'data'/'WR_candidates.fits',overwrite=True)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "filename = basedir/'reports'/'WR_candidates_spectra'\n",
    "nrows=6\n",
    "ncols=1\n",
    "width = two_column\n",
    "N = len(WR_candidates)\n",
    "Npage = nrows*ncols\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = spectra[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width*1.41))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            gal_name,region_ID,spectral_axis,spectrum = row\n",
    "            ax = next(axes_iter)\n",
    "            ax.plot(spectral_axis,spectrum,lw=0.5)\n",
    "            ax.set(xlim=[4860,6800],yscale='log',ylim=[1e2,1e6])\n",
    "            t = ax.text(0.01,0.87,f'{gal_name}: {region_ID:.0f}', transform=ax.transAxes,color='black',fontsize=8)\n",
    "        plt.subplots_adjust(wspace=-0.1, hspace=0)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "            h,l = fig.axes[0].get_legend_handles_labels()\n",
    "            ax = next(axes_iter)\n",
    "            ax.axis('off')\n",
    "            #ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "            #t = ax.text(0.06,0.87,'galaxy name: region ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use elipses for uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "ellipses = [Ellipse((row['age'],row['HA/FUV']),\n",
    "                     row['age_err'],row['HA/FUV_err'],\n",
    "                    alpha=0.5) for row in tmp]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for e in ellipses:\n",
    "    ax.add_artist(e)\n",
    "\n",
    "ax.set(xlim=(0, 10),ylim=(0,80),xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "ax.errorbar(tmp['age'],tmp['HA/FUV'],xerr=tmp['age_err'],yerr=tmp['HA/FUV_err'],fmt='o')\n",
    "\n",
    "ax.set(xlim=(0, 10),ylim=(0,80),xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look for correlations by comparing all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=8)\n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= (catalogue['neighbors']==0)\n",
    "\n",
    "tmp = catalogue[criteria].copy()\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    tmp['log_age'] = 6+np.log10(tmp['age'])\n",
    "    tmp['log_eq_width'] = np.log10(tmp['eq_width'])\n",
    "    tmp['log[SIII]/[SII]'] = np.log10(tmp['[SIII]/[SII]'])\n",
    "print(len(tmp))\n",
    "    \n",
    "columns = ['HA/FUV','eq_width','Delta_met_scal','logq_D91','age','mass',\n",
    "           'EBV_balmer','EBV_stars','GMC_sep','density','temperature']\n",
    "\n",
    "correlation = []\n",
    "pairs = []\n",
    "for a,b in combinations(columns,2):\n",
    "    y,x = tmp[a],tmp[b] \n",
    "    not_nan = ~np.isnan(x) & ~np.isnan(y)\n",
    "    r,p = spearmanr(x[not_nan],y[not_nan])\n",
    "    correlation.append(r)\n",
    "    pairs.append(a+'+'+b)\n",
    "correlation = Table([correlation,pairs],names=['r','pair'])\n",
    "correlation.sort('r')\n",
    "\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits['EBV_balmer'] = (0,1)\n",
    "limits['EBV_stars'] = (0,1)\n",
    "limits['GMC_sep'] = (0,10)\n",
    "\n",
    "corner(tmp,['EBV_balmer','EBV_stars','HA/FUV','eq_width','GMC_sep'],function=plot_function,\n",
    "       limits=limits,labels=labels,\n",
    "       filename=None,\n",
    "       figsize=two_column,aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Abundance gradients\n",
    "\n",
    "to help better understand the difference between abundances and local abundance offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "bins = np.logspace(-1.5,-0.2,10)\n",
    "xlim = (1e-2,1e0)\n",
    "cmap = mpl.cm.get_cmap('plasma')\n",
    "norm = mpl.colors.Normalize(vmin=9.4,vmax=11)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "table = nebulae[nebulae['[SIII]/[SII]']>3*nebulae['[SIII]/[SII]_err']]\n",
    "bins = np.logspace(*np.log10(np.nanpercentile(table['[SIII]/[SII]'],(2,98))),10)\n",
    "\n",
    "groups = table.group_by('gal_name')\n",
    "\n",
    "ax1.scatter(table['[SIII]/[SII]'],table['met_scal'],color='gray',s=0.1)\n",
    "ax2.scatter(table['[SIII]/[SII]'],table['Delta_met_scal'],color='gray',s=0.1)\n",
    "\n",
    "rho1,rho2 = [], []\n",
    "for group in groups.groups:\n",
    "    gal_name = group[0]['gal_name']\n",
    "\n",
    "    x,mean,std = bin_stat(group['[SIII]/[SII]'],group['met_scal'],[None,None],nbins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "\n",
    "    x,mean,std = bin_stat(group['[SIII]/[SII]'],group['Delta_met_scal'],[None,None],nbins=bins,statistic='median')\n",
    "    ax2.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(sample_table.loc[gal_name]['mass'])),label=gal_name)\n",
    "  \n",
    "    r1,p1 = spearmanr(group['[SIII]/[SII]'],group['met_scal'],nan_policy='omit')\n",
    "    r2,p2 = spearmanr(group['[SIII]/[SII]'],group['Delta_met_scal'],nan_policy='omit')\n",
    "    #print(f'rho = {r1:.2f}, {r2:.2f}')\n",
    "    rho1.append(r1)\n",
    "    rho2.append(r2)\n",
    "r,p = spearmanr(table['[SIII]/[SII]'],table['met_scal'],nan_policy='omit')\n",
    "t = ax1.text(0.07,0.9,r'$\\rho'+f'={r:.2f}$',transform=ax1.transAxes,fontsize=7)\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "r,p = spearmanr(table['[SIII]/[SII]'],table['Delta_met_scal'],nan_policy='omit')\n",
    "t = ax2.text(0.07,0.9,r'$\\rho'+f'={r:.2f}$',transform=ax2.transAxes,fontsize=7)\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',xlim=[2e-2,1],ylim=[8,8.8],\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$12+\\log (\\mathrm{O}/\\mathrm{H})$')\n",
    "ax2.set(xscale='log',xlim=[2e-2,1],ylim=[-0.15,0.15],\n",
    "        xlabel=r'$[\\mathrm{S}\\,\\textsc{iii}]/[\\mathrm{S}\\,\\textsc{ii}]$',ylabel=r'$\\Delta$(O/H)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "abundance_gradients = ascii.read(basedir/'data'/'external'/'radial_abundance_gradients.txt',\n",
    "                                names=['name','R0','g_r25'])\n",
    "abundance_gradients.add_index('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\Delta = 12+\\log (O/H) - g\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['r_R25'],tmp['met_scal'])\n",
    "r = np.linspace(0,0.5)\n",
    "ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for gal_name in np.unique(catalogue['gal_name']):\n",
    "    tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "    plt.scatter(tmp['met_scal'],tmp['Delta_met_scal'],c=tmp['r_R25'])\n",
    "    #plt.scatter(tmp['galactic_radius'],tmp['Delta_met_scal'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc=ax.scatter(tmp['met_scal'],tmp['Delta_met_scal'],c=tmp['r_R25'])\n",
    "fig.colorbar(sc,label='r / R25')\n",
    "ax.set(xlabel='12+logOH',ylabel=r'$\\Delta$')\n",
    "plt.show()\n",
    "#r = np.linspace(0,0.5)\n",
    "#ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gal_name = 'NGC0628'\n",
    "tmp = catalogue[catalogue['gal_name']==gal_name]\n",
    "R0 = abundance_gradients.loc[gal_name]['R0']\n",
    "g_r25 = abundance_gradients.loc[gal_name]['g_r25']\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc=ax.scatter(tmp['logq_D91'],tmp['met_scal'],c=tmp['r_R25'])\n",
    "fig.colorbar(sc,label='r / R25')\n",
    "ax.set(xlabel='log q',ylabel=r'$\\Delta$(O/H)')\n",
    "plt.show()\n",
    "#r = np.linspace(0,0.5)\n",
    "#ax.plot(r,R0+r*g_r25,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "for gal_name in np.unique(nebulae['gal_name']):\n",
    "    tmp = nebulae[nebulae['gal_name']==gal_name]\n",
    "    x,mean,std = bin_stat(tmp['logq_D91'],tmp['Delta_met_scal'],[5,8.5],nbins=10)\n",
    "    ax.errorbar(x,mean,yerr=std,fmt='-',label=gal_name)\n",
    "    #ax.scatter(tmp['logq_D91'],tmp['Delta_met_scal'],label=gal_name)\n",
    "ax.set(xlim=[5,8.5],ylim=[-0.2,0.2],xscale='linear')\n",
    "#ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age and Ha/FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.2) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#tmp = catalogue[criteria].copy()\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "\n",
    "xlim = [0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(4,4/1.618))\n",
    "sc = ax.scatter(tmp['age'],tmp['HA/FUV'],s=10,alpha=0.8,vmin=0,vmax=1)\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel=r'H$\\alpha$ / FUV',xlim=xlim,ylim=[0,100])\n",
    "\n",
    "#fig.colorbar(sc,label=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$')\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/f'all_galaxies_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(8,3))\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['overlap']=='contained')]\n",
    "ax1.scatter(tmp['dig/hii'],tmp['HA/FUV'],c=tmp['age'],vmin=0,vmax=20,s=2)\n",
    "\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['overlap']=='partial')]\n",
    "sc=ax2.scatter(tmp['dig/hii'],tmp['HA/FUV'],c=tmp['age'],vmin=0,vmax=20,s=2)\n",
    "\n",
    "ax1.set(xlim=[0,1],ylim=[0,100],xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$',ylabel=r'H$\\alpha$/FUV')\n",
    "ax2.set(xlim=[0,1],ylim=[0,100],xlabel=r'$f_\\mathrm{dig}/f_\\mathrm{H\\,\\textsc{ii}}$',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.11, 0.02, 0.84])\n",
    "fig.colorbar(sc,cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperate subplot for each galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "from scipy.stats import binned_statistic, pearsonr, spearmanr\n",
    "\n",
    "# '[SIII]/[SII]' , 'HA/FUV', 'AGE_MINCHISQ', 'AGE_BAYES'\n",
    "x_name, y_name = 'age', 'HA/FUV'\n",
    "xlim = [0.5,10.5]\n",
    "bins = 10\n",
    "max_sep = 1*u.arcsec\n",
    "\n",
    "#sample = set(np.unique(nebulae['gal_name'])) & hst_sample\n",
    "sample = np.unique(catalogue['gal_name'])[:-1]\n",
    "\n",
    "filename = basedir/'reports'/'all_objects_age_over_SII.pdf'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 3\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample): \n",
    "\n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    criteria = (catalogue['gal_name']==name) \n",
    "    criteria &= (catalogue['mass']>1e3) \n",
    "    #criteria &= ~np.isnan(catalogue['HA/FUV'])\n",
    "    criteria &= (catalogue['overlap'] == 'contained')\n",
    "    #criteria &= (catalogue['neighbors'] == 0)\n",
    "    \n",
    "    tmp = catalogue[criteria]\n",
    "    print(f'{name}: {len(tmp)} objects')\n",
    "    \n",
    "    mean, bin_edges, binnumber = binned_statistic(tmp[x_name],\n",
    "                                                  tmp[y_name],\n",
    "                                                  statistic='mean',\n",
    "                                                  bins=bins,\n",
    "                                                  range=xlim)\n",
    "    std, _, _ = binned_statistic(tmp[x_name],\n",
    "                                  tmp[y_name],\n",
    "                                  statistic='std',\n",
    "                                  bins=bins,\n",
    "                                  range=xlim)\n",
    "\n",
    "    ax.scatter(tmp[x_name],tmp[y_name],color='tab:blue',s=1)\n",
    "    # plot the standard divation with yerr=std\n",
    "    ax.errorbar((bin_edges[1:]+bin_edges[:-1])/2,mean,fmt='-')\n",
    "    ax.text(0.65,0.85,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "\n",
    "    ax.set(xlim=xlim)\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(f'{y_name.replace(\"_\",\" \")}')\n",
    "\n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "\n",
    "\n",
    "#plt.savefig(filename,dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "\n",
    "# '[SIII]/[SII]' , 'HA/FUV'\n",
    "x_name, y_name, z_name = '[SIII]/[SII]', 'HA/FUV', 'AGE_MINCHISQ'\n",
    "xlim = [0.5,10.5]\n",
    "bins = 10\n",
    "max_sep = 2*u.arcsec\n",
    "\n",
    "sample = muse_sample & hst_sample & astrosat_sample\n",
    "\n",
    "filename = basedir/'reports'/'all_objects_FUV_over_SII_with_age.pdf'\n",
    "\n",
    "#----------------------------------------------\n",
    "# DO NOT MODIFY BELOW\n",
    "#----------------------------------------------\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "if nrows*ncols<len(sample):\n",
    "    raise ValueError('not enough subplots for selected objects') \n",
    "width = two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "# loop over the galaxies we want to plot\n",
    "for name in sorted(sample): \n",
    "\n",
    "    # it makes a different if we match the clusters to the nebulae or the other way around\n",
    "    catalogcoord = clusters[clusters['gal_name']==name].copy()\n",
    "    matchcoord   = nebulae[nebulae['gal_name']==name].copy()\n",
    "\n",
    "    idx, sep, _ = match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n",
    "\n",
    "    catalogue = matchcoord.copy()\n",
    "\n",
    "    for col in catalogcoord.columns:\n",
    "        if col in catalogue.columns:\n",
    "            catalogue[f'{col}2'] = catalogcoord[idx][col]\n",
    "        else:\n",
    "            catalogue[col] = catalogcoord[idx][col]\n",
    "\n",
    "    catalogue['sep'] = sep\n",
    "    catalogue = catalogue[sep.__lt__(max_sep)]\n",
    "    catalogue = catalogue[~np.isnan(catalogue[x_name]) & ~np.isnan(catalogue[y_name]) & (catalogue['AGE_MINCHISQ']<10)]\n",
    "    print(f'{name}: {len(catalogue)} objects in joined catalogue')\n",
    "\n",
    "    # get the next axis and find position on the grid\n",
    "    ax = next(axes_iter)\n",
    "    if nrows>1 and ncols>1:\n",
    "        i, j = np.where(axes == ax)\n",
    "        i,j=i[0],j[0]\n",
    "    elif ncols>1:\n",
    "        i,j = 0, np.where(axes==ax)[0]\n",
    "    elif nrows>1:\n",
    "        i,j = np.where(axes==ax)[0],0\n",
    "    else:\n",
    "        i,j=0,0\n",
    "\n",
    "    #catalogue = catalogue[catalogue['HA6562_FLUX']>np.nanpercentile(catalogue['HA6562_FLUX'],50)]\n",
    "    #catalogue = catalogue[catalogue['FUV_FLUX_CORR']>3*catalogue['FUV_FLUX_CORR_ERR']]\n",
    "    #catalogue = catalogue[catalogue['SII6716_FLUX_CORR']>3*catalogue['SII6716_FLUX_CORR_ERR']]\n",
    "    #catalogue = catalogue[catalogue['SIII9068_FLUX_CORR']>3*catalogue['SIII9068_FLUX_CORR_ERR']]\n",
    "\n",
    "    r,p = spearmanr(catalogue['[SIII]/[SII]'],catalogue['HA/FUV'])\n",
    "    print(f'{name}: rho={r:.2f}, {len(catalogue)} objects')\n",
    "\n",
    "    sc = ax.scatter(catalogue['[SIII]/[SII]'],catalogue['HA/FUV'],\n",
    "                    c=catalogue[z_name],vmin=0, vmax=10,cmap=plt.cm.RdBu_r,\n",
    "                    s=3,marker='.')\n",
    "    \n",
    "    ax.text(0.05,0.9,f'{name}', transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.7,0.15,r'$\\rho$'+f'={r:.2f}',transform=ax.transAxes,fontsize=7)\n",
    "    ax.text(0.55,0.05,f'{len(catalogue):.0f} objects', transform=ax.transAxes,fontsize=7)\n",
    "    \n",
    "    ax.set(xscale='log',yscale='log',xlim=[1e-2,1],ylim=[2,2e2])\n",
    "    # https://stackoverflow.com/questions/21920233/matplotlib-log-scale-tick-label-number-formatting/33213196\n",
    "    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    if i==nrows-1:\n",
    "        ax.set_xlabel('[SIII]/[SII]')\n",
    "    if j==0:\n",
    "        ax.set_ylabel(r'H$\\alpha$ / FUV')\n",
    "\n",
    "fig.colorbar(sc,ax=axes.ravel().tolist(),label=f'{z_name.replace(\"_\",\" \")}')\n",
    "        \n",
    "for i in range(nrows*ncols-len(sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "    # add the xlabel to the axes above\n",
    "    axes[nrows-2,ncols-1-i].set_xlabel(f'{x_name.replace(\"_\",\" \")}')\n",
    "\n",
    "\n",
    "plt.savefig(filename,dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalogue statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3,ax4)=plt.subplots(nrows=4,figsize=(single_column,single_column*1.5))\n",
    "\n",
    "ax1.hist(catalogue['mass'],bins=np.logspace(2,5.5,15))\n",
    "ax1.axvline(1e4,color='black')\n",
    "ax1.set(xscale='log',xlabel='mass / Msun',xlim=[1e2,10**5.5])\n",
    "\n",
    "ax2.hist(catalogue['age'],bins=np.logspace(0,2,11))\n",
    "ax2.axvline(8,color='black')\n",
    "ax2.set(xscale='log',xlabel='age / Myr',xlim=[1,100])\n",
    "\n",
    "ax3.hist(100*catalogue['overlap_asc'],bins=np.linspace(0,100,11))\n",
    "ax3.axvline(90,color='black')\n",
    "ax3.set(xlabel='overlap percentage associations',xlim=[0,100])\n",
    "\n",
    "ax4.hist(100*catalogue['overlap_neb'],bins=np.linspace(0,100,11))\n",
    "#ax4.axvline(95,color='black')\n",
    "ax4.set(xlabel='overlap percentage nebulae',xlim=[0,100])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'tmp_catalogue_properties.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "xlim = [1e2,1e5]\n",
    "ylim = [1,100]\n",
    "\n",
    "x,y = catalogue['mass'], catalogue['age']\n",
    "nbins=11\n",
    "bins = [np.logspace(*np.log10(xlim),16),np.logspace(*np.log10(ylim),9)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=False)\n",
    "im = ax.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.Reds)\n",
    "ax.plot([1e4,1e4,1e5],[1,8,8],color='black',lw=2)\n",
    "\n",
    "ax.set(xlim=xlim,xscale='log',xlabel=r'mass / M$_\\odot$',\n",
    "       ylim=ylim,yscale='log',ylabel='age / Myr')\n",
    "fig.colorbar(im,label='count')\n",
    "plt.savefig(basedir/'reports'/'tmp_catalogue_properties_2D_hist.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(7, 2), height_ratios=(2, 7),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.03, hspace=0.05)\n",
    "\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "\n",
    "xlim = [63.095734446471226,1.5848931925e5]\n",
    "ylim = [1,39.81071706] # based on 10 with 9 bins\n",
    "ylim = [1,38.05457130] # based on 8 with 8 bins\n",
    "x,y = catalogue['mass'], catalogue['age']\n",
    "nbins=11\n",
    "bins = [np.logspace(*np.log10(xlim),18),np.logspace(*np.log10(ylim),8)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=False)\n",
    "im = ax.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.Reds,zorder=0)\n",
    "ax.plot([1e4,1e4,xlim[1]],[1,8,8],color='black',lw=2)\n",
    "\n",
    "ax_histx.hist(x,bins[0],color='#af1d1d',histtype='step')\n",
    "ax_histy.hist(y,bins[1],orientation='horizontal',color='#af1d1d',histtype='step')\n",
    "\n",
    "ax_histx.set_yticks([0,100,200,300,400,500],minor=True)\n",
    "ax_histy.set_xticks([0,250,500,750,1000],minor=True)\n",
    "ax_histx.set(ylim=[0,600])\n",
    "\n",
    "ax.set(xlim=xlim,xscale='log',xlabel=r'mass / M$_\\odot$',\n",
    "       ylim=ylim,yscale='log',ylabel='age / Myr')\n",
    "ax.set_yticks([1,10])\n",
    "ax.set_yticklabels([1,10])\n",
    "fig.colorbar(im,label='count',ax=ax_histy)\n",
    "plt.savefig(basedir/'reports'/'catalogue_properties_2D_hist.pdf',dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HII regions without 1to1 match\n",
    "\n",
    "\n",
    "### Multiple associations in one nebula\n",
    "\n",
    "so far we restricted ourselves to HII regions and associations that show a one-to-one relation. This reduces the sample to 4062 HII regions. However we saw that over 8000 HII regions overlap with an associations, so there is a lot of potential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.hist(nebulae['Nassoc'],bins=np.arange(.5,6.5))\n",
    "ax1.set_title('Number of associations per nebulae')\n",
    "\n",
    "ax2.hist(associations['Nnebulae'],bins=np.arange(0.5,6.5))\n",
    "ax2.set_title('Number of nebulae per associations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for our analysis we limit ourselves to the associations that show a one-to-one relation. However including HII regions that overlap with multiple associations has the potential to increase the sample size by 50%. \n",
    "However it is difficult to assess the contribution of multiple associations. We first look at the age. According to stellar models, the ionising photon flux stays almost constant for the first few Myr (<5Myr) and than quickly drops. By the time the cluster is 10 Myr old, it has dropped to less than 1% of its initial value. By the time the cluster is 20 Myr old, the ionising photon flux has dropped to 1/6000 of its initial value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form tqdm import tqdm \n",
    "\n",
    "candidates = []\n",
    "\n",
    "for gal_name in tqdm(np.unique(associations['gal_name'])):\n",
    "\n",
    "    HIIregion_mask = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "    neb_tmp = nebulae[(nebulae['gal_name']==gal_name) & HIIregion_mask]\n",
    "    assoc_tmp = associations[associations['gal_name']==gal_name].copy()\n",
    "    assoc_tmp.add_index('assoc_ID')\n",
    "\n",
    "    with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_nebulae.yml') as f:\n",
    "        nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{gal_name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "        associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "    # we select each nebulae with more than one HII region\n",
    "    for region_ID in neb_tmp[neb_tmp['Nassoc']>1]['region_ID']:\n",
    "        sub = assoc_tmp.loc[nebulae_dict[region_ID]]\n",
    "        # first we check if we can ignore the other associations in the nebulae\n",
    "        if np.sum((sub['age']<10) & (sub['mass']>1e4)) == 1:\n",
    "            # then we make sure that the one associations also overlaps sufficiently\n",
    "            if np.sum((sub['age']<10) & (sub['mass']>1e4) & (sub['overlap_asc']>0.5)) == 1:\n",
    "                candidates.append(region_ID)\n",
    "        \n",
    "print(f'{len(candidates)} of {np.sum(nebulae_tmp[\"Nassoc\"]>1)} match the criteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imf import make_cluster\n",
    "\n",
    "cluster = make_cluster(5e3)\n",
    "cluster.sort()\n",
    "cluster = cluster[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolated HII regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with DOLPHONT peaks\n",
    "\n",
    "the following code goes through each compact cluster and DOLPHOT peak and determines if they fall inside an HII region. The `region_ID` is recorded (nan if outside) and the result is saved in a new catalogue file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(matrix, index, default_value=np.nan):\n",
    "    '''\n",
    "    The `to_pixel` method returns the x,y coordinates. However in the \n",
    "    image they correspond to img[y,x]\n",
    "    '''\n",
    "    result = np.zeros(len(index))+default_value\n",
    "    mask = (index[:,1] < matrix.shape[0]) & (index[:,0] < matrix.shape[1])\n",
    "    mask &= (index[:,1] >= 0) & (index[:,0] >=0)\n",
    "\n",
    "    valid = index[mask]\n",
    "    result[mask] = matrix[valid[:,1], valid[:,0]]\n",
    "    return result\n",
    "\n",
    "isolated = nebulae[nebulae['overlap_neb']==0]\n",
    "\n",
    "compact_clusters_list = []\n",
    "dolphot_peaks_list = []\n",
    "\n",
    "for gal_name in np.unique(sample_table['name']):\n",
    "\n",
    "    # we are only interested in peaks that overlap with isolated HII regions\n",
    "    sub_nebulae = nebulae[(nebulae['gal_name']==gal_name) & (nebulae['in_frame'])] # & (nebulae['overlap_neb']==0)]\n",
    "    \n",
    "    print(f'working on {gal_name}')\n",
    "    \n",
    "    # read the nebula mask\n",
    "    filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{gal_name}*.fits'))\n",
    "    copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "    \n",
    "    # we only need the wcs to convert from pixel to skycoord\n",
    "    filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "    if not filename.is_file():\n",
    "        print(f'no F275W image for {gal_name}')\n",
    "        continue\n",
    "    with fits.open(filename) as hdul:\n",
    "        #F275 = NDData(hdul[0].data,\n",
    "        #              mask=hdul[0].data==0,\n",
    "        #              meta=hdul[0].header,\n",
    "        #              wcs=WCS(hdul[0].header))\n",
    "        wcs_F275=WCS(hdul[0].header)\n",
    "    \n",
    "    # read the compact cluster and DOLPHOT catalogues\n",
    "    filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "    if not filename.is_file():\n",
    "        print(f'no compact clusters for {gal_name}')\n",
    "    else:     \n",
    "        with fits.open(filename) as hdul:\n",
    "            compact_clusters = Table(hdul[1].data)\n",
    "        compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "        compact_clusters.add_index('ID_PHANGS_CLUSTERS')\n",
    "\n",
    "        compact_clusters['region_ID'] = get_value(nebulae_mask.data,np.array(compact_clusters['SkyCoord'].to_pixel(nebulae_mask.wcs)).T.astype(int))\n",
    "\n",
    "        # we save all objects that fall inside one of our empty HII regions\n",
    "        compact_clusters.add_column(gal_name,index=0,name='gal_name')\n",
    "        del compact_clusters['SkyCoord']\n",
    "        compact_clusters_list.append(compact_clusters[np.isin(compact_clusters[\"region_ID\"],sub_nebulae[\"region_ID\"])])\n",
    "        #compact_clusters_list.append(compact_clusters)\n",
    "\n",
    "    # and now for the peak catalogue\n",
    "    filename = data_ext/'Products'/'stellar_associations'/'DOLPHOT'/f'{gal_name.lower()}_uvis_dolphot.fits'\n",
    "    if not filename.is_file():\n",
    "        print(f'no DOLPHOT catalogue for {gal_name}')\n",
    "        continue  \n",
    "    with fits.open(filename) as hdul:\n",
    "        dolphot_peaks = Table(hdul[1].data)\n",
    "    dolphot_peaks.add_index('Dolphot_ID')\n",
    "    dolphot_peaks['SkyCoord'] = SkyCoord.from_pixel(dolphot_peaks['Dolphot_x'],dolphot_peaks['Dolphot_y'],wcs=wcs_F275)\n",
    "    dolphot_peaks['region_ID'] = get_value(nebulae_mask.data,np.array(dolphot_peaks['SkyCoord'].to_pixel(nebulae_mask.wcs)).T.astype(int))\n",
    "\n",
    "    dolphot_peaks.add_column(gal_name,index=0,name='gal_name')\n",
    "    dolphot_peaks.add_column(dolphot_peaks['SkyCoord'].ra.deg,index=3,name='RA')\n",
    "    dolphot_peaks.add_column(dolphot_peaks['SkyCoord'].dec.deg,index=4,name='DEC')\n",
    "    del dolphot_peaks['SkyCoord']\n",
    "    dolphot_peaks_list.append(dolphot_peaks[np.isin(dolphot_peaks[\"region_ID\"],sub_nebulae[\"region_ID\"])])\n",
    "\n",
    "compact_clusters = vstack(compact_clusters_list)\n",
    "dolphot_peaks = vstack(dolphot_peaks_list)\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(compact_clusters)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto(basedir/'data'/'interim'/f'compact_clusters_in_isolated_HIIregions.fits',overwrite=True)\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(dolphot_peaks)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto(basedir/'data'/'interim'/f'dolphot_peaks_in_isolated_HIIregions.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result from the previous step is saved in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolphot_peaks = Table(fits.getdata(basedir/'data'/'interim'/f'dolphot_peaks_in_isolated_HIIregions.fits'))\n",
    "compact_clusters = Table(fits.getdata(basedir/'data'/'interim'/f'compact_clusters_in_isolated_HIIregions.fits'))\n",
    "compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "dolphot_peaks['SkyCoord'] = SkyCoord(dolphot_peaks['RA']*u.deg,dolphot_peaks['DEC']*u.deg)\n",
    "dolphot_peaks['S2N_WFC3_F555W'][dolphot_peaks['gal_name']=='NGC0628'] = dolphot_peaks['S2N_WFC3_F555W'][dolphot_peaks['gal_name']=='NGC0628'] = dolphot_peaks['S2N_ACS_F555W'][dolphot_peaks['gal_name']=='NGC0628']\n",
    "\n",
    "S2N = 5\n",
    "dolphot_peaks['detection'] = np.sum([(dolphot_peaks[f'S2N_WFC3_F{band}W']>S2N) for band in [275,336,438,555,814]],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record how many peaks fall in each HII region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebulae['Ncluster'] = False\n",
    "for gal_name in sample_table['name']:  \n",
    "    nebulae['Ncluster'][nebulae['gal_name']==gal_name] = np.isin(nebulae[nebulae['gal_name']==gal_name]['region_ID'],compact_clusters[compact_clusters['gal_name']==gal_name]['region_ID'])\n",
    "\n",
    "# there is no nebulae catalogue for NGC1300 and NGC4254\n",
    "criteria = nebulae['in_frame'] & np.isin(nebulae['gal_name'],np.unique(compact_clusters['gal_name']))\n",
    "N_isolated = np.sum(criteria)\n",
    "print(f\"{np.sum(nebulae['Ncluster'])} ({100*np.sum(nebulae['Ncluster']) / N_isolated:.1f} %) have a compact cluster\")    \n",
    "\n",
    "nebulae['Ndolphot'] = np.nan\n",
    "for gal_name in sample_table['name']:\n",
    "    tmp = dolphot_peaks[(dolphot_peaks['S2N_WFC3_F275W']>S2N) &         # detection in NUV\n",
    "                        (dolphot_peaks['detection']>=2) &               # detection in at least one other band\n",
    "                        (dolphot_peaks['gal_name']==gal_name)]    \n",
    "    nebulae['Ndolphot'][nebulae['gal_name']==gal_name] = [np.sum(tmp['region_ID']==region_ID) for region_ID in nebulae[nebulae['gal_name']==gal_name]['region_ID']]\n",
    "\n",
    "# same criteria as above but also with no clusters\n",
    "criteria = (nebulae['overlap_neb']==0) & (~nebulae['Ncluster']) & nebulae['in_frame'] & np.isin(nebulae['gal_name'],np.unique(compact_clusters['gal_name']))\n",
    "N_isolated = np.sum(criteria)\n",
    "N_dolphot = np.sum((nebulae['Ndolphot']>0) & criteria)\n",
    "print(f\"{N_dolphot} ({100*N_dolphot / N_isolated:.1f} %) have a DOLPHOT peak\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "gal_name = 'NGC2835'\n",
    "sub = nebulae #[(nebulae['gal_name']==gal_name)]\n",
    "\n",
    "\n",
    "tmp = sub[(sub['overlap_neb']>0) & (sub['in_frame'])]\n",
    "label = f\"stellar association: {np.mean(tmp['HA6562_LUM_CORR']):.2g} erg s-1\"\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,histtype='step',alpha=0.6,label=label)\n",
    "print(label)\n",
    "\n",
    "tmp = sub[(sub['overlap_neb']==0) & (sub['in_frame']) & (sub['Ncluster']>0)]\n",
    "label = f\"Compact cluster: {np.mean(tmp['HA6562_LUM_CORR']):.2g} erg s-1\"\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,histtype='step',alpha=0.6,label=label)\n",
    "print(label)\n",
    "\n",
    "tmp = sub[(sub['overlap_neb']==0) & (sub['in_frame']) & (sub['Ncluster']==0) & (sub['Ndolphot']>0)]\n",
    "label = f\"DOLPHOT: {np.mean(tmp['HA6562_LUM_CORR']):.2g} erg s-1\"\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,histtype='step',alpha=0.6,label=label)\n",
    "print(label)\n",
    "\n",
    "\n",
    "tmp = sub[(sub['overlap_neb']==0) & (sub['in_frame']) & (sub['Ndolphot']==0)]\n",
    "label = f\"isolated: {np.mean(tmp['HA6562_LUM_CORR']):.2g} erg s-1\"\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,histtype='step',alpha=0.6,label=label)\n",
    "print(label)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='linear',ylim=[1,None],ylabel='N')\n",
    "plt.savefig(basedir/'reports'/'Ha_luminosity_function_isolated_vs_matched.png',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a joined HII region and dolphot peak catalogue for single peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(dolphot_peaks)} peaks in initial catalogue')\n",
    "lst = []\n",
    "S2N = 10\n",
    "for gal_name in sample_table['name']:\n",
    "    tmp = dolphot_peaks[dolphot_peaks['gal_name']==gal_name]\n",
    "    tmp = tmp[(tmp['S2N_WFC3_F275W']>S2N) & (tmp['S2N_WFC3_F555W']>S2N)]\n",
    "    ids, counts = np.unique(tmp['region_ID'],return_counts=True)\n",
    "    ids = ids[counts==1]\n",
    "    tmp = tmp[np.isin(tmp['region_ID'],ids)]\n",
    "    HII_sub = nebulae[(nebulae['gal_name']==gal_name)]\n",
    "    del HII_sub['SkyCoord_neb']\n",
    "    \n",
    "    if len(tmp)>0:\n",
    "        tmp = join(tmp,HII_sub,keys=['gal_name','region_ID'])\n",
    "        tmp['MV'] = tmp['Mag_WFC3_F555W'] - Distance(tmp['distance']*u.Mpc).distmod.value\n",
    "        lst.append(tmp)\n",
    "\n",
    "single_peaks = vstack(lst)\n",
    "single_peaks['SkyCoord'] = SkyCoord(single_peaks['RA']*u.deg,single_peaks['DEC']*u.deg)\n",
    "print(f'{len(single_peaks)} single peaks in final catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show color-color-diagram and cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[-0.33 , -1.662],[-0.307, -1.577],[-0.25 , -1.468],[-0.218, -1.394],[-0.186, -1.321],\n",
    "                   [-0.138, -1.235],[-0.076, -1.213],[-0.007, -1.206],[ 0.065, -1.197],[ 0.135, -1.187],\n",
    "                   [ 0.203, -1.198],[ 0.276, -1.201],[ 0.347, -1.209],[ 0.418, -1.219],[ 0.488, -1.233],\n",
    "                   [ 0.559, -1.251],[ 0.63 , -1.27 ],[ 0.706, -1.264],[ 0.771, -1.287],[ 0.648, -1.119],\n",
    "                   [ 0.626, -1.026],[ 0.565, -0.989],[ 0.559, -0.87 ],[ 0.502, -0.807],[ 0.481, -0.706],\n",
    "                   [ 0.457, -0.546],[ 0.505, -0.449],[ 0.487, -0.384],[ 0.488, -0.328],[ 0.468, -0.239],\n",
    "                   [ 0.517, -0.109],[ 0.555, -0.015],[ 0.6  ,  0.041],[ 0.65 ,  0.132],[ 0.724,  0.174],\n",
    "                   [ 0.864,  0.169],[ 0.948,  0.166],[ 1.02 ,  0.152],[ 1.138,  0.182],[ 1.203,  0.194],\n",
    "                   [ 1.253,  0.282],[ 1.331,  0.378],[ 1.354,  0.469]])\n",
    "\n",
    "x,y=points.T\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "S2N = 10\n",
    "criteria  =  single_peaks['S2N_WFC3_F275W']>S2N\n",
    "for band in [275,336,438,555,814]:\n",
    "    criteria &= single_peaks[f'S2N_WFC3_F{band}W']>S2N\n",
    "#criteria &= single_peaks['HA6562_LUM_CORR']<1.5e37\n",
    "\n",
    "tmp = single_peaks[criteria]\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "U = tmp['Mag_WFC3_F336W']\n",
    "B = tmp['Mag_WFC3_F438W']\n",
    "V = tmp['Mag_WFC3_F555W']\n",
    "I = tmp['Mag_WFC3_F814W']\n",
    "\n",
    "ax1.scatter(V-I,U-B)\n",
    "ax1.plot(x,y,color='black')\n",
    "ax1.set(xlim=[-1,2],ylim=[-2.5,1],\n",
    "      ylabel=r'F336W$-$F438W',xlabel=r'F555W$-$F814W')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.scatter(B-V,V)\n",
    "ax2.set(xlim=[-1,2.5],ylim=[18,28],\n",
    "      ylabel=r'F555W',xlabel=r'F438W$-$F555W')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "bins = np.logspace(35,39,20)\n",
    "ax3.hist(tmp['HA6562_LUM_CORR'],bins=bins)\n",
    "ax3.set(xscale='log',xlabel=r'H$\\alpha$ / erg s$^{-1}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be careful. For NGC0628, the S/N for some filters is weird (F275W and F336W look fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 0  # isolated\n",
    "n2 = 0  # cluster\n",
    "n3 = 0  # dolphot peaks\n",
    "S2N = 3\n",
    "for gal_name in np.unique(sample_table['name']):\n",
    "    \n",
    "    sub_nebulae = nebulae[(nebulae['gal_name']==gal_name)  & (nebulae['in_frame'])]\n",
    "    sub_dolphot_peaks = dolphot_peaks[(dolphot_peaks['gal_name']==gal_name) & (dolphot_peaks['S2N_WFC3_F275W']>S2N) & (dolphot_peaks['S2N_WFC3_F555W']>S2N)]\n",
    "    #sub_dolphot_peaks = dolphot_peaks[(dolphot_peaks['gal_name']==gal_name) & (dolphot_peaks['Mag_WFC3_F275W']<25)] # & (dolphot_peaks['Mag_WFC3_F555W']<25)]\n",
    "    sub_compact_clusters = compact_clusters[compact_clusters['gal_name']==gal_name]\n",
    "    \n",
    "    if len(sub_dolphot_peaks)==0:\n",
    "        continue\n",
    "    \n",
    "    isolated_nebulae = sub_nebulae[(sub_nebulae['overlap_neb']==0)]\n",
    "    n_overlap_with_cluster = np.sum(np.isin(isolated_nebulae['region_ID'],sub_compact_clusters['region_ID']))\n",
    "    n_overlap_with_peak = np.sum(np.isin(isolated_nebulae['region_ID'],sub_dolphot_peaks['region_ID']))\n",
    "       \n",
    "    n1+=len(isolated_nebulae)\n",
    "    n2+=n_overlap_with_cluster\n",
    "    n3+=n_overlap_with_peak\n",
    "    print(f'{gal_name:>8}: {len(isolated_nebulae):>5} isolated HII regions. {n_overlap_with_cluster:>3} ({100*n_overlap_with_cluster/len(isolated_nebulae):>4.1f}%), {n_overlap_with_peak:>3} ({100*n_overlap_with_peak/len(isolated_nebulae):>4.1f}%)')\n",
    "        \n",
    "print(f'{\"all\":>8}: {n1:>5} isolated HII regions. {n2:>3} ({100*n2/n1:>4.1f}%), {n3:>3} ({100*n3/n1:>4.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Account for associations of different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the nebulae catalogue with additional information\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'16pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*16pc_nebulae.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    lst.append(tbl)\n",
    "nebulae_tmp16 = vstack(lst)\n",
    "nebulae_tmp16.rename_column('overlap_neb','overlap_16pc')\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'32pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*32pc_nebulae.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    lst.append(tbl)\n",
    "nebulae_tmp32 = vstack(lst)\n",
    "nebulae_tmp32.rename_column('overlap_neb','overlap_32pc')\n",
    "folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'64pc'\n",
    "lst = []\n",
    "for file in folder.glob(f'*64pc_nebulae.fits'):\n",
    "    gal_name = file.stem.split('_')[0]\n",
    "    tbl = Table(fits.getdata(file,ext=1))\n",
    "    tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "    lst.append(tbl)\n",
    "nebulae_tmp64 = vstack(lst)\n",
    "nebulae_tmp64.rename_column('overlap_neb','overlap_64pc')\n",
    "\n",
    "multiscale_nebulae = join(nebulae_tmp16[['gal_name','region_ID','overlap_16pc']],nebulae_tmp32[['gal_name','region_ID','overlap_32pc']],keys=['gal_name','region_ID'])\n",
    "multiscale_nebulae = join(multiscale_nebulae,nebulae_tmp64[['gal_name','region_ID','overlap_64pc']],keys=['gal_name','region_ID'])\n",
    "multiscale_nebulae = join(multiscale_nebulae,nebulae[['gal_name','region_ID','in_frame','SkyCoord_neb']],keys=['gal_name','region_ID'])\n",
    "\n",
    "np.sum(multiscale_nebulae['in_frame'] & (multiscale_nebulae['overlap_16pc']==0) & (multiscale_nebulae['overlap_32pc']==0) & (multiscale_nebulae['overlap_64pc']==0))\n",
    "\n",
    "multiscale_nebulae['overlap_any'] = (multiscale_nebulae['overlap_16pc']>0) | (multiscale_nebulae['overlap_32pc']>0) | (multiscale_nebulae['overlap_64pc']>0)\n",
    "\n",
    "# there is no nebulae catalogue for NGC1300 and NGC4254\n",
    "N_isolated = np.sum(multiscale_nebulae['in_frame'] & (multiscale_nebulae['overlap_32pc']==0))\n",
    "N_multi = np.sum(multiscale_nebulae['overlap_any'] & (multiscale_nebulae['overlap_32pc']==0))\n",
    "\n",
    "print(f'{N_isolated} HII regions do not overlap with association')\n",
    "print(f\"{N_multi} ({100*N_multi / N_isolated:.1f} %) have association of different scale\")    \n",
    "\n",
    "multiscale_nebulae['Ncluster'] = False\n",
    "for gal_name in sample_table['name']:  \n",
    "    multiscale_nebulae['Ncluster'][multiscale_nebulae['gal_name']==gal_name] = np.isin(multiscale_nebulae[multiscale_nebulae['gal_name']==gal_name]['region_ID'],compact_clusters[compact_clusters['gal_name']==gal_name]['region_ID'])\n",
    "\n",
    "# there is no nebulae catalogue for NGC1300 and NGC4254\n",
    "criteria = ~multiscale_nebulae['overlap_any'] & multiscale_nebulae['in_frame'] & np.isin(multiscale_nebulae['gal_name'],np.unique(compact_clusters['gal_name']))\n",
    "N_isolated = np.sum(criteria)\n",
    "N_cluster = np.sum(multiscale_nebulae['Ncluster'] & ~multiscale_nebulae['overlap_any'])\n",
    "print(f\"{N_cluster} ({100*N_cluster / N_isolated:.1f} %) have a compact cluster\")    \n",
    "\n",
    "multiscale_nebulae['Ndolphot'] = np.nan\n",
    "for gal_name in sample_table['name']:\n",
    "    tmp = dolphot_peaks[(dolphot_peaks['S2N_WFC3_F275W']>S2N) &         # detection in NUV\n",
    "                        (dolphot_peaks['detection']>=2) &               # detection in at least one other band\n",
    "                        (dolphot_peaks['gal_name']==gal_name)]    \n",
    "    multiscale_nebulae['Ndolphot'][multiscale_nebulae['gal_name']==gal_name] = [np.sum(tmp['region_ID']==region_ID) for region_ID in multiscale_nebulae[multiscale_nebulae['gal_name']==gal_name]['region_ID']]\n",
    "\n",
    "# same criteria as above but also with no clusters\n",
    "criteria = ~multiscale_nebulae['overlap_any'] & (~multiscale_nebulae['Ncluster']) & multiscale_nebulae['in_frame'] & np.isin(multiscale_nebulae['gal_name'],np.unique(compact_clusters['gal_name']))\n",
    "N_isolated = np.sum(criteria)\n",
    "N_dolphot = np.sum((multiscale_nebulae['Ndolphot']>0) & criteria)\n",
    "print(f\"{N_dolphot} ({100*N_dolphot / N_isolated:.1f} %) have a DOLPHOT peak\")    \n",
    "\n",
    "N_truley_isolated = np.sum(~multiscale_nebulae['overlap_any'] & ~multiscale_nebulae['Ncluster'] & multiscale_nebulae['in_frame'] & (multiscale_nebulae['Ndolphot']==0))\n",
    "N_tot = np.sum(multiscale_nebulae['in_frame'])\n",
    "print(f\"{100*N_truley_isolated/N_tot:.2f}% of the HII regions are truely isolated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~multiscale_nebulae['overlap_any'] & ~multiscale_nebulae['Ncluster'] & multiscale_nebulae['in_frame']) / np.sum(multiscale_nebulae['in_frame'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cutouts for isolated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from cluster.plot import single_cutout\n",
    "\n",
    "S2N = 10\n",
    "\n",
    "#tmp = compact_clusters[(compact_clusters['PHANGS_MASS_MINCHISQ']>1e4) & (compact_clusters['PHANGS_AGE_MINCHISQ']<10)]\n",
    "#criteria =  single_peaks['S2N_WFC3_F275W']>S2N\n",
    "#criteria &= single_peaks['S2N_WFC3_F555W']>S2N\n",
    "#criteria &= single_peaks['Mag_WFC3_F336W']-single_peaks['Mag_WFC3_F438W']<-1\n",
    "#tmp = single_peaks[criteria]\n",
    "\n",
    "tmp = nebulae[(nebulae['Ndolphot']==0) & (nebulae['overlap_neb']==0) & (nebulae['in_frame'])]\n",
    "tmp = nebulae[(nebulae['Ndolphot']==0) & (nebulae['overlap_neb']==0) & (nebulae['in_frame']) & \n",
    "              (nebulae['gal_name']=='NGC1672')]\n",
    "tmp = nebulae_with_single_peak[:50]\n",
    "\n",
    "print(f'{len(tmp)} objects in sample')\n",
    "\n",
    "size=8*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'nebulae_with_single_peak'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(tmp) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = tmp[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "            \n",
    "            # for a new galaxy we need to read in the masks/images\n",
    "            if row['gal_name'] != gal_name:\n",
    "                \n",
    "                gal_name = row['gal_name']\n",
    "                \n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    F275 = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                    \n",
    "                '''\n",
    "                # HST image for the background\n",
    "                filename = data_ext / 'HST' / 'narrowband_Halpha' / f'{gal_name}_ha_sub.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    hst_halpha = NDData(hdul[0].data,\n",
    "                                  mask=hdul[0].data==0,\n",
    "                                  meta=hdul[0].header,\n",
    "                                  wcs=WCS(hdul[0].header))\n",
    "                '''\n",
    "                \n",
    "                filename = data_ext / 'MUSE' / 'DR2.1' / 'MUSEDAP' / f'{gal_name}_MAPS.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                                    meta=hdul['HA6562_FLUX'].header,\n",
    "                                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "            \n",
    "                # nebulae mask\n",
    "                filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "                with fits.open(filename) as hdul:\n",
    "                    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "                    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "                \n",
    "                # association mask\n",
    "                associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                      target=gal_name.lower(),\n",
    "                                                      scalepc=scalepc,\n",
    "                                                      data='mask')\n",
    "\n",
    "            \n",
    "            ax = next(axes_iter)\n",
    "            ax = single_cutout(ax,\n",
    "                             position = row['SkyCoord_neb'],\n",
    "                             image = F275,\n",
    "                             mask1 = nebulae_mask,\n",
    "                             mask2 = associations_mask,\n",
    "                             points = single_peaks,\n",
    "                             label = f\"{row['gal_name']}: {row['region_ID']:.0f}\",\n",
    "                             size  = size)\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truly isolated HII regions\n",
    "\n",
    "are the HII regions without a peak more extincted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "bins = np.linspace(0,1,10)\n",
    "\n",
    "tmp = nebulae[(nebulae['overlap_neb']>0) & (nebulae['in_frame'])]\n",
    "#ax.hist(tmp['EBV_balmer'],bins=bins,alpha=0.5,label='association')\n",
    "\n",
    "tmp = nebulae[(nebulae['Ndolphot']>0) & (nebulae['overlap_neb']==0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['EBV_balmer'],bins=bins,alpha=0.5,label='overlap')\n",
    "\n",
    "tmp = nebulae[(nebulae['Ndolphot']==0) & (nebulae['overlap_neb']==0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['EBV_balmer'],bins=bins,alpha=0.5,label='isolated')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel=r'$E(B-V)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks at the brightest peak inside of them\n",
    "\n",
    "with pandas it is easy to select the min/max row in a group based on a column so we convert to a DataFrame and back to a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "faint_peaks = []\n",
    "for gal_name in sample_table['name']:\n",
    "    \n",
    "    sub_nebulae = nebulae[(nebulae['gal_name']==gal_name) & (nebulae['Ndolphot']==0)]\n",
    "    sub_peaks = dolphot_peaks[dolphot_peaks['gal_name']==gal_name]\n",
    "\n",
    "    df = sub_peaks.to_pandas()\n",
    "    faint_peaks.append(Table.from_pandas(df.loc[df.groupby('region_ID')['Mag_WFC3_F275W'].idxmin()]))\n",
    "    break\n",
    "faint_peaks=faint_peaks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "\n",
    "ax1.hist(faint_peaks['Mag_WFC3_F275W'],bins=np.linspace(20,28,16))\n",
    "ax1.set(xlim=[20,28],xlabel=r'm NUV',yscale='linear',ylabel='N')\n",
    "ax2.hist(faint_peaks['S2N_WFC3_F275W'],bins=np.linspace(-0.5,10.5,11))\n",
    "ax2.set(xlim=[0,10],xlabel=r'S/N NUV',yscale='linear',ylabel='N')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(35,40,20)\n",
    "\n",
    "tmp = nebulae[(nebulae['overlap_neb']>0) & (nebulae['in_frame'])]\n",
    "ax.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='matched')\n",
    "\n",
    "ax.hist(isolated_HIIregions['HA6562_LUM_CORR'],bins=bins,alpha=0.6,label='isolated')\n",
    "\n",
    "ax.hist(single_peaks['HA6562_LUM_CORR'],bins=bins,alpha=0.7,label='O-Star?')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',xlim=[2e35,9e39],xlabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='linear',ylim=[1,2e3],ylabel='N')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the concentration index of the new objects to see why they were rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2N = 5\n",
    "\n",
    "gal_name = 'NGC4535'\n",
    "tmp = dolphot_peaks[(dolphot_peaks['detection']>=2) & \n",
    "                    (dolphot_peaks['Mag_WFC3_F275W']<25) & \n",
    "                    (dolphot_peaks['S2N_WFC3_F275W']>S2N) &\n",
    "                    (dolphot_peaks['gal_name']==gal_name)] \n",
    "\n",
    "dolphot_augmented = Table.read(basedir/'data'/'external'/'MCI'/'ngc4535_uvis_f555w_exp_drc_augmenteddolcat.fits')\n",
    "dolphot_augmented.rename_column('dolphot_id','Dolphot_ID')\n",
    "dolphot_augmented = dolphot_augmented[~dolphot_augmented['Dolphot_ID'].mask]\n",
    "tmpIC5332 = join(tmp,dolphot_augmented,keys='Dolphot_ID')\n",
    "\n",
    "gal_name = 'NGC1433'\n",
    "tmp = dolphot_peaks[(dolphot_peaks['detection']>=2) & \n",
    "                    (dolphot_peaks['Mag_WFC3_F275W']<25) & \n",
    "                    (dolphot_peaks['S2N_WFC3_F275W']>S2N) &\n",
    "                    (dolphot_peaks['gal_name']==gal_name)] \n",
    "\n",
    "dolphot_augmented = Table.read(basedir/'data'/'external'/'MCI'/'ic5332_augmented_dolphot_cat_pre_candidates_v1_1.fits')\n",
    "dolphot_augmented.rename_column('dolphot_id','Dolphot_ID')\n",
    "dolphot_augmented = dolphot_augmented[~dolphot_augmented['Dolphot_ID'].mask]\n",
    "tmpNGC1433 = join(tmp,dolphot_augmented,keys='Dolphot_ID')\n",
    "\n",
    "gal_name = 'NGC1672'\n",
    "tmp = dolphot_peaks[(dolphot_peaks['detection']>=2) & \n",
    "                    (dolphot_peaks['Mag_WFC3_F275W']<25) & \n",
    "                    (dolphot_peaks['S2N_WFC3_F275W']>S2N) &\n",
    "                    (dolphot_peaks['gal_name']==gal_name)] \n",
    "\n",
    "dolphot_augmented = Table.read(basedir/'data'/'external'/'MCI'/'ngc1672_uvis_f555w_exp_drc_augmenteddolcat_v1p1.fits')\n",
    "dolphot_augmented.rename_column('dolphot_id','Dolphot_ID')\n",
    "dolphot_augmented = dolphot_augmented[~dolphot_augmented['Dolphot_ID'].mask]\n",
    "tmpNGC1672 = join(tmp,dolphot_augmented,keys='Dolphot_ID')\n",
    "\n",
    "gal_name = 'NGC4535'\n",
    "tmp = dolphot_peaks[(dolphot_peaks['detection']>=2) & \n",
    "                    (dolphot_peaks['Mag_WFC3_F275W']<25) & \n",
    "                    (dolphot_peaks['S2N_WFC3_F275W']>S2N) &\n",
    "                    (dolphot_peaks['gal_name']==gal_name)] \n",
    "\n",
    "dolphot_augmented = Table.read(basedir/'data'/'external'/'MCI'/'ngc4535_uvis_f555w_exp_drc_augmenteddolcat.fits')\n",
    "dolphot_augmented.rename_column('dolphot_id','Dolphot_ID')\n",
    "dolphot_augmented = dolphot_augmented[~dolphot_augmented['Dolphot_ID'].mask]\n",
    "tmpNGC4535 = join(tmp,dolphot_augmented,keys='Dolphot_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[-0.3283853837400431,0.00882538879757222,0.10767480085977987,0.2926033632570486,0.04397521810595495,-0.3283853837400431]\n",
    "Y=[-1.1139840687824003,-1.1377544569477822,-0.7350486787204453,0.5399544822354279,0.5598052851182196,-1.1139840687824003]\n",
    "cmap = plt.cm.get_cmap('viridis',8)\n",
    "vmin,vmax=22,26\n",
    "\n",
    "fig,((ax1,ax2),(ax3,ax4))=plt.subplots(figsize=(two_column,two_column),ncols=2,nrows=2)\n",
    "\n",
    "s = tmpIC5332[tmpIC5332['S2N_WFC3_F555W']<10]\n",
    "ax1.scatter(s['mci_in'],s['mci_out'],label=r'S/N$_\\mathrm{V}<10$',color='0.8',s=1.5,rasterized=True)\n",
    "s = tmpIC5332[tmpIC5332['S2N_WFC3_F555W']>=10]\n",
    "sc=ax1.scatter(s['mci_in'],s['mci_out'],c=s['Mag_WFC3_F555W'],vmin=vmin,vmax=vmax,cmap=cmap,\n",
    "            label=r'S/N$_\\mathrm{V}>10$',s=1.5,rasterized=True)\n",
    "ax1.legend()\n",
    "ax1.set_title('IC5332')\n",
    "\n",
    "s = tmpNGC1433[tmpNGC1433['S2N_WFC3_F555W']<10]\n",
    "ax2.scatter(s['mci_in'],s['mci_out'],label=r'S/N$<10$',color='0.8',s=1.5,rasterized=True)\n",
    "s = tmpNGC1433[tmpNGC1433['S2N_WFC3_F555W']>=10]\n",
    "ax2.scatter(s['mci_in'],s['mci_out'],c=s['Mag_WFC3_F555W'],vmin=vmin,vmax=vmax,cmap=cmap,\n",
    "            label=r'S/N$_\\mathrm{V}>10$',s=1.5,rasterized=True)\n",
    "ax2.set_title('NGC1433')\n",
    "\n",
    "s = tmpNGC1672[tmpNGC1672['S2N_WFC3_F555W']<10]\n",
    "ax3.scatter(s['mci_in'],s['mci_out'],label=r'S/N$<10$',color='0.8',s=1.5,rasterized=True)\n",
    "s = tmpNGC1672[tmpNGC1672['S2N_WFC3_F555W']>=10]\n",
    "ax3.scatter(s['mci_in'],s['mci_out'],c=s['Mag_WFC3_F555W'],vmin=vmin,vmax=vmax,cmap=cmap,\n",
    "            label=r'S/N$>10$',s=1.5,rasterized=True)\n",
    "ax3.set_title('NGC1672')\n",
    "\n",
    "s = tmpNGC4535[tmpNGC4535['S2N_WFC3_F555W']<10]\n",
    "ax4.scatter(s['mci_in'],s['mci_out'],label=r'S/N$<10$',color='0.8',s=1.5,rasterized=True)\n",
    "s = tmpNGC4535[tmpNGC4535['S2N_WFC3_F555W']>=10]\n",
    "ax4.scatter(s['mci_in'],s['mci_out'],c=s['Mag_WFC3_F555W'],vmin=vmin,vmax=vmax,cmap=cmap,\n",
    "            label=r'S/N$>10$',s=1.5,rasterized=True)\n",
    "ax4.set_title('NGC4535')\n",
    "\n",
    "for ax in (ax1,ax2,ax3,ax4):\n",
    "    ax.plot(X,Y,color='black')\n",
    "    ax.set(xlim=[-0.6,0.5],ylim=[-2,1],xlabel=r'MCI$_\\mathrm{in}$',ylabel=r'MCI$_\\mathrm{in}$')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(sc,cax=cbar_ax,label=r'$m_V$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'concentration_index.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGC0628 consists of two pointings. Here we merge the two catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_name = 'NGC0628c'\n",
    "with fits.open(data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits') as hdul:\n",
    "    compact_clusters1 = Table(hdul[1].data)\n",
    "gal_name = 'NGC0628e'\n",
    "with fits.open(data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits') as hdul:\n",
    "    compact_clusters2 = Table(hdul[1].data)\n",
    "    \n",
    "compact_clusters2['ID_PHANGS_CLUSTERS'][np.isin(compact_clusters2['ID_PHANGS_CLUSTERS'],compact_clusters1['ID_PHANGS_CLUSTERS'])] += np.max(compact_clusters1['ID_PHANGS_CLUSTERS'])\n",
    "\n",
    "merged = vstack([compact_clusters1,compact_clusters2])\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(merged)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto(data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_ngc0628_phangs-hst_v1p1_ml_class12.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_name = 'NGC0628c'\n",
    "with fits.open(data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_human_class12.fits') as hdul:\n",
    "    compact_clusters1 = Table(hdul[1].data)\n",
    "gal_name = 'NGC0628e'\n",
    "with fits.open(data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_human_class12.fits') as hdul:\n",
    "    compact_clusters2 = Table(hdul[1].data)\n",
    "    \n",
    "compact_clusters2['ID_PHANGS_CLUSTERS'][np.isin(compact_clusters2['ID_PHANGS_CLUSTERS'],compact_clusters1['ID_PHANGS_CLUSTERS'])] += np.max(compact_clusters1['ID_PHANGS_CLUSTERS'])\n",
    "\n",
    "merged = vstack([compact_clusters1,compact_clusters2])\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(merged)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto(data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_ngc0628_phangs-hst_v1p1_human_class12.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models of single stars\n",
    "\n",
    "we can use the stellar atmosphere models from cloudy\n",
    "https://gitlab.nublado.org/cloudy/cloudy/-/wikis/StellarAtmospheres\n",
    "\n",
    "they use the following models\n",
    "http://tlusty.oca.eu/Tlusty2002/tlusty-frames-OS02.html\n",
    "\n",
    "see Figure 1 to estimate radius based on log g\n",
    "https://ui.adsabs.harvard.edu/abs/2003ApJS..146..417L/abstract\n",
    "\n",
    "Example: G35000g400v10. The first letter indicates the composition (G=solar), followed by the effective temperature, the gravity and the turbulent velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speclite.filters\n",
    "\n",
    "filter_path = basedir/'data'/'external'/'UVIS'\n",
    "\n",
    "wlen,response=np.loadtxt(filter_path/'wfc3_uvis1_f275w.txt',delimiter=' ').T\n",
    "uvis_f275w = speclite.filters.FilterResponse(\n",
    "    wavelength=wlen*u.Angstrom,response = response, \n",
    "    meta=dict(group_name='uvis', band_name='f275w'))\n",
    "\n",
    "wlen,response=np.loadtxt(filter_path/'wfc3_uvis1_f336w.txt',delimiter=' ').T\n",
    "uvis_f336w = speclite.filters.FilterResponse(\n",
    "    wavelength=wlen*u.Angstrom,response = response, \n",
    "    meta=dict(group_name='uvis', band_name='f336w'))\n",
    "\n",
    "wlen,response=np.loadtxt(filter_path/'wfc3_uvis1_f438w.txt',delimiter=' ').T\n",
    "uvis_f438w = speclite.filters.FilterResponse(\n",
    "    wavelength=wlen*u.Angstrom,response = response, \n",
    "    meta=dict(group_name='uvis', band_name='f438w'))\n",
    "\n",
    "wlen,response=np.loadtxt(filter_path/'wfc3_uvis1_f555w.txt',delimiter=' ').T\n",
    "uvis_f555w = speclite.filters.FilterResponse(\n",
    "    wavelength=wlen*u.Angstrom,response = response, \n",
    "    meta=dict(group_name='uvis', band_name='f555w'))\n",
    "\n",
    "wlen,response=np.loadtxt(filter_path/'wfc3_uvis1_f814w.txt',delimiter=' ').T\n",
    "uvis_f814w = speclite.filters.FilterResponse(\n",
    "    wavelength=wlen*u.Angstrom,response = response, \n",
    "    meta=dict(group_name='uvis', band_name='f814w'))\n",
    "\n",
    "uvis = speclite.filters.load_filters('uvis-f275w', 'uvis-f336w','uvis-f438w','uvis-f555w','uvis-f814w')\n",
    "#speclite.filters.plot_filters(uvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "mass = 120*u.Msun\n",
    "logg = 300\n",
    "temp = 30000\n",
    "\n",
    "model = f'G{temp}g{logg}v10'\n",
    "result = re.search('([A-Z])(\\d+)g(\\d+)',model)\n",
    "Z,temp,logg = result.groups()\n",
    "g = 10**(float(logg)/100) * u.cm/u.s**2\n",
    "radius = np.sqrt(c.G*mass / g).to(u.Rsun)\n",
    "\n",
    "OBstars = ascii.read(basedir/'data'/'external'/'OBstars'/f'{model}.vis.17',\n",
    "                     data_start=0,names=['wavelength','flux'])\n",
    "wlen = OBstars['wavelength'] * u.Angstrom\n",
    "flux = OBstars['flux'] * u.erg / (u.cm**2 * u.s * u.Angstrom)\n",
    "flux, wlen = uvis.pad_spectrum(flux, wlen)\n",
    "\n",
    "#uvis.get_ab_magnitudes(flux*(radius/u.pc)**2,wlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "mass = np.array([15,120]) * u.Msun\n",
    "logg = [400,300]\n",
    "temp = [55000,30000]\n",
    "\n",
    "\n",
    "def AB_mag_from_spec(mass,logg,temp,mu):\n",
    "    \n",
    "    # https://www.astronomy.ohio-state.edu/martini.10/usefuldata.html\n",
    "    AB_to_vega = {\n",
    "        'bessell-U' : 0.79,\n",
    "        'bessell-B' : -0.09,\n",
    "        'bessell-V' : 0.02\n",
    "    }\n",
    "    \n",
    "    model = f'G{temp}g{logg}v10'\n",
    "    result = re.search('([A-Z])(\\d+)g(\\d+)',model)\n",
    "    Z,temp,logg = result.groups()\n",
    "    g = 10**(float(logg)/100) * u.cm/u.s**2\n",
    "    radius = np.sqrt(c.G*mass / g).to(u.Rsun)\n",
    "\n",
    "    OBstars = ascii.read(basedir/'data'/'external'/'OBstars'/f'{model}.vis.17',\n",
    "                         data_start=0,names=['wavelength','flux'])\n",
    "    wlen = OBstars['wavelength'] * u.Angstrom\n",
    "    flux = OBstars['flux'] * u.erg / (u.cm**2 * u.s * u.Angstrom)\n",
    "    flux, wlen = uvis.pad_spectrum(flux, wlen)\n",
    "\n",
    "    mags = uvis.get_ab_magnitudes(flux*(radius/u.pc)**2,wlen)\n",
    "    print(f\"M={mass:>5}, R={radius:>5.1f}, T={temp} K, g={g:>8.2g}: M_U={mags['uvis-f336w'][0]+ AB_to_vega['bessell-U']:>5.1f}, m_U={mags['uvis-f336w'][0]+mu+AB_to_vega['bessell-U']:.1f}\")\n",
    "    \n",
    "mu =30\n",
    "AB_mag_from_spec(120*u.Msun,400,55000,mu)\n",
    "AB_mag_from_spec(120*u.Msun,300,30000,mu)\n",
    "AB_mag_from_spec(40*u.Msun,400,40000,mu)\n",
    "AB_mag_from_spec(40*u.Msun,300,27500,mu)\n",
    "AB_mag_from_spec(20*u.Msun,450,35000,mu)\n",
    "AB_mag_from_spec(20*u.Msun,350,27500,mu)\n",
    "AB_mag_from_spec(15*u.Msun,450,27500,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'G50000g400v10'\n",
    "OBstars = ascii.read(basedir/'data'/'external'/'OBstars'/f'{model}.vis.7',\n",
    "                     data_start=0,names=['wavelength','flux'])\n",
    "    \n",
    "fig,ax=plt.subplots(figsize=(8,3))\n",
    "ax.plot(OBstars['wavelength'],OBstars['flux'])\n",
    "\n",
    "ax.set(xscale='log',yscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the calibration from Martins+2015 we can estimate the HII region that a single star could ionise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_stars_1 = ascii.read(basedir/'data'/'external'/'martins+2015'/'table1_OV_theo.tex',format='latex')\n",
    "O_stars_1['LHa'] = 10**O_stars_1['Q0'] / 7.31e11\n",
    "\n",
    "O_stars_2 = ascii.read(basedir/'data'/'external'/'martins+2015'/'table2_Oiii_theo.tex',format='latex')\n",
    "O_stars_2['LHa'] = 10**O_stars_2['Q0'] / 7.31e11\n",
    "\n",
    "O_stars_3 = ascii.read(basedir/'data'/'external'/'martins+2015'/'table3_Oi_theo.tex',format='latex')\n",
    "O_stars_3['LHa'] = 10**O_stars_3['Q0'] / 7.31e11\n",
    "\n",
    "O_stars_4 = ascii.read(basedir/'data'/'external'/'martins+2015'/'table4_OV_obs.tex',format='latex')\n",
    "O_stars_4['LHa'] = 10**O_stars_4['Q0'] / 7.31e11\n",
    "\n",
    "O_stars_5 = ascii.read(basedir/'data'/'external'/'martins+2015'/'table5_Oiii_obs.tex',format='latex')\n",
    "O_stars_5['LHa'] = 10**O_stars_5['Q0'] / 7.31e11\n",
    "\n",
    "O_stars_6 = ascii.read(basedir/'data'/'external'/'martins+2015'/'table6_Oi_obs.tex',format='latex')\n",
    "O_stars_6['LHa'] = 10**O_stars_6['Q0'] / 7.31e11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2),(ax3,ax4))=plt.subplots(ncols=2,nrows=2,figsize=(two_column,two_column))\n",
    "\n",
    "ax1.plot(O_stars_1['mass'],O_stars_1['LHa'],label='V')\n",
    "ax1.plot(O_stars_2['mass'],O_stars_2['LHa'],label='III')\n",
    "ax1.plot(O_stars_3['mass'],O_stars_3['LHa'],label='I')\n",
    "ax1.set(xlabel=r'Mass / M$_\\odot$',ylabel=r'L(H$\\alpha$) / erg s$^{-1}$',\n",
    "        xlim=[15,60],ylim=[4e35,1e38],yscale='log')\n",
    "ax1.set_title(r'theoretical $T_\\mathrm{eff}$')\n",
    "\n",
    "ax2.plot(O_stars_4['mass'],O_stars_4['LHa'],label='V')\n",
    "ax2.plot(O_stars_5['mass'],O_stars_5['LHa'],label='III')\n",
    "ax2.plot(O_stars_6['mass'],O_stars_6['LHa'],label='I')\n",
    "ax2.set(xlabel=r'Mass / M$_\\odot$',ylabel=r'L(H$\\alpha$) / erg s$^{-1}$',\n",
    "        xlim=[15,60],ylim=[4e35,1e38],yscale='log')\n",
    "ax2.set_title(r'observational $T_\\mathrm{eff}$')\n",
    "ax2.legend(loc=4)\n",
    "\n",
    "ax3.plot(O_stars_1['mass'],O_stars_1['Mv'],label='V')\n",
    "ax3.plot(O_stars_2['mass'],O_stars_2['Mv'],label='III')\n",
    "ax3.plot(O_stars_3['mass'],O_stars_3['Mv'],label='I')\n",
    "ax3.set(xlabel=r'Mass / M$_\\odot$',ylabel=r'$M_V$',\n",
    "        xlim=[15,60],ylim=[-7,-3.5],yscale='linear')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "ax4.plot(O_stars_4['mass'],O_stars_4['Mv'],label='V')\n",
    "ax4.plot(O_stars_5['mass'],O_stars_5['Mv'],label='III')\n",
    "ax4.plot(O_stars_6['mass'],O_stars_6['Mv'],label='I')\n",
    "ax4.set(xlabel=r'Mass / M$_\\odot$',ylabel=r'$M_V$',\n",
    "        xlim=[15,60],ylim=[-7,-3.5],yscale='linear')\n",
    "ax4.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[-0.33 , -1.662],[-0.307, -1.577],[-0.25 , -1.468],[-0.218, -1.394],[-0.186, -1.321],\n",
    "                   [-0.138, -1.235],[-0.076, -1.213],[-0.007, -1.206],[ 0.065, -1.197],[ 0.135, -1.187],\n",
    "                   [ 0.203, -1.198],[ 0.276, -1.201],[ 0.347, -1.209],[ 0.418, -1.219],[ 0.488, -1.233],\n",
    "                   [ 0.559, -1.251],[ 0.63 , -1.27 ],[ 0.706, -1.264],[ 0.771, -1.287],[ 0.648, -1.119],\n",
    "                   [ 0.626, -1.026],[ 0.565, -0.989],[ 0.559, -0.87 ],[ 0.502, -0.807],[ 0.481, -0.706],\n",
    "                   [ 0.457, -0.546],[ 0.505, -0.449],[ 0.487, -0.384],[ 0.488, -0.328],[ 0.468, -0.239],\n",
    "                   [ 0.517, -0.109],[ 0.555, -0.015],[ 0.6  ,  0.041],[ 0.65 ,  0.132],[ 0.724,  0.174],\n",
    "                   [ 0.864,  0.169],[ 0.948,  0.166],[ 1.02 ,  0.152],[ 1.138,  0.182],[ 1.203,  0.194],\n",
    "                   [ 1.253,  0.282],[ 1.331,  0.378],[ 1.354,  0.469]])\n",
    "\n",
    "x,y=points.T\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "U = single_peaks['Mag_WFC3_F336W']\n",
    "B = single_peaks['Mag_WFC3_F438W']\n",
    "V = single_peaks['Mag_WFC3_F555W']\n",
    "I = single_peaks['Mag_WFC3_F814W']\n",
    "\n",
    "ax.scatter(V-I,U-B)\n",
    "ax.plot(x,y,color='black')\n",
    "ax.set(xlim=[-1,2],ylim=[-2.5,1],\n",
    "      ylabel=r'F336W$-$F438W',xlabel=r'F555W$-$F814W')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=1,figsize=(single_column,single_column/1.))\n",
    "\n",
    "ax.scatter(single_peaks['MV'],single_peaks['HA6562_LUM_CORR'],color='0.8',s=0.6,zorder=0,rasterized=True)\n",
    "\n",
    "ax.plot(O_stars_1['Mv'],O_stars_1['LHa'],label='class V stars',color=tab10[0])\n",
    "ax.plot(O_stars_2['Mv'],O_stars_2['LHa'],label='class III stars',color=tab10[1])\n",
    "ax.plot(O_stars_3['Mv'],O_stars_3['LHa'],label='class I stars',color=tab10[2])\n",
    "ax.plot(O_stars_4['Mv'],O_stars_4['LHa'],ls='--',color=tab10[0])\n",
    "ax.plot(O_stars_5['Mv'],O_stars_5['LHa'],ls='--',color=tab10[1])\n",
    "ax.plot(O_stars_6['Mv'],O_stars_6['LHa'],ls='--',color=tab10[2])\n",
    "\n",
    "t = ax.text(-5.79,5.9e+37,r'60 M$_\\odot$',color='black',fontsize=7,ha='right',va='bottom')\n",
    "t = ax.text(-3.9,4.2e+35,r'16 M$_\\odot$',color='black',fontsize=7,ha='center',va='top')\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set(xlabel='$M_V$ / mag',ylabel=r'$L(\\mathrm{H}\\alpha)$ / erg s$^{-1}$',\n",
    "       yscale='log',ylim=[1e35,1e39],xlim=[-9,-3])\n",
    "ax.invert_xaxis()\n",
    "\n",
    "axin = ax.inset_axes([0.7,0.12,0.25,1.*0.25])\n",
    "\n",
    "axin.scatter(V-I,U-B,marker='p',s=0.5,ec=None,c='0.8',zorder=0,rasterized=True)\n",
    "axin.plot(x,y,color='black',lw=0.4)\n",
    "axin.set(xlim=[-1.5,1.5],ylim=[-2.5,0.9])\n",
    "axin.set_xlabel(r'$V-I$',fontsize=5)\n",
    "axin.set_ylabel(r'$U-B$',fontsize=6)\n",
    "axin.set_xticks([-1,0,1])\n",
    "axin.tick_params(axis='both', labelsize=5)\n",
    "axin.invert_yaxis()\n",
    "\n",
    "plt.savefig(basedir/'reports'/'dolphot_peaks_stellar_models_new.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.hist(single_peaks['HA6562_LUM_CORR'],np.logspace(35,40,20))\n",
    "ax1.set(xlabel=r'H$\\alpha$ / erg s$^{-1}$',xscale='log',xlim=[1e35,1e40])\n",
    "\n",
    "ax2.hist(single_peaks['MV'],bins=np.linspace(-8,-3,10))\n",
    "ax2.set(xlabel='$M_V$',xlim=[-3,-8])\n",
    "ax2.invert_xaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model single stars\n",
    "\n",
    "https://beast.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use compact clusters for age trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(compact_clusters)} peaks in initial catalogue')\n",
    "lst = []\n",
    "for gal_name in sample_table['name']:\n",
    "    tmp = compact_clusters[compact_clusters['gal_name']==gal_name]\n",
    "    ids, counts = np.unique(tmp['region_ID'],return_counts=True)\n",
    "    ids = ids[counts==1]\n",
    "    tmp = tmp[np.isin(tmp['region_ID'],ids)]\n",
    "    HII_sub = nebulae[(nebulae['gal_name']==gal_name)]\n",
    "    del HII_sub['SkyCoord_neb']\n",
    "    if len(tmp)>0:\n",
    "        tmp = join(tmp,HII_sub,keys=['gal_name','region_ID'])\n",
    "        lst.append(tmp)\n",
    "\n",
    "compact_clusters_1to1 = vstack(lst)\n",
    "compact_clusters_1to1['SkyCoord'] = SkyCoord(compact_clusters_1to1['PHANGS_RA']*u.deg,compact_clusters_1to1['PHANGS_DEC']*u.deg)\n",
    "compact_clusters_1to1.rename_columns(['PHANGS_AGE_MINCHISQ','PHANGS_MASS_MINCHISQ'],['age','mass'])\n",
    "print(f'{len(compact_clusters_1to1)} single peaks in final catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot.corner import corner_binned_stat, corner_density_scatter,\\\n",
    "                                corner_binned_percentile, corner_spearmanr\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "    \n",
    "nbins = 8\n",
    "criteria  = (compact_clusters_1to1['mass']>1e4) \n",
    "criteria &= (compact_clusters_1to1['age']<=8) \n",
    "#criteria &= (compact_clusters_1to1['overlap_neb']==0) \n",
    "\n",
    "tmp = compact_clusters_1to1[criteria].copy()\n",
    "tmp['HA/FUV_corr'][tmp['FUV_FLUX_CORR']/tmp['FUV_FLUX_CORR_ERR']<3] = np.nan\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "columns  = ['age','EW_HA','HA/FUV_corr','logq_D91']\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(nrows=3,figsize=(single_column,single_column*1.5),sharex=True)\n",
    "\n",
    "for col,ax in zip(['EW_HA','HA/FUV_corr','logq_D91'],[ax1,ax2,ax3]):\n",
    "    ax.set(ylim=limits[col])\n",
    "    x,y = tmp['age'],tmp[col]\n",
    "    x,y = x[np.isfinite(x) & np.isfinite(y)], y[np.isfinite(x) & np.isfinite(y)]\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=limits['age'],n=68,color='gray',alpha=0.6)\n",
    "    corner_binned_percentile(x,y,ax,nbins=nbins,range=limits['age'],n=98,color='gray',alpha=0.2)\n",
    "\n",
    "    mean, edges, _ = binned_statistic(x,y,statistic='median',bins=nbins,range=limits['age'])\n",
    "    xp = (edges[1:]+edges[:-1])/2\n",
    "    ax.errorbar(xp,mean,fmt='o-',ms=2,color='0.3')\n",
    "    \n",
    "    corner_density_scatter(x,y,ax,nbins=20,s=0.5,cmap=plt.cm.Reds)\n",
    "    corner_spearmanr(x,y,ax,position=(0.93,0.93),pvalue=False,fontsize=7)\n",
    "    ax.set(ylabel=labels[col])\n",
    "ax3.set(xlabel='age / Myr')\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = basedir/'reports'/f'age_tracers_clusters.pdf'\n",
    "#plt.savefig(filename,dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "bins=np.arange(0.5,11.5,1)\n",
    "\n",
    "ax.hist(compact_clusters_1to1['age'],bins=bins,label='compact clusters',histtype='step')\n",
    "ax.hist(catalogue['age'],bins=bins,label='stellar associations',histtype='step')\n",
    "ax.set(xlabel='age / Myr')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age difference between multiple associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "\n",
    "for name in sample_table['name']:\n",
    "    print(name)\n",
    "    nebulae_sub = nebulae[nebulae['gal_name']==name]\n",
    "    associations_sub = associations[associations['gal_name']==name]\n",
    "    associations_sub.add_index('assoc_ID')\n",
    "\n",
    "    with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_nebulae.yml') as f:\n",
    "        nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "        associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "\n",
    "    multiple = nebulae_sub[nebulae_sub['Nassoc']>1]['region_ID']\n",
    "\n",
    "    for region_ID in multiple:\n",
    "        ages = associations_sub.loc[nebulae_dict[region_ID]]['age']\n",
    "        diff.append(np.max(ages)-np.min(ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match with compact clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(matrix, index, default_value=np.nan):\n",
    "    '''\n",
    "    The `to_pixel` method returns the x,y coordinates. However in the \n",
    "    image they correspond to img[y,x]\n",
    "    '''\n",
    "    result = np.zeros(len(index))+default_value\n",
    "    mask = (index[:,1] < matrix.shape[0]) & (index[:,0] < matrix.shape[1])\n",
    "    mask &= (index[:,1] >= 0) & (index[:,0] >=0)\n",
    "\n",
    "    valid = index[mask]\n",
    "    result[mask] = matrix[valid[:,1], valid[:,0]]\n",
    "    return result\n",
    "\n",
    "\n",
    "compact_clusters_list = []\n",
    "\n",
    "N_neb = 0\n",
    "N_ovl = 0\n",
    "for gal_name in np.unique(sample_table['name']):\n",
    "\n",
    "    # we are only interested in peaks that overlap with isolated HII regions\n",
    "    sub_nebulae = nebulae[(nebulae['gal_name']==gal_name) & (nebulae['in_frame'])]\n",
    "    N_neb += len(sub_nebulae)\n",
    "    \n",
    "    print(f'working on {gal_name}')\n",
    "    \n",
    "    # read the nebula mask\n",
    "    filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{gal_name}*.fits'))\n",
    "    copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "    \n",
    "    # we only need the wcs to convert from pixel to skycoord\n",
    "    filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "    if not filename.is_file():\n",
    "        print(f'no F275W image for {gal_name}')\n",
    "        continue\n",
    "    with fits.open(filename) as hdul:\n",
    "        #F275 = NDData(hdul[0].data,\n",
    "        #              mask=hdul[0].data==0,\n",
    "        #              meta=hdul[0].header,\n",
    "        #              wcs=WCS(hdul[0].header))\n",
    "        wcs_F275=WCS(hdul[0].header)\n",
    "    \n",
    "    # read the compact cluster and DOLPHOT catalogues\n",
    "    filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "    if not filename.is_file():\n",
    "        print(f'no compact clusters for {gal_name}')\n",
    "    else:     \n",
    "        with fits.open(filename) as hdul:\n",
    "            compact_clusters = Table(hdul[1].data)\n",
    "        compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "        compact_clusters.add_index('ID_PHANGS_CLUSTERS')\n",
    "\n",
    "        compact_clusters['region_ID'] = get_value(nebulae_mask.data,np.array(compact_clusters['SkyCoord'].to_pixel(nebulae_mask.wcs)).T.astype(int))\n",
    "\n",
    "        # we save all objects that fall inside one of our empty HII regions\n",
    "        compact_clusters.add_column(gal_name,index=0,name='gal_name')\n",
    "        del compact_clusters['SkyCoord']\n",
    "        compact_clusters_list.append(compact_clusters[np.isin(compact_clusters[\"region_ID\"],sub_nebulae[\"region_ID\"])])\n",
    "    N_ovl += np.sum(np.isin(sub_nebulae['region_ID'],compact_clusters['region_ID']))\n",
    "    \n",
    "    \n",
    "compact_clusters = vstack(compact_clusters_list)\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(compact_clusters)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "#hdul.writeto(basedir/'data'/'interim'/f'compact_clusters_in_isolated_HIIregions.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ovl / N_neb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many matches are by change?\n",
    "\n",
    "we rotate one of the masks by 90 degree and see how many still overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from cluster.io import read_associations\n",
    "\n",
    "\n",
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 32\n",
    "\n",
    "gal_name = 'NGC1365'\n",
    "\n",
    "\n",
    "# 1 = 90 deg, 2 = 180 deg ...\n",
    "print(9*' '+'Nneb ovlp Nasc cont part 1to1')\n",
    "\n",
    "# these are the HST images with a single pointing\n",
    "#for gal_name in ['IC5332','NGC1087','NGC1365','NGC1385','NGC1433','NGC1566','NGC2835','NGC4303','NGC4321','NGC4535','NGC7496']\n",
    "for gal_name in ['NGC4535','NGC7496']:\n",
    "    \n",
    "    print(f'{gal_name}')\n",
    "    filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{gal_name}*.fits'))\n",
    "    copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "\n",
    "    # filter image with uncertainties\n",
    "    filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        F275 = NDData(hdul[0].data,\n",
    "                      mask=hdul[0].data==0,\n",
    "                      meta=hdul[0].header,\n",
    "                      wcs=WCS(hdul[0].header))\n",
    "\n",
    "    rot_img = np.rot90(F275.data,k=1)\n",
    "    if np.sum((F275.data!=0) ^ (rot_img!=0)) / np.sum(F275.data!=0) > 0.1:\n",
    "        print(np.sum((F275.data!=0) ^ (rot_img!=0)) / np.sum(F275.data!=0))\n",
    "        print('warning: rotated image does not overlap')\n",
    "\n",
    "    associations, associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',target=gal_name.lower(),\n",
    "                                                        HSTband=HSTband,scalepc=scalepc,version=version)\n",
    "\n",
    "    nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                       output_projection=associations_mask.wcs,\n",
    "                                       shape_out=associations_mask.data.shape,\n",
    "                                       order='nearest-neighbor') \n",
    "    # we need this to determine which \n",
    "    _ , footprint  = reproject_interp(Halpha,\n",
    "                                       output_projection=associations_mask.wcs,\n",
    "                                       shape_out=associations_mask.data.shape,\n",
    "                                       order='nearest-neighbor') \n",
    "\n",
    "    for k in [0,1]:\n",
    "\n",
    "        region_ID = np.unique(nebulae_hst[~np.isnan(nebulae_hst)])\n",
    "        #print(f'neb total: {len(region_ID)}')\n",
    "\n",
    "        not_in_frame_region_ID = np.rot90(nebulae_hst,k=k)[F275.data==k]\n",
    "        not_in_frame_region_ID = np.unique(not_in_frame_region_ID[~np.isnan(not_in_frame_region_ID)])\n",
    "        #print(f'not in frame: {len(not_in_frame_region_ID)}')\n",
    "\n",
    "        in_frame_region_ID = np.setdiff1d(region_ID,not_in_frame_region_ID)\n",
    "        #print(f'in frame: {len(in_frame_region_ID)}')\n",
    "\n",
    "        assoc_ID = np.unique(associations_mask.data[~np.isnan(associations_mask.data)])\n",
    "        #print(f'assoc total: {len(assoc_ID)}')\n",
    "\n",
    "        not_in_frame_assoc_ID = associations_mask.data[~np.rot90(footprint.astype(bool),k=k)]\n",
    "        not_in_frame_assoc_ID = np.unique(not_in_frame_assoc_ID[~np.isnan(not_in_frame_assoc_ID)])\n",
    "        #print(f'not in frame: {len(not_in_frame_assoc_ID)}')\n",
    "\n",
    "        in_frame_assoc_ID = np.setdiff1d(assoc_ID,not_in_frame_assoc_ID)\n",
    "        #print(f'in frame: {len(in_frame_assoc_ID)}')\n",
    "\n",
    "        # we scale the associations such that the the id is in the decimal\n",
    "        scale = 10**np.ceil(np.log10(max(associations_mask.data[~np.isnan(associations_mask.data)])))\n",
    "        s_arr = associations_mask.data/scale+np.rot90(nebulae_hst,k=k)\n",
    "\n",
    "        # ids of associations, nebulae and combination (sum) of both\n",
    "        a_id = np.unique(associations_mask.data[~np.isnan(associations_mask.data)]).astype(int)\n",
    "        n_id = np.unique(nebulae_mask.data[~np.isnan(nebulae_mask.data)]).astype(int)\n",
    "        s_id = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "        # this splits the sum into two parts (nebulae and associations)\n",
    "        a_modf,n_modf = np.modf(s_id)\n",
    "        n_modf = n_modf.astype(int)\n",
    "        a_modf = np.round(a_modf*scale).astype(int)\n",
    "\n",
    "        unique_a, count_a = np.unique(a_modf,return_counts=True)\n",
    "        unique_n, count_n = np.unique(n_modf,return_counts=True)\n",
    "\n",
    "        nebulae_dict = {int(n) : a_modf[n_modf==n].tolist() for n in n_id}     \n",
    "        associations_dict = {int(a) : n_modf[a_modf==a].tolist() for a in a_id}     \n",
    "\n",
    "        # so far we ensured that the nebulae in unique_n have only one association,\n",
    "        # but it is possible that this association goes beyond the nebulae and into\n",
    "        # a second nebulae. Those objects are excluded here\n",
    "        isolated_nebulae = set()\n",
    "        isolated_assoc   = set()\n",
    "        for n,v in nebulae_dict.items():\n",
    "            if len(v)==1:\n",
    "                if len(associations_dict[v[0]])==1:\n",
    "                    isolated_nebulae.add(n)\n",
    "                    isolated_assoc.add(v[0])\n",
    "\n",
    "        # find all assoc that have at least one pixel outside of the nebulae masks\n",
    "        mask = associations_mask.data.copy()\n",
    "        mask[~np.isnan(np.rot90(nebulae_hst,k=k))] = np.nan\n",
    "        outside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "        # find all assoc that have at least one pixel inside of the nebulea masks\n",
    "        mask = associations_mask.data.copy()\n",
    "        mask[np.isnan(np.rot90(nebulae_hst,k=k))] = np.nan\n",
    "        inside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "        contained = np.setdiff1d(inside,outside)\n",
    "        partial   = np.intersect1d(inside,outside)\n",
    "\n",
    "        # we only use the nebulae that fall in the FOV\n",
    "        in_frame_region_ID = np.rot90(nebulae_hst,k=k)[F275.data!=0]\n",
    "        in_frame_region_ID = np.unique(in_frame_region_ID[~np.isnan(in_frame_region_ID)])\n",
    "        n_over = np.sum([len(a_modf[n_modf==n].tolist())>0 for n in n_id])\n",
    "\n",
    "        print(f'{90*k:>3} deg: {len(in_frame_region_ID):>4} {n_over:>4} {len(in_frame_assoc_ID):>4} {len(contained):>4} {len(partial):>4} {len(isolated_nebulae):>4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =  ['IC5332','NGC1087','NGC1365','NGC1385','NGC1433','NGC1566','NGC2835','NGC4303','NGC4535','NGC7496']\n",
    "# without rotation\n",
    "text = '''Nneb ovlp Nasc cont part 1to1\n",
    "777  295  393   45  215  154\n",
    "1006  299  487  280  137  182\n",
    "926  191  489  286   95  112\n",
    "1029  310  525  341  139  133\n",
    "1024  355  494  134  254  250\n",
    "2119  816 1578  707  579  373\n",
    "1039  365  641  232  244  223\n",
    "2696  973 1735  853  584  402\n",
    "1567  385  465  181  207  197\n",
    "743  163  264  185   54   96\n",
    "'''\n",
    "\n",
    "t0 = ascii.read(text)\n",
    "t0.add_column(names,name='gal_name',index=0)\n",
    "\n",
    "t0.add_row(['total',np.sum(t0['Nneb']),np.sum(t0['ovlp']),np.sum(t0['Nasc']),np.sum(t0['cont']),np.sum(t0['part']),np.sum(t0['1to1'])])\n",
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation\n",
    "text = '''Nneb ovlp Nasc cont part 1to1\n",
    "783  144  378    4  131   81\n",
    "1005  139  447   51  126   68\n",
    "908   66  493   25   61   46\n",
    "1029  193  521   98  173   82\n",
    "982   82  438    4   87   57\n",
    "2138  463 1540   86  507  203\n",
    "1046  208  658   39  213  129\n",
    "2691  625 1738  192  616  290\n",
    "1561   86  383    9   74   59\n",
    "722   37  123    6   35   29\n",
    "'''\n",
    "\n",
    "t90 = ascii.read(text)\n",
    "t90.add_column(names,name='gal_name',index=0)\n",
    "\n",
    "t90.add_row(['total',np.sum(t90['Nneb']),np.sum(t90['ovlp']),np.sum(t90['Nasc']),np.sum(t90['cont']),np.sum(t90['part']),np.sum(t90['1to1'])])\n",
    "t90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the result\n",
    "```        Nneb ovlp Nasc cont part 1to1\n",
    "IC5332\n",
    "  0 deg:  777  295  393   45  215  154\n",
    " 90 deg:  783  144  378    4  131   81\n",
    "NGC1087\n",
    "  0 deg: 1006  299  487  280  137  182\n",
    " 90 deg: 1005  139  447   51  126   68\n",
    "NGC1365\n",
    "  0 deg:  926  191  489  286   95  112\n",
    " 90 deg:  908   66  493   25   61   46\n",
    "NGC1385\n",
    "  0 deg: 1029  310  525  341  139  133\n",
    " 90 deg: 1029  193  521   98  173   82\n",
    "NGC1433\n",
    "  0 deg: 1024  355  494  134  254  250\n",
    " 90 deg:  982   82  438    4   87   57\n",
    "NGC1566\n",
    "  0 deg: 2119  816 1578  707  579  373\n",
    " 90 deg: 2138  463 1540   86  507  203\n",
    "NGC2835\n",
    "  0 deg: 1039  365  641  232  244  223\n",
    " 90 deg: 1046  208  658   39  213  129\n",
    "NGC4303\n",
    "  0 deg: 2696  973 1735  853  584  402\n",
    " 90 deg: 2691  625 1738  192  616  290\n",
    "NGC4535\n",
    "  0 deg: 1567  385  465  181  207  197\n",
    " 90 deg: 1561   86  383    9   74   59\n",
    "NGC7496\n",
    "  0 deg:  743  163  264  185   54   96\n",
    " 90 deg:  722   37  123    6   35   29```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import hstack\n",
    "tbl = join(t0,t90,keys='gal_name')\n",
    "\n",
    "tbl['by_chance_part'] = 100*tbl['part_2']/tbl['Nasc_2'] / (tbl['part_1']/tbl['Nasc_1'])\n",
    "tbl['by_chance_cont'] = 100*tbl['cont_2']/tbl['Nasc_2'] / (tbl['cont_1']/tbl['Nasc_1'])\n",
    "\n",
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"contained decrease by {100*(tbl['cont_1'][-1]-tbl['cont_2'][-1])/tbl['cont_1'][-1]:.1f} %\")\n",
    "print(f\"partial decrease by {100*(tbl['part_1'][-1]-tbl['part_2'][-1])/tbl['part_1'][-1]:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mask_total = []\n",
    "A_FOV_total = []\n",
    "\n",
    "for gal_name in tbl['gal_name'][:-1]:\n",
    "    \n",
    "    filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{gal_name}*.fits'))\n",
    "    copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "    filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "        \n",
    "    A_mask = np.sum(~np.isnan(nebulae_mask.data))\n",
    "    A_FOV  = np.sum(~np.isnan(Halpha.data))\n",
    "    print(f'{gal_name}: {A_mask/A_FOV*100:.1f}%')\n",
    "    \n",
    "    A_mask_total.append(A_mask)\n",
    "    A_FOV_total.append(A_FOV)\n",
    "    \n",
    "A_mask_total.append(np.sum(A_mask_total))\n",
    "A_FOV_total.append(np.sum(A_FOV_total))\n",
    "tbl['A_mask'] = A_mask_total \n",
    "tbl['A_FOV'] = A_FOV_total \n",
    "\n",
    "#print(f'Total: {A_mask_total/A_FOV_total*100:.1f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ages of the association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column*2/3))\n",
    "axes = iter(axes.flatten())\n",
    "\n",
    "version = 'v1p2'\n",
    "for HSTband in ['nuv','v']:\n",
    "    for scalepc in [16,32,64]:\n",
    "        folder = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'\n",
    "        lst = []\n",
    "        for file in folder.glob(f'*{scalepc}pc_associations.fits'):\n",
    "            gal_name = file.stem.split('_')[0]\n",
    "            tbl = Table(fits.getdata(file,ext=1))\n",
    "            tbl.add_column(gal_name,name='gal_name',index=0)\n",
    "            lst.append(tbl)\n",
    "        assoc_tmp = vstack(lst)\n",
    "        \n",
    "        with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "            associations = Table(hdul[1].data)\n",
    "        associations = join(associations,assoc_tmp,keys=['gal_name','assoc_ID'])\n",
    "\n",
    "        \n",
    "        ax = next(axes)\n",
    "\n",
    "        tmp=associations[associations['1to1']]\n",
    "        ax.hist(tmp['reg_dolflux_Age_MinChiSq'],bins=np.arange(0,20))\n",
    "        ax.set_title(f'{HSTband}, {scalepc}pc, {np.mean(tmp[\"reg_dolflux_Age_MinChiSq\"]):.1f} Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## BPASS\n",
    "\n",
    "https://flexiblelearning.auckland.ac.nz/bpass/8/files/bpassv2_1_manual.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_bpass(binary='bin',imf='imf135',upper_mass='300',metallicity='z014'):\n",
    "    '''read the output from BPASS\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    binary : str (`bin` or `sin`)\n",
    "        use binarys or single stars\n",
    "        \n",
    "    imf : str\n",
    "        the slope of the IMF (the model)\n",
    "        \n",
    "    upper_mass : str\n",
    "        mass of the most massive stars\n",
    "    \n",
    "    metallicity : str\n",
    "    '''\n",
    "    \n",
    "    filename = basedir/'..'/'BPASS'/f'bpass_v2.2.1_{imf}_{upper_mass}'/binary/f'ionizing-{binary}-{imf}_{upper_mass}.{metallicity}.dat'\n",
    "\n",
    "    if not filename.is_file():\n",
    "        print('file does not exist')\n",
    "        return filename\n",
    "    \n",
    "    units = [u.LogUnit(u.year),u.LogUnit(1/u.s),u.LogUnit(u.erg/u.s),u.LogUnit(u.erg/u.s/u.A),u.LogUnit(u.erg/u.s/u.A)]\n",
    "    names = ['log_age','log_Q','log_Halpha','log_FUV','log_NUV']\n",
    "    bpass = ascii.read(filename,names=names)\n",
    "    bpass = QTable(bpass)\n",
    "\n",
    "    for unit, col in zip(units,bpass.columns):\n",
    "        bpass[col].unit = unit \n",
    "    for col in list(bpass.columns):\n",
    "        if col.startswith('log'):\n",
    "            bpass[col] = bpass[col].physical\n",
    "            bpass.rename_column(col,col[4:])\n",
    "    return bpass\n",
    "\n",
    "bpass = read_bpass(metallicity='z020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(bpass['age'],bpass['Halpha']/bpass['FUV'])\n",
    "ax.set(xlim=[1e6,1e7],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "HI_rate = bpass['Q'].value\n",
    "time = bpass['age']\n",
    "for row in tqdm(catalogue):\n",
    "    idx = np.argmin(np.abs(time-row['age']*u.Myr))\n",
    "    row['Qpredicted'] = HI_rate[idx] * row['mass'] / 1e6\n",
    "    \n",
    "catalogue['distance'] = np.nan\n",
    "for gal_name in catalogue['gal_name']:\n",
    "    distance = Distance(distmod=sample_table.loc[gal_name]['(m-M)'])\n",
    "    catalogue['distance'][catalogue['gal_name']==gal_name] = distance\n",
    "    \n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(catalogue['distance']*u.Mpc)**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for objects with negative fesc we redo the analysis with age-age_err\n",
    "Qpredict_new = []\n",
    "for row in tqdm(catalogue):\n",
    "    if row['fesc']<0:\n",
    "        idx = np.argmin(np.abs(time-(row['age']-3*row['age_err'])*u.Myr))\n",
    "        row['Qpredicted'] = ( HI_rate[idx] * row['mass'] / 1e6 )\n",
    "\n",
    "fesc_classic = (catalogue['Qpredicted']-catalogue['Qobserved'])/catalogue['Qpredicted']\n",
    "catalogue['fesc'] = fesc_classic\n",
    "catalogue['HA/FUV_corr'] = catalogue['HA/FUV'] / (1-fesc_classic)\n",
    "catalogue['eq_width_corr'] = catalogue['eq_width'] / (1-fesc_classic)\n",
    "\n",
    "print(f'fesc={np.nanmean(fesc_classic[fesc_classic>0]):.2f}+-{np.nanstd(fesc_classic[fesc_classic>0]):.2f} (from {np.sum(fesc_classic>0)} objects)')\n",
    "print(f\"{np.sum(fesc_classic<0)} of {len(catalogue)} ({np.sum(fesc_classic<0)/len(catalogue)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "#criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']==1) \n",
    "#criteria &= fesc>0\n",
    "tmp = catalogue[criteria].copy()\n",
    "\n",
    "print(f'fesc={np.nanmean(tmp[tmp[\"fesc\"]>0][\"fesc\"]):.2f} (from {np.sum(criteria)} objects)')\n",
    "print(f\"{np.sum(tmp['fesc']<0)} of {len(tmp)} ({np.sum(tmp['fesc']<0)/len(tmp)*100:.1f}%) regions have negative fesc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Compare different stellar models/population synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bpass_z014 = read_bpass(metallicity='z014')\n",
    "bpass_z008 = read_bpass(metallicity='z008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(nrows=3,figsize=(two_column,two_column),sharex=True)\n",
    "\n",
    "ax1.plot(bpass['age']/1e6,bpass['Halpha'],label='BPASSv014')\n",
    "#ax1.plot(bpass_z008['age']/1e6,bpass_z008['Halpha'],label='BPASSv008')\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A'],label='GENEVAv40')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],ylabel=r'$\\mathrm{H}\\alpha \\,/\\, \\mathrm{erg}\\ \\mathrm{s}^{-1}$',\n",
    "        yscale='log',ylim=[1e39,2e41])\n",
    "\n",
    "ax2.plot(bpass['age']/1e6,bpass['FUV'],label='BPASSv014')\n",
    "#ax2.plot(bpass_z008['age']/1e6,bpass_z008['FUV'],label='BPASSv008')\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.FUV['FUV'],label='GENEVAv40')\n",
    "#ax2.legend()\n",
    "ax2.set(xlim=[0,10],ylabel=r'$\\mathrm{FUV}\\,/\\, \\mathrm{erg}\\ \\mathrm{s}^{-1}\\ \\mathrm{\\AA}^{-1}$',\n",
    "       yscale='log',ylim=[8e37,3e39])\n",
    "\n",
    "ax3.plot(bpass['age']/1e6,bpass['Halpha']/bpass['FUV'],label='BPASSv014')\n",
    "#ax3.plot(bpass_z008['age']/1e6,bpass_z008['Halpha']/bpass_z008['FUV'],label='BPASSv008')\n",
    "ax3.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],label='GENEVAv40')\n",
    "#ax3.legend()\n",
    "ax3.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'$\\mathrm{H}\\alpha/\\mathrm{FUV}$')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sample\n",
    "criteria  = (catalogue['mass']>1e4) \n",
    "criteria &= (catalogue['age']<=10) \n",
    "criteria &= (catalogue['overlap_neb']>0.1) \n",
    "criteria &= (catalogue['overlap_asc']>0.9) \n",
    "tmp = catalogue[criteria].copy()\n",
    "tmp = tmp[['age','mass','EBV_balmer','density','temperature','met_scal','logq_D91','fesc']]\n",
    "\n",
    "x = tmp.to_pandas()\n",
    "# remove all columns with NaN\n",
    "tmp = tmp[np.all(~np.isnan(x),axis=1).values]\n",
    "x = x[np.all(~np.isnan(x),axis=1)]\n",
    "x = StandardScaler().fit_transform(x)\n",
    "#x = x[...,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_data = pd.DataFrame(x,columns=[f'feature{i}' for i in range(x.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_hiiregion = PCA(n_components=2)\n",
    "principalComponents_hiiregion = pca_hiiregion.fit_transform(x)\n",
    "principal_hiiregion_Df = pd.DataFrame(data = principalComponents_hiiregion\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "idx = tmp['age']<3\n",
    "ax.scatter(principal_hiiregion_Df.loc[idx,'principal component 1'],\n",
    "           principal_hiiregion_Df.loc[idx,'principal component 2'],color='tab:blue')\n",
    "\n",
    "idx = tmp['age']>3\n",
    "ax.scatter(principal_hiiregion_Df.loc[idx,'principal component 1'],\n",
    "           principal_hiiregion_Df.loc[idx,'principal component 2'],color='tab:red')\n",
    "\n",
    "ax.set(xlabel='PCA-1',ylabel='PCA-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty Pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot regions (MUSE & HST over WFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "sample = set([x.stem.split('_')[0].upper() for x in (data_ext/'HST'/'white_light').iterdir()])\n",
    "sample = np.unique(sample_table['name'])\n",
    "ncols = 4\n",
    "nrows = int(np.ceil(len(sample)/ncols))\n",
    "\n",
    "width = thesis_width\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows),facecolor='black')\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "ax=next(axes_iter)\n",
    "# plot the PHANGS logo in the top left axis\n",
    "image = mpl.image.imread(str(basedir/'references'/'Logo_white.png'))\n",
    "img = np.sum(image,axis=2)\n",
    "img = np.pad(img,[(120,36),(25,25)],mode='constant')\n",
    "ax.imshow(img,cmap=plt.cm.gray)\n",
    "ax.axis('off')\n",
    "\n",
    "hst_sample = []\n",
    "for name in sorted(sample):\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "    \n",
    "    if filename.is_file():\n",
    "        with fits.open(filename) as hdul:\n",
    "            hst_whitelight = NDData(hdul[0].data,mask=hdul[0].data==0,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "            hst_whitelight.data[hst_whitelight.data==0] = np.nan\n",
    "        hst_sample.append(name)\n",
    "\n",
    "    \n",
    "    filename = data_ext / 'MUSE' / 'DR2.1' / 'MUSEDAP' / f'{name}_MAPS.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "        \n",
    "    filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "\n",
    "        WFI = NDData(data=hdul[0].data,\n",
    "                     meta=hdul[0].header,\n",
    "                     wcs=WCS(hdul[0].header))\n",
    "        \n",
    "\n",
    "    reg_muse_pix, reg_muse_sky = find_sky_region(Halpha.mask.astype(int),wcs=Halpha.wcs)\n",
    "    if name in hst_sample:\n",
    "        reg_hst_pix, reg_hst_sky = find_sky_region(hst_whitelight.mask.astype(int),wcs=hst_whitelight.wcs)\n",
    "    \n",
    "    WFI_cutout = Cutout2D(WFI.data,sample_table.loc[name]['SkyCoord'],size=8*u.arcmin,wcs=WFI.wcs)\n",
    "    \n",
    "    # project from muse to hst coordinates\n",
    "    reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "    if name in hst_sample:\n",
    "        reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "    ax = next(axes_iter)\n",
    "\n",
    "    # plot image\n",
    "    norm = simple_norm(WFI_cutout.data,clip=False,percent=99)\n",
    "    ax.imshow(WFI_cutout.data,norm=norm,cmap=plt.cm.gray,origin='lower')\n",
    "\n",
    "    reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE',lw=0.3)\n",
    "    if name in hst_sample:\n",
    "        reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST',lw=0.3)\n",
    "    t = ax.text(0.05,0.91,name, transform=ax.transAxes,color='white',fontsize=7)\n",
    "    #t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i in range(nrows*ncols-len(sample)-1):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=-0.01,hspace=0.05)\n",
    "plt.savefig(basedir/'reports'/'PHANGS_atlas.jpg',facecolor='white',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(len(muse_sample)/ncols))\n",
    "\n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for name in sorted(muse_sample):\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    catalogue_file = basedir/'..'/'PNLF'/'data'/'catalogues'/f'{name}_nebulae.txt'\n",
    "    catalogue = ascii.read(catalogue_file,format='fixed_width_two_line',delimiter_pad=' ',position_char='=')\n",
    "    catalogue['SkyCoord'] = SkyCoord(catalogue['RaDec'])\n",
    "    catalogue=catalogue[catalogue['type']=='PN']\n",
    "    \n",
    "    filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "\n",
    "        WFI = NDData(data=hdul[0].data,\n",
    "                     meta=hdul[0].header,\n",
    "                     wcs=WCS(hdul[0].header))\n",
    "        \n",
    "    \n",
    "    WFI_cutout = Cutout2D(WFI.data,sample_table.loc[name]['SkyCoord'],size=5*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "    ax = next(axes_iter)\n",
    "\n",
    "    # plot image\n",
    "    norm = simple_norm(WFI_cutout.data,clip=False,percent=99)\n",
    "    ax.imshow(WFI_cutout.data,norm=norm,cmap=plt.cm.gray,origin='lower')\n",
    "    \n",
    "    x,y = catalogue['SkyCoord'].to_pixel(WFI_cutout.wcs)\n",
    "    ax.scatter(x,y,marker='o',ec='tab:red',fc='none',s=1,lw=0.2)\n",
    "    \n",
    "    t = ax.text(0.05,0.91,name, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i in range(nrows*ncols-len(muse_sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=-0.01,hspace=0.05)\n",
    "plt.savefig(basedir/'reports'/'all_objects_PN.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from cluster.regions import find_sky_region\n",
    "\n",
    "\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(len(muse_sample)/ncols))\n",
    "\n",
    "width = 1.5*two_column\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for name in sorted(muse_sample):\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    catalogue = filter_table(nebulae,gal_name=name)\n",
    "    #catalogue['SkyCoord'] = SkyCoord(catalogue['RaDec'])\n",
    "    \n",
    "    filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        WFI = NDData(data=hdul[0].data,\n",
    "                     meta=hdul[0].header,\n",
    "                     wcs=WCS(hdul[0].header))\n",
    "        \n",
    "    \n",
    "    WFI_cutout = Cutout2D(WFI.data,sample_table.loc[name]['SkyCoord'],size=5*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "    ax = next(axes_iter)\n",
    "\n",
    "    # plot image\n",
    "    norm = simple_norm(WFI_cutout.data,clip=False,percent=99)\n",
    "    ax.imshow(WFI_cutout.data,norm=norm,cmap=plt.cm.gray,origin='lower')\n",
    "    \n",
    "    x,y = catalogue['SkyCoord'].to_pixel(WFI_cutout.wcs)\n",
    "    ax.scatter(x,y,marker='o',ec='tab:blue',fc='none',s=1,lw=0.2)\n",
    "    \n",
    "    t = ax.text(0.05,0.91,name, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i in range(nrows*ncols-len(muse_sample)):\n",
    "\n",
    "    # remove the empty axes at the bottom\n",
    "    ax = next(axes_iter)\n",
    "    ax.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=-0.01,hspace=0.05)\n",
    "plt.savefig(basedir/'reports'/'all_objects_nebulae.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of the sky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import Distance\n",
    "\n",
    "sample = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample['SkyCoord'] = SkyCoord(sample['R.A.'],sample['Dec.'])\n",
    "sample['d/Mpc'] = Distance(distmod=sample['(m-M)'])\n",
    "\n",
    "ra = sample['SkyCoord'].ra\n",
    "ra = ra.wrap_at(180*u.degree)\n",
    "dec = sample['SkyCoord'].dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpl.use('pdf')\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111, projection=\"mollweide\")\n",
    "ax.scatter(ra.radian,dec.radian,marker='.')\n",
    "#ax.set_xticklabels(['14h','16h','18h','20h','22h','0h','2h','4h','6h','8h','10h'])\n",
    "ax.grid(False)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "d_max = np.max(sample['d/Mpc'])\n",
    "width0 = 0.3 # in radian\n",
    "for x,y,name,distance in zip(ra,dec,sample['name'],sample['d/Mpc']):\n",
    "            \n",
    "    filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{name}*.fits'))\n",
    "    copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "        \n",
    "    img_width,img_height = Halpha.data.shape\n",
    "    width  = width0 * distance / d_max\n",
    "    height = width/img_width*img_height\n",
    "    bounds = [x.radian-width/2,y.radian-height/2,width,height]\n",
    "\n",
    "    ax_img = ax.inset_axes(bounds,transform=ax.transData)\n",
    "    norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "    ax_img.imshow(Halpha.data,cmap=plt.cm.gray_r,norm=norm,origin='lower')\n",
    "    ax_img.axis('off')\n",
    "    ax_img.set_title(name)\n",
    "        \n",
    "    #ax.annotate(s,(x.radian,y.radian),xycoords='data',size='x-small')\n",
    "\n",
    "fig.savefig(\"sky_map.pdf\",dpi=600)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a LaTeX table for the journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['RA_asc'],catalogue['DEC_asc'] = zip(*[x.split(' ') for x in catalogue['SkyCoord_asc'].to_string(style='hmsdms',precision=2)])\n",
    "catalogue['RA_neb'],catalogue['DEC_neb'] = zip(*[x.split(' ') for x in catalogue['SkyCoord_neb'].to_string(style='hmsdms',precision=2)])\n",
    "catalogue['logmass'] = np.log10(catalogue['mass'])\n",
    "catalogue['logmass_err'] = catalogue['mass_err'] / catalogue['mass']\n",
    "catalogue['dots'] = '$\\hdots$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one large table\n",
    "columns = ['gal_name','region_ID','RA_neb','DEC_neb','[SIII]/[SII]','[SIII]/[SII]_err','Delta_met_scal','met_scal_err','HA/FUV','HA/FUV_err','eq_width','EBV_balmer','EBV_balmer_err','overlap_neb','assoc_ID','RA_asc','DEC_asc','age','age_err','mass','mass_err','EBV_stars','EBV_stars_err','overlap_asc']\n",
    "# create LaTeX table for a single galaxy (for paper)\n",
    "tmp = catalogue[columns]\n",
    "\n",
    "\n",
    "for col in columns:\n",
    "    if not col.startswith('RA') and not col.startswith('DEC') and col!='gal_name':\n",
    "        tmp[col].info.format = '%.2f'\n",
    "\n",
    "ascii.write(tmp[-10:],sys.stdout,Writer=ascii.Latex,overwrite=True,exclude_names=['x','y','fwhm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or two separate tables\n",
    "# for the associations\n",
    "columns = ['gal_name','assoc_ID','RA_asc','DEC_asc','age','age_err','logmass','logmass_err','EBV_stars','EBV_stars_err','overlap_asc','dots']\n",
    "# create LaTeX table for a single galaxy (for paper)\n",
    "tmp = catalogue[columns]\n",
    "\n",
    "\n",
    "for col in columns:\n",
    "    if not col.startswith('RA') and not col.startswith('DEC') and col!='gal_name' and col!='dots':\n",
    "        tmp[col].info.format = '%.2f'\n",
    "tmp['assoc_ID'].info.format = '%.0f'\n",
    "        \n",
    "ascii.write(tmp[:10],sys.stdout,Writer=ascii.Latex,overwrite=True,exclude_names=['x','y','fwhm'])\n",
    "\n",
    "\n",
    "# and now for the HII regions\n",
    "columns = ['dots','region_ID','RA_neb','DEC_neb','[SIII]/[SII]','[SIII]/[SII]_err','Delta_met_scal','met_scal_err','HA/FUV','HA/FUV_err','eq_width','EBV_balmer','EBV_balmer_err','overlap_neb']\n",
    "# create LaTeX table for a single galaxy (for paper)\n",
    "tmp = catalogue[columns]\n",
    "\n",
    "for col in columns:\n",
    "    if not col.startswith('RA') and not col.startswith('DEC') and col!='gal_name' and col!='dots':\n",
    "        tmp[col].info.format = '%.2f'\n",
    "tmp['region_ID'].info.format = '%.0f'\n",
    "\n",
    "ascii.write(tmp[:10],sys.stdout,Writer=ascii.Latex,overwrite=True,exclude_names=['x','y','fwhm'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density & Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSII = nebulae['SII6716_FLUX_CORR'] / nebulae['SII6730_FLUX_CORR']\n",
    "RSII_ERR = RSII * np.sqrt( (nebulae['SII6716_FLUX_CORR_ERR']/nebulae['SII6716_FLUX_CORR'])**2 + (nebulae['SII6730_FLUX_CORR_ERR']/nebulae['SII6730_FLUX_CORR'])**2 )\n",
    "dif = (1.46-RSII)/(RSII_ERR*1.43)\n",
    "print(np.sum(dif>3))\n",
    "\n",
    "#nebulae['density'][dif<3] = np.nan\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(figsize=(two_column,two_column),nrows=2)\n",
    "\n",
    "ax1.hist(RSII,bins=np.linspace(1.1,1.8,40))\n",
    "ax1.axvline(1.46,color='black')\n",
    "ax1.set(yscale='log',xlim=[1.1,1.8],ylim=[10,None],xlabel=r'$R_\\mathrm{[SII]}$')\n",
    "\n",
    "\n",
    "ax2.hist(dif,bins=np.linspace(0,20,40))\n",
    "ax2.axvline(3,color='black')\n",
    "ax2.set(yscale='log',xlim=[0,20],xlabel=r'$(1.46-R_\\mathrm{[SII]}) / \\delta R_\\mathrm{[SII]}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rlim = lambda Te: 1.49-3.94e-6*Te\n",
    "Rlim(12e3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nebulae.copy()\n",
    "tmp['RSII'] = tmp['SII6716_FLUX_CORR'] / tmp['SII6730_FLUX_CORR']\n",
    "tmp['RSII_ERR'] = tmp['RSII'] * np.sqrt( (tmp['SII6716_FLUX_CORR_ERR']/tmp['SII6716_FLUX_CORR'])**2 + (tmp['SII6730_FLUX_CORR_ERR']/tmp['SII6730_FLUX_CORR'])**2 )\n",
    "\n",
    "# make some S/N cuts\n",
    "S2N = 5\n",
    "#mask =  (tmp['NII5754_FLUX_CORR_REFIT']>S2N*tmp['NII5754_FLUX_CORR_REFIT_ERR']) & (tmp['NII6583_FLUX_CORR']>S2N*tmp['NII6583_FLUX_CORR_ERR'])\n",
    "#mask &= (tmp['SII6730_FLUX_CORR']>S2N*tmp['SII6730_FLUX_CORR_ERR']) & (tmp['SII6716_FLUX_CORR']>S2N*tmp['SII6716_FLUX_CORR_ERR'])\n",
    "mask = (1.46-tmp['RSII'])/(tmp['RSII_ERR']*1.43)>3\n",
    "\n",
    "tmp =tmp[mask]\n",
    "print(f'{len(tmp)} objects in sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2/1.618))\n",
    "\n",
    "ax1.scatter(nebulae['EW_HA'],nebulae['SII6716_FLUX_CORR']/nebulae['SII6730_FLUX_CORR'])\n",
    "corner_binned_stat(nebulae['EW_HA'],nebulae['SII6716_FLUX_CORR']/nebulae['SII6730_FLUX_CORR'],ax=ax1)\n",
    "ax1.set(xlabel='EW(Ha)',ylabel='RSII',xlim=[0,450])\n",
    "ax2.scatter(tmp['EW_HA'],densities)\n",
    "corner_binned_stat(tmp['EW_HA'],densities,ax=ax2)\n",
    "ax2.set(xlabel='EW(Ha)',ylabel='density',xlim=[0,450])\n",
    "\n",
    "#ax.set(yscale='log',xlabel='[SII]6716/6730')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2/1.618))\n",
    "\n",
    "ax1.scatter(tmp['EW_HA'],tmp['SII6716_FLUX_CORR']/tmp['SII6730_FLUX_CORR'])\n",
    "corner_binned_stat(tmp['EW_HA'],tmp['SII6716_FLUX_CORR']/tmp['SII6730_FLUX_CORR'],ax=ax1)\n",
    "ax1.set(xlabel='EW(Ha)',ylabel='RSII',xlim=[0,450])\n",
    "ax2.scatter(tmp['EW_HA'],densities)\n",
    "corner_binned_stat(tmp['EW_HA'],densities,ax=ax2)\n",
    "ax2.set(xlabel='EW(Ha)',ylabel='density',xlim=[0,450])\n",
    "\n",
    "#ax.set(yscale='log',xlabel='[SII]6716/6730')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn\n",
    "\n",
    "den = 100\n",
    "tem = 8e3\n",
    "\n",
    "N2 = pn.Atom('N', 2)\n",
    "temperatures = N2.getTemDen(int_ratio=tmp['NII5754_FLUX_CORR_REFIT']/tmp['NII6583_FLUX_CORR'], den=den, wave1=5755, wave2=6584)\n",
    "\n",
    "S2 = pn.Atom('S', 2)\n",
    "densities = S2.getTemDen(int_ratio=tmp['SII6716_FLUX_CORR']/tmp['SII6730_FLUX_CORR'], tem=tem, wave1=6716, wave2=6730)\n",
    "\n",
    "diags = pn.Diagnostics()\n",
    "Te,ne = diags.getCrossTemDen('[NII] 5755/6548', '[SII] 6731/6716', \n",
    "                     tmp['NII5754_FLUX_CORR_REFIT']/tmp['NII6583_FLUX_CORR']/0.34,\n",
    "                     tmp['SII6730_FLUX_CORR']/tmp['SII6716_FLUX_CORR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to nebula catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebulae['density'] = np.nan\n",
    "nebulae['density'][mask] = ne\n",
    "\n",
    "nebulae['temperature'] = np.nan\n",
    "nebulae['temperature'][mask] = Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "ax.hist(temperatures,bins=np.linspace(5.5e3,1.1e4,24),histtype='step',label=r'$\\mathrm{density}=100\\,\\mathrm{cm}^{-1}$')\n",
    "ax.hist(Te,bins=np.linspace(5.5e3,1.1e4,24),histtype='step',label='crossTemDen')\n",
    "\n",
    "#ax.axvline(8e3,color='gray',ls='--')\n",
    "ax.axvline(np.nanmean(Te),color='gray',label='mean')\n",
    "ax.set(xlabel='Temperature / K',ylabel='N')\n",
    "#ax.set_title(f'$T_{{mean}} = {np.nanmean(Te):.1f}$ K')\n",
    "ax.legend()\n",
    "plt.savefig(basedir/'reports'/'temperature_hist.pdf',dpi=400)\n",
    "plt.show()\n",
    "\n",
    "print(f'Tmean={np.nanmean(Te):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn\n",
    "\n",
    "tem = 8e3\n",
    "\n",
    "ratio = np.linspace(0.5,1.5,100)\n",
    "\n",
    "S2 = pn.Atom('S', 2)\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(figsize=(thesis_width,thesis_width/1.5),\n",
    "                           nrows=2,sharex=True,gridspec_kw={'height_ratios':[1.618,1]})\n",
    "\n",
    "ax1.hist(nebulae['SII6716_FLUX_CORR']/nebulae['SII6730_FLUX_CORR'],\n",
    "         bins=np.linspace(0.5,2.5,41),histtype='step',color=tab10[2])\n",
    "ax1.hist(nebulae['SII6716_FLUX_CORR']/nebulae['SII6730_FLUX_CORR'],\n",
    "         bins=np.linspace(0.5,2.5,41),histtype='bar',color=tab10[2],alpha=0.2)\n",
    "\n",
    "ax1.axvline(1.45,color=tab10[0])\n",
    "ax1.set(yscale='log',ylabel=r'$N$',ylim=[2,None])\n",
    "\n",
    "for tem in [6000,12000]:\n",
    "    densities = S2.getTemDen(int_ratio=ratio, tem=tem, wave1=6716, wave2=6731)\n",
    "    ax2.plot(ratio,densities,label=f'{tem} K',color=tab10[1],zorder=0)\n",
    "ax2.axvline(1.45,color=tab10[0])\n",
    "ax2.text(0.12,0.85,'12,000 K',rotation=-15,transform=ax2.transAxes,ha='left',va='top',color=tab10[1])\n",
    "ax2.text(0.15,0.54,'6,000 K',rotation=-15,transform=ax2.transAxes,ha='left',va='top',color=tab10[1])\n",
    "    \n",
    "ax2.arrow(0.475,0.5,0.2,0,transform=ax2.transAxes,head_width=0.08,head_length=0.04,color=tab10[0])\n",
    "ax2.text(0.5,0.55,'low density limit',transform=ax2.transAxes,ha='left',va='bottom',fontsize=10,color=tab10[0])\n",
    "\n",
    "ax2.set(yscale='log',xlabel=r'$F_{[\\mathrm{S}\\,\\tiny{\\textsc{ii}}] \\lambda 6716}\\,/\\,F_{[\\mathrm{S}\\,\\tiny{\\textsc{ii}}] \\lambda 6731}$',ylabel='density / cm$^{-1}$',\n",
    "        xlim=[0.5,2.5],ylim=[20,2e4])\n",
    "ax2.set_yticks([1e2,1e3,1e4])\n",
    "\n",
    "plt.subplots_adjust(hspace=0.02)\n",
    "plt.savefig(basedir/'reports'/'density.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities1 = S2.getTemDen(int_ratio=ratio, tem=6000, wave1=6716, wave2=6731)\n",
    "densities2 = S2.getTemDen(int_ratio=ratio, tem=12000, wave1=6716, wave2=6731)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDen(tmp,temp=1e4,sample_size = 1000):\n",
    "    '''compute Density with uncertainties\n",
    "    \n",
    "\n",
    "    '''\n",
    "    S2 = pn.Atom('S', 2)\n",
    "\n",
    "    SII6730 = np.random.normal(loc=tmp['SII6730_FLUX_CORR'],scale=tmp['SII6730_FLUX_CORR_ERR'],size=(sample_size,len(tmp)))\n",
    "    SII6716 = np.random.normal(loc=tmp['SII6716_FLUX_CORR'],scale=tmp['SII6716_FLUX_CORR_ERR'],size=(sample_size,len(tmp)))\n",
    "\n",
    "    # calculcate for an array of values and use the std as the uncertainty\n",
    "    densities   = S2.getTemDen(int_ratio=SII6716/SII6730, tem=temp, wave1=6717, wave2=6731)\n",
    "    density_err = np.nanstd(densities,axis=0)\n",
    "    \n",
    "    density   = S2.getTemDen(int_ratio=tmp['SII6716_FLUX_CORR']/tmp['SII6730_FLUX_CORR'], \n",
    "                             tem=temp, wave1=6717, wave2=6731)\n",
    "\n",
    "    \n",
    "    return density,density_err\n",
    "\n",
    "density1,density_err1 = getDen(tmp[:10],temp=8e3,sample_size=1000)\n",
    "density2,density_err2 = getDen(tmp[:10],temp=8e3,sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2.getTemDen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = tmp['SII6716_FLUX_CORR']/tmp['SII6730_FLUX_CORR']\n",
    "density_8k = S2.getTemDen(int_ratio=ratios, tem=8e3, wave1=6717, wave2=6731)\n",
    "density_10k = S2.getTemDen(int_ratio=ratios, tem=1e4, wave1=6717, wave2=6731)\n",
    "density_12k = S2.getTemDen(int_ratio=ratios, tem=1.2e4, wave1=6717, wave2=6731)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tmp['SII6716_FLUX_CORR']/tmp['SII6716_FLUX_CORR_ERR'],density/density_err)\n",
    "plt.axvline(100)\n",
    "plt.axhline(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "lim = (0,100)\n",
    "ax.errorbar(density,density,yerr=density_err,xerr=density_err,fmt='o',color='gray',zorder=0)\n",
    "ax.scatter(density_8k,density_10k,label='8k vs 10k')\n",
    "ax.scatter(density_8k,density_12k,label='8k vs 12k')\n",
    "ax.scatter(density_10k,density_12k,label='10k vs 12k')\n",
    "ax.legend()\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.set(xlim=lim,ylim=lim,xlabel='density',ylabel='density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMC sep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "def get_value(matrix, index, default_value=np.nan):\n",
    "    '''\n",
    "    The `to_pixel` method returns the x,y coordinates. However in the \n",
    "    image they correspond to img[y,x]\n",
    "    '''\n",
    "    result = np.zeros(len(index))+default_value\n",
    "    mask = (index[:,1] < matrix.shape[0]) & (index[:,0] < matrix.shape[1])\n",
    "    mask &= (index[:,1] >= 0) & (index[:,0] >=0)\n",
    "\n",
    "    valid = index[mask]\n",
    "    result[mask] = matrix[valid[:,1], valid[:,0]]\n",
    "    return result\n",
    "\n",
    "\n",
    "for gal_name in np.unique(nebulae['gal_name']):\n",
    "\n",
    "    # first the CO\n",
    "    co_filename = data_ext/'ALMA'/'v4p0'/f'{gal_name.lower()}_12m+7m+tp_co21_broad_tpeak.fits'\n",
    "    if not co_filename.is_file():\n",
    "        print(f'no ALMA CO for {gal_name}')\n",
    "    else:\n",
    "        with fits.open(co_filename) as hdul:\n",
    "            CO = NDData(data=hdul[0].data,\n",
    "                        meta=hdul[0].header,\n",
    "                        wcs=WCS(hdul[0].header))\n",
    "\n",
    "\n",
    "    # and then the GMC catalogue\n",
    "    gmc_directory  = data_ext/'Products'/'GMC'/'matched'\n",
    "    gmc_resolution = '150pc'\n",
    "    gmc_filename = [x for x in (gmc_directory/gmc_resolution).iterdir() if gal_name.lower() in x.stem]\n",
    "    if len(gmc_filename)==0:\n",
    "        print(f'no GMC catalogue for {gal_name}')\n",
    "    else:\n",
    "        gmc_filename = gmc_filename[0]\n",
    "        with fits.open(gmc_filename) as hdul:\n",
    "            GMC = Table(hdul[1].data)\n",
    "        GMC['SkyCoord'] = SkyCoord(GMC['XCTR_DEG']*u.deg,GMC['YCTR_DEG']*u.deg)\n",
    "        \n",
    "        N_HII = np.sum(nebulae['gal_name']==gal_name)\n",
    "        points = np.random.uniform(low=(0,0),high=CO.data.shape,size=(2*N_HII,2))\n",
    "\n",
    "        value = get_value(CO.data,points.astype(int))\n",
    "        x,y = points[~np.isnan(value)][:N_HII].T\n",
    "        coords = SkyCoord.from_pixel(x,y,wcs=CO.wcs)\n",
    "        \n",
    "        idx,sep,_=match_coordinates_sky(coords,GMC['SkyCoord'])\n",
    "        \n",
    "        sep_arcsec = np.mean(sep).to(u.arcsec)\n",
    "        sep_pc  = sep_arcsec.to(u.radian).value * sample_table.loc[gal_name]['distance']*1e6\n",
    "        \n",
    "        print(f'{gal_name}: {sep_arcsec:.2f}, {sep_pc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure DIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_dig(data,mask,label,position,factor=1,max_iter=10,size=32,plot=False):\n",
    "    '''measure the diffuse ionized gas around an HII-region'''\n",
    "    \n",
    "    cutout_mask = Cutout2D(mask.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    cutout_data = Cutout2D(data.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    \n",
    "    area_mask  = np.sum(cutout_mask.data==label)\n",
    "    input_mask = cutout_mask.data==label\n",
    "    \n",
    "    n_iter = 0\n",
    "    while True:\n",
    "        n_iter+=1\n",
    "        boundaries = find_boundaries(input_mask,mode='outer')\n",
    "        input_mask |=boundaries\n",
    "        area_boundary = np.sum(input_mask & np.isnan(cutout_mask.data)) \n",
    "        if area_boundary > factor*area_mask or n_iter>max_iter: break\n",
    "            \n",
    "    if plot:\n",
    "        fig,ax=plt.subplots(figsize=(5,5))\n",
    "        ax.imshow(cutout_mask.data,origin='lower')\n",
    "        mask = np.zeros((*cutout_mask.shape,4))\n",
    "        mask[input_mask & np.isnan(cutout_mask.data),:] = (1,0,0,0.5)\n",
    "        ax.imshow(mask,origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "    #if np.sum(boundaries & np.isnan(cutout_mask.data))==0:\n",
    "    #    print(f'no boundaries for {label}')\n",
    "    dig = cutout_data.data[input_mask & np.isnan(cutout_mask.data)]\n",
    "\n",
    "    return np.median(dig),np.mean(dig),np.sum(dig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old and young populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regions import read_ds9\n",
    "\n",
    "tmp = catalogue[catalogue['gal_name']=='NGC1365']\n",
    "\n",
    "# filter image with uncertainties\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'NGC1365_uvis_f275w_exp_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "\n",
    "associations,associations_mask = read_associations(folder=data_ext/'HST',target='NGC1365',scalepc=32)\n",
    "reg_young, reg_old = read_ds9(basedir/'data'/'tmp'/'young_old.reg')\n",
    "\n",
    "# the catalogue\n",
    "young_sample = tmp[reg_young.contains(tmp['SkyCoord_asc'],wcs=associations_mask.wcs)]\n",
    "young_sample = young_sample[young_sample['age']<20]\n",
    "old_sample = tmp[reg_old.contains(tmp['SkyCoord_asc'],wcs=associations_mask.wcs)]\n",
    "old_sample = old_sample[old_sample['age']>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the maps\n",
    "young = np.isin(associations_mask.data,associations[associations['age']<10]['assoc_ID'])\n",
    "old   = np.isin(associations_mask.data,associations[associations['age']>10]['assoc_ID'])\n",
    "\n",
    "young = young.astype(float)\n",
    "young[young==0] = np.nan\n",
    "\n",
    "old = old.astype(float)\n",
    "old[old==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(projection=F275.wcs)\n",
    "\n",
    "norm = simple_norm(F275.data,clip=False,percent=99)\n",
    "ax.imshow(F275.data,norm=norm,origin='lower',cmap=plt.cm.Greys,alpha=0.5)\n",
    "ax.imshow(old,vmin=0,vmax=1,cmap=plt.cm.Reds,alpha=0.8)\n",
    "ax.imshow(young,vmin=0,vmax=1,cmap=plt.cm.Blues,alpha=0.8)\n",
    "\n",
    "reg_young_pix = reg_young.to_pixel(F275.wcs)\n",
    "reg_old_pix = reg_old.to_pixel(F275.wcs)\n",
    "\n",
    "ax.add_artist(reg_young_pix.as_artist())\n",
    "ax.add_artist(reg_old_pix.as_artist())\n",
    "\n",
    "\n",
    "ax.set(xlim=[2000,5000],ylim=[3000,5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,10]\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "sc = ax.scatter(young_sample['EBV_stars'],young_sample['EBV_balmer'],color='g',label='upper arm')\n",
    "sc = ax.scatter(old_sample['EBV_stars'],old_sample['EBV_balmer'],color='m',label='lower arm')\n",
    "ax.legend()\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "#fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel='E(B-V) stars',ylabel='E(B-V) Balmer')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deproject radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import project\n",
    "\n",
    "def r25(name,table):\n",
    "    '''calculate deprojected r25\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # get the pixel position of the centre\n",
    "    with fits.open(data_ext/'MUSE_DR2.1'/'MUSEDAP'/f'{name}_MAPS.fits') as hdul:\n",
    "        wcs = WCS(hdul['FLUX'].header)\n",
    "    centre = sample_table.loc[name]['SkyCoord']\n",
    "    x_cen,y_cen = centre.to_pixel(wcs)\n",
    "    \n",
    "    pa  = sample_table.loc[name]['posang']\n",
    "    inc = sample_table.loc[name]['Inclination']\n",
    "    r25 = sample_table.loc[name]['r25']*u.arcmin\n",
    "    \n",
    "    # deproject\n",
    "    x_depr,y_depr = project(table['x_neb']-x_cen,table['y_neb']-y_cen,pa,inc)\n",
    "    skycoord_depr = SkyCoord.from_pixel(x_depr+x_cen,y_depr+y_cen,wcs)\n",
    "    \n",
    "    # separation to centre\n",
    "    sep = skycoord_depr.separation(centre)\n",
    "    \n",
    "    return (sep/r25).decompose()\n",
    "\n",
    "name = 'NGC0628'\n",
    "\n",
    "# the catalogue with the positions\n",
    "tmp = nebulae[nebulae['gal_name']==name]\n",
    "\n",
    "r25(name,tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at units of FUV and Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speclite.filters import FilterResponse, load_filters, plot_filters\n",
    "\n",
    "response_curve = ascii.read(basedir/'data'/'external'/'astrosat_response_curve.txt',\n",
    "                                     names=['wavelength','EA','Filter'])\n",
    "\n",
    "F148W_mask = response_curve['Filter']=='F148W'\n",
    "F148W_lam = response_curve['wavelength'][F148W_mask]*u.angstrom\n",
    "F148W_res = response_curve['EA'][F148W_mask] / max(response_curve['EA'][F148W_mask])\n",
    "F148W = FilterResponse(F148W_lam,F148W_res,meta=dict(group_name='Astrosat',band_name='F148W'))\n",
    "\n",
    "F154W_mask = response_curve['Filter']=='F154W'\n",
    "F154W_lam  = response_curve['wavelength'][F154W_mask]*u.angstrom\n",
    "F154W_res  = response_curve['EA'][F154W_mask] / max(response_curve['EA'][F154W_mask])\n",
    "F154W = FilterResponse(F154W_lam,F154W_res,meta=dict(group_name='Astrosat',band_name='F154W'))\n",
    "\n",
    "astrosat_filter = load_filters('Astrosat-F148W', 'Astrosat-F154W')\n",
    "plot_filters(astrosat_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = 1e6 * u.yr\n",
    "\n",
    "# find closest availalbe age\n",
    "ages=cluster.spectrum['Time']\n",
    "age =ages[np.argmin(np.abs(ages-age))]\n",
    "\n",
    "wavelength = cluster.spectrum[cluster.spectrum['Time']==age]['Wavelength']\n",
    "spectrum   = cluster.spectrum[cluster.spectrum['Time']==age]['Total']\n",
    "\n",
    "ages=cluster.uvline['Time']\n",
    "age =ages[np.argmin(np.abs(ages-age))]\n",
    "wavelength_uv = cluster.uvline[cluster.uvline['Time']==age]['Wavelength']\n",
    "spectrum_uv   = cluster.uvline[cluster.uvline['Time']==age]['lum']\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(wavelength,spectrum,color='tab:red')\n",
    "ax.plot(wavelength_uv,spectrum_uv,color='tab:blue')\n",
    "ax.set(xlim=[1000,2000],yscale='log',ylim=[1e38,1e40],\n",
    "       xlabel='wavelength / AA',ylabel='erg / s / AA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different methods to calculate FUV from starburst99\n",
    "age = np.unique(cluster.uvline['Time'])[4]\n",
    "\n",
    "wavelength = cluster.uvline['Wavelength'][cluster.uvline['Time']==age]\n",
    "spectrum   = cluster.uvline['lum'][cluster.uvline['Time']==age]\n",
    "print(f'uvline,   filter: {np.mean(np.interp(wavelength,F148W_lam,F148W_res)*spectrum):.2g}')\n",
    "\n",
    "mask = (wavelength>1250*u.angstrom) & (wavelength<1800*u.angstrom)\n",
    "print(f'uvline,   range:  {np.mean(spectrum[mask]):.2g}')\n",
    "\n",
    "wavelength = cluster.spectrum['Wavelength'][cluster.spectrum['Time']==age]\n",
    "spectrum   = cluster.spectrum['Total'][cluster.spectrum['Time']==age]\n",
    "print(f'spectrum, filter: {np.mean(np.interp(wavelength,F148W_lam,F148W_res)*spectrum):.2g}')\n",
    "\n",
    "mask = (wavelength>1250*u.angstrom) & (wavelength<1800*u.angstrom)\n",
    "print(f'spectrum, range:  {np.mean(spectrum[mask]):.2g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "for stellar_model in [41,42,43,44,45]:\n",
    "    cluster = Cluster(stellar_model=stellar_model)\n",
    "    cluster.measure_FUV()\n",
    "    _,_,metallicity,_ = find_model(stellar_model)\n",
    "    \n",
    "    ax.plot(cluster.ewidth['Time']/1e6,\n",
    "            cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],\n",
    "            label=f'Z={metallicity}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "#ax.set_title('Geneva Tracks with Rotation')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'HaFUV_starburst99_padova.pdf',dpi=400)\n",
    "plt.savefig(basedir/'reports'/'HaFUV_starburst99_padova.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,0.75*two_column/2))\n",
    "\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'])\n",
    "ax1.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'H$\\alpha$ / FUV')\n",
    "\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['eq_width_H_A'])\n",
    "ax2.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'EW(H$\\alpha$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'HaFUV.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = 1e6 * u.yr\n",
    "\n",
    "# find closest availalbe age\n",
    "ages=cluster.spectrum['Time']\n",
    "age =ages[np.argmin(np.abs(ages-age))]\n",
    "\n",
    "wavelength = cluster.spectrum[cluster.spectrum['Time']==age]['Wavelength']\n",
    "spectrum   = cluster.spectrum[cluster.spectrum['Time']==age]['Total']\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(wavelength,spectrum)\n",
    "ax.set(xlim=[500,8000],yscale='log',ylim=[1e35,1e40])\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(response_curve['lam'],response_curve['r'],color='grey')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D density histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic_2d\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "x = np.array([0.5,0.3,0.5,3.5])\n",
    "y = np.array([0.5,0.6,2.5,1.5])\n",
    "z = np.array([7.2,7.8,6.5,7.9])\n",
    "\n",
    "xlim,ylim=[0,4],[0,4]\n",
    "\n",
    "def hist_scatter(x,y,z,xlim,ylim):\n",
    "    \n",
    "    hist , x_e, y_e    = np.histogram2d(x,y, bins=4,range =[xlim,ylim], density=True)\n",
    "    mean , x_e, y_e, _ = binned_statistic_2d(x,y,z,bins=4,range = [xlim,ylim])\n",
    "    z = interpn((0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ),mean,np.vstack([x,y]).T,method=\"nearest\",bounds_error=False)\n",
    "\n",
    "    fig,ax=plt.subplots()\n",
    "    cmap = plt.cm.get_cmap('gray_r',4)\n",
    "    im = ax.imshow(mean.T,origin='lower',cmap=cmap,extent=[*xlim,*ylim],vmin=0,vmax=2)\n",
    "    sc=ax.scatter(x,y,c=z,cmap=plt.cm.viridis,vmin=6,vmax=8)\n",
    "    fig.colorbar(sc)\n",
    "    ax.set(xlim=xlim,ylim=ylim)\n",
    "    plt.show()\n",
    "    \n",
    "hist_scatter(x,y,z,xlim,ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure NUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import InverseVariance\n",
    "\n",
    "gal_name = 'NGC2835'\n",
    "\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "error_file = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_err-drc-wht.fits'\n",
    "\n",
    "if not filename.is_file():\n",
    "    print(f'no NUV data for {gal_name}')\n",
    "else:\n",
    "    with fits.open(filename) as hdul:\n",
    "        F275 = NDData(hdul[0].data,\n",
    "                        mask=hdul[0].data==0,\n",
    "                        meta=hdul[0].header,\n",
    "                        wcs=WCS(hdul[0].header))\n",
    "        with fits.open(error_file) as hdul:\n",
    "            F275.uncertainty = InverseVariance(hdul[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "HSTband = 'nuv'\n",
    "\n",
    "NUV_fluxes = {}\n",
    "NUV_fluxes_err = {}\n",
    "for scalepc in [8,16,32,64]:\n",
    "    print(f'working on {scalepc}pc')\n",
    "    associations, associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                                        target=gal_name.lower(),scalepc=scalepc,\n",
    "                                                        HSTband='nuv',version='v1p2',data='all')\n",
    "\n",
    "    # measure flux in mask and covert to physical units\n",
    "    std_err_map = np.sqrt(1/F275.uncertainty.array)\n",
    "\n",
    "    NUV = [np.sum(F275.data[associations_mask.data==assoc_ID]) for assoc_ID in associations['assoc_ID']]\n",
    "    NUV_err = [np.sqrt(np.sum(std_err_map[associations_mask.data==assoc_ID]**2)) for assoc_ID in associations['assoc_ID']]\n",
    "\n",
    "    NUV_mJy = 1e3*np.array(NUV)* F275.meta['PHOTFNU']*u.mJy\n",
    "    NUV_mJy_err = 1e3*np.array(NUV_err)* F275.meta['PHOTFNU']*u.mJy\n",
    "\n",
    "    NUV_flam = np.array(NUV)* F275.meta['PHOTFLAM']\n",
    "    NUV_flam_err = np.array(NUV_err)* F275.meta['PHOTFLAM']\n",
    "    \n",
    "    NUV_fluxes[scalepc] = NUV_mJy\n",
    "    NUV_fluxes_err[scalepc] = NUV_mJy_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(associations['NUV_dolflux_mjy'],NUV_mJy,s=1)\n",
    "ax1.plot([5e-5,1],[5e-5,1],color='black')\n",
    "ax1.set(xlim=[5e-5,1e0],ylim=[5e-5,1e0],xscale='log',yscale='log',\n",
    "        xlabel='NUV / mJy from catalogue',ylabel='NUV / mJy from image')\n",
    "ax1.set_title(r'using \\texttt{PHOTFNU}')\n",
    "\n",
    "\n",
    "# we are missing a factor 100 in the untis here\n",
    "ax2.scatter(associations['NUV_FLUX'],NUV_flam,s=1)\n",
    "ax2.plot([5e-19,5e-13],[5e-19,5e-13],color='black')\n",
    "ax2.set(xlim=[5e-19,5e-15],ylim=[5e-19,5e-15],xscale='log',yscale='log',\n",
    "        xlabel=r'NUV / erg s$^{-1}$ cm$^{-2}$ \\AA$^{-1}$ from catalogue',ylabel=r'NUV / erg s$^{-1}$ cm$^{-2}$ \\AA$^{-1}$ from image')\n",
    "ax2.set_title(r'using \\texttt{PHOTFLAM}')\n",
    "\n",
    "#fig.suptitle(f'{gal_name}, NUV (F275W) for {scalepc}pc associations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/f'remeasure_NUV_{gal_name}_{scalepc}pc.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(F275.meta['PHOTFNU']*u.Jy).to(u.erg/u.s/u.cm**2/u.Angstrom,equivalencies=u.spectral_density(2704*u.AA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F275.meta['PHOTFLAM']*u.erg/u.s/u.cm**2/u.Angstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(ncols=4,figsize=(1.5*two_column,two_column/4*1.5))\n",
    "\n",
    "for scalepc,ax in zip([8,16,32,64],axes):\n",
    "    \n",
    "    associations = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                     target=gal_name.lower(),scalepc=scalepc,\n",
    "                                     HSTband='nuv',version='v1p2',data='catalogue')\n",
    "    \n",
    "    dif = np.mean((associations['NUV_dolflux_mjy']-1.67*NUV_fluxes[scalepc].value)/associations['NUV_dolflux_mjy'])\n",
    "    ax.scatter(associations['NUV_dolflux_mjy'],1.67*NUV_fluxes[scalepc],s=1)\n",
    "    ax.plot([5e-5,1],[5e-5,1],color='black')\n",
    "    ax.set(xlim=[5e-5,1e0],ylim=[5e-5,1e0],xscale='log',yscale='log',\n",
    "            xlabel='NUV / mJy from catalogue',ylabel='NUV / mJy from image')\n",
    "    ax.set_title(f'{scalepc}pc ({100*dif:.2f}\\% smaller)')\n",
    "\n",
    "\n",
    "#fig.suptitle(f'{gal_name}, NUV (F275W) for {scalepc}pc associations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/f'remeasure_NUV_all_resolutions_{gal_name}.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now measure FUV inside the nebula mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "from astropy.nddata import NDData, StdDevUncertainty, InverseVariance\n",
    "\n",
    "HSTbands_wave = {'NUV':2704*u.AA,'U':3355*u.AA,'B':4325*u.AA,'V':5308*u.AA,'I':8024*u.AA}\n",
    "freq_to_wave = lambda band: u.mJy.to(u.erg/u.s/u.cm**2/u.Angstrom,equivalencies=u.spectral_density(HSTbands_wave[band]))\n",
    "\n",
    "extinction_model = O94(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    " \n",
    "    return ext,ext_err\n",
    "\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae catalogue' / 'Nebulae_catalogue_v2.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['NUV_FLUX'] = np.nan\n",
    "nebulae['NUV_FLUX_ERR'] = np.nan\n",
    "nebulae['NUV_FLUX_CORR'] = np.nan\n",
    "nebulae['NUV_FLUX_CORR_ERR'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_name = 'NGC1365'\n",
    "\n",
    "print(f'start with {gal_name}')\n",
    "p = {x:sample_table.loc[gal_name][x] for x in sample_table.columns}\n",
    "\n",
    "filename = data_ext / 'MUSE'/ 'DR2.1' / 'MUSEDAP' / f'{gal_name}_MAPS.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "filename = data_ext / 'Products' / 'Nebulae catalogue' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "print(f'read in nebulae catalogue')\n",
    "\n",
    "# NUV image\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "error_file = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_err-drc-wht.fits'\n",
    "\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                    mask=hdul[0].data==0,\n",
    "                    meta=hdul[0].header,\n",
    "                    wcs=WCS(hdul[0].header))\n",
    "    with fits.open(error_file) as hdul:\n",
    "        F275.uncertainty = InverseVariance(hdul[0].data)\n",
    "print(f'read in HST data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.regions import Regions\n",
    "from reproject import reproject_interp\n",
    "\n",
    "muse_regions = Regions(mask=nebulae_mask.data,projection=nebulae_mask.meta,bkg=-1)\n",
    "hst_regions = muse_regions.reproject(F275.meta)\n",
    "print('regions reprojected')\n",
    "\n",
    "muse_reproj, footprint = reproject_interp((nebulae_mask.mask,nebulae_mask.wcs),F275.meta)\n",
    "mean,median,std=sigma_clipped_stats(F275.data[footprint.astype(bool)])\n",
    "print('measuring sigma_clipped_stats')\n",
    "\n",
    "tmp = nebulae[nebulae['gal_name']==gal_name]\n",
    "\n",
    "std_err_map = np.sqrt(1/F275.uncertainty.array)\n",
    "\n",
    "flux = np.array([np.sum(F275.data[hst_regions.mask==ID]) for ID in tmp['region_ID']])\n",
    "err  = np.array([np.sqrt(np.sum(std_err_map[hst_regions.mask==ID]**2)) for ID in tmp['region_ID']])\n",
    "\n",
    "# convert counts to physical units\n",
    "flux = np.array(flux) * F275.meta['PHOTFLAM']\n",
    "err  = np.array(err) * F275.meta['PHOTFLAM']\n",
    "print('measuring flux')\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. NUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(2704*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(0.5*tmp['EBV'],tmp['EBV_ERR'],wavelength=2704*u.angstrom)\n",
    "\n",
    "nebulae['NUV_FLUX'][nebulae['gal_name']==gal_name] = 1e20*flux / extinction_mw\n",
    "nebulae['NUV_FLUX_ERR'][nebulae['gal_name']==gal_name] = 1e20*err / extinction_mw\n",
    "\n",
    "nebulae['NUV_FLUX_CORR'][nebulae['gal_name']==gal_name] = 1e20*flux / extinction_mw / ext_int \n",
    "nebulae['NUV_FLUX_CORR_ERR'][nebulae['gal_name']==gal_name] =  nebulae['NUV_FLUX_CORR'][nebulae['gal_name']==gal_name] *np.sqrt((err/flux)**2 + (ext_int_err/ext_int)**2)  \n",
    "\n",
    "print('extinction correction and write to catalogue\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors of FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/'old'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv_old = Table(hdul[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "tmp = fuv_old[fuv_old['gal_name']=='NGC3627']\n",
    "ax1.hist(tmp['FUV_FLUX'],bins=np.logspace(2,5,12))\n",
    "ax1.set(xscale='log')\n",
    "\n",
    "tmp = fuv[fuv['gal_name']=='NGC3351']\n",
    "ax2.hist(tmp['FUV_FLUX'],bins=np.logspace(2,5,12))\n",
    "ax2.set(xscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [1e2,1e6]\n",
    "\n",
    "fuv_flux_old = fuv_old['FUV_FLUX_CORR'][fuv['gal_name']!='NGC3351']\n",
    "fuv_flux_new = fuv['FUV_FLUX_CORR'][fuv['gal_name']!='NGC3351']\n",
    "ax.scatter(fuv_old['FUV_FLUX_CORR'][fuv['gal_name']=='NGC3351'],fuv['FUV_FLUX'][fuv['gal_name']=='NGC3351'],alpha=0.6,label='NGC3351',s=0.8)\n",
    "ax.scatter(fuv_flux_old,fuv_flux_new,alpha=0.6,label='not NGC3351',s=0.8)\n",
    "print(f'difference {100*np.nanmean((fuv_flux_old-fuv_flux_new)/fuv_flux_old):.2f}%')\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.legend()\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim,\n",
    "       xlabel='FUV old',ylabel='FUV new')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "bins = np.logspace(-1,2)\n",
    "ax.hist(fuv['FUV_FLUX_CORR']/fuv['FUV_FLUX_CORR_ERR'],bins=bins)\n",
    "ax.set(xscale='log',xlim=[0.1,1e2],xlabel=r'FUV/ $\\delta$FUV')\n",
    "mean_ston = np.nanmean(fuv['FUV_FLUX_CORR']/fuv['FUV_FLUX_CORR_ERR'])\n",
    "ax.set_title(f'mean S/N={mean_ston:.2f}')\n",
    "plt.savefig(basedir/'reports'/'benchmarks'/'FUV_StoN.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuv['FUV_FLUX_ERR_NEW'] = np.nan\n",
    "fuv['IntTime'] = np.nan\n",
    "fuv['CTSTOFLUX'] = np.nan\n",
    "for gal_name in np.unique(fuv['gal_name']):\n",
    "    astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F148W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F154W_flux_reproj.fits'\n",
    "        if not astro_file.is_file():\n",
    "            print(f'no astrosat file for {gal_name}')\n",
    "            continue\n",
    "            \n",
    "    with fits.open(astro_file) as hdul:\n",
    "        for row in hdul[0].header['COMMENT']:\n",
    "            if row.startswith('CTSTOFLUX'):\n",
    "                _,CTSTOFLUX = row.split(':')\n",
    "                CTSTOFLUX = float(CTSTOFLUX)\n",
    "            if row.startswith('IntTime'):\n",
    "                _,IntTime = row.split(':')\n",
    "                IntTime = float(IntTime)\n",
    "        header=hdul[0].header\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.nanmean(100*(fuv['FUV_FLUX_ERR']-fuv['FUV_FLUX_ERR_NEW'])/fuv['FUV_FLUX_ERR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [1e2,2e3]\n",
    "\n",
    "for gal_name in np.unique(fuv['gal_name']):\n",
    "    tmp = fuv[fuv['gal_name']==gal_name]\n",
    "    ax.scatter(tmp['FUV_FLUX_ERR'],tmp['FUV_FLUX_ERR_NEW'],c=tmp['IntTime']/60,vmin=0,vmax=100)\n",
    "\n",
    "ax.plot(lim,lim,color='black')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [5e2,5e8]\n",
    "\n",
    "ax.scatter(nebulae['HA6562_FLUX_CORR'],nebulae['HA_conv_FLUX_CORR'])\n",
    "\n",
    "ax.plot(lim,lim,color='black')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim,\n",
    "       xlabel=r'H$\\alpha$ catalogue',ylabel=r'H$\\alpha$ from convolved image')\n",
    "plt.savefig(basedir/'reports'/'benchmarks'/'Halpha_convolved.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [5e-3,5e5]\n",
    "\n",
    "ax.scatter(nebulae['HA6562_FLUX_ERR'],nebulae['HA_conv_FLUX_ERR'])\n",
    "\n",
    "ax.plot(lim,lim,color='black')\n",
    "\n",
    "ax.set(xscale='log',yscale='log',xlim=lim,ylim=lim,\n",
    "       xlabel=r'H$\\alpha$ Error catalogue',ylabel=r'H$\\alpha$ Error from convolved image')\n",
    "#plt.savefig(basedir/'reports'/'benchmarks'/'Halpha_convolved.pdf',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "sc = ax.scatter(fuv['FUV_FLUX'],fuv['FUV_FLUX']/fuv['FUV_FLUX_ERR'],c=fuv['IntTime']/60)\n",
    "ax.set(xscale='log',xlim=[7e1,2e6],yscale='log',ylim=[1,100],\n",
    "       xlabel='FUV flux',ylabel='S/N FUV')\n",
    "fig.colorbar(sc,label='Integration time / min')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.regions import Regions\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae catalogue'/ 'Nebulae_catalogue_v2.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "\n",
    "nebulae['FUV_FLUX'] = np.nan\n",
    "nebulae['FUV_FLUX_ERR'] = np.nan\n",
    "nebulae['FUV_FLUX_CORR'] = np.nan\n",
    "nebulae['FUV_FLUX_CORR_ERR'] = np.nan\n",
    "\n",
    "nebulae['HA_conv_FLUX'] = np.nan\n",
    "nebulae['HA_conv_FLUX_ERR'] = np.nan\n",
    "nebulae['HA_conv_FLUX_CORR'] = np.nan\n",
    "nebulae['HA_conv_FLUX_CORR_ERR'] = np.nan\n",
    "\n",
    "\n",
    "gal_name = 'NGC1566'\n",
    "        \n",
    "print(f'start with {gal_name}')\n",
    "\n",
    "print(f'read in nebulae catalogue')\n",
    "filename = next((data_ext/'MUSE'/'DR2.1'/'copt').glob(f'{gal_name}*.fits'))\n",
    "copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "filename = data_ext / 'Products' / 'Nebulae catalogue' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "print(f'read in astrosat data')\n",
    "astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F148W_flux_reproj.fits'\n",
    "if not astro_file.is_file():\n",
    "    astro_file = data_ext /'Astrosat' / f'{gal_name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        print(f'no astrosat file for {gal_name}')\n",
    "\n",
    "with fits.open(astro_file) as hdul:\n",
    "    d = hdul[0].data\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    for row in hdul[0].header['COMMENT']:\n",
    "        if row.startswith('CTSTOFLUX'):\n",
    "            _,CTSTOFLUX = row.split(':')\n",
    "            CTSTOFLUX = float(CTSTOFLUX)\n",
    "        if row.startswith('IntTime'):\n",
    "            _,IntTime = row.split(':')\n",
    "            IntTime = float(IntTime)\n",
    "\n",
    "\n",
    "print('reproject regions')\n",
    "muse_regions = Regions(mask=nebulae_mask.data,projection=nebulae_mask.meta,bkg=-1)\n",
    "astrosat_regions = muse_regions.reproject(astrosat.meta)\n",
    "\n",
    "tmp = nebulae[nebulae['gal_name']==gal_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "position = tmp[1]['SkyCoord']\n",
    "size = 5*u.arcsec\n",
    "Cutout_Halpha = Cutout2D(Halpha.data,position,size=size,wcs=Halpha.wcs)\n",
    "Cutout_FUV = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)\n",
    "Cutout_mask = Cutout2D(astrosat_regions.mask,position,size=size,wcs=astrosat.wcs)\n",
    "Cutout_mask_neb = Cutout2D(muse_regions.mask,position,size=size,wcs=Halpha.wcs)\n",
    "\n",
    "norm = simple_norm(Cutout_Halpha.data,clip=False,percent=99)\n",
    "ax1.imshow(Cutout_Halpha.data,norm=norm,origin='lower',cmap=plt.cm.Greens)\n",
    "ax1.axis('off')\n",
    "\n",
    "contours = []\n",
    "region_ID = np.unique(Cutout_mask_neb.data[~np.isnan(Cutout_mask_neb.data)])\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(Cutout_mask_neb.data)\n",
    "    blank_mask[Cutout_mask_neb.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "for coords in contours:\n",
    "    ax1.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label='association')\n",
    "\n",
    "norm = simple_norm(Cutout_FUV.data,clip=False,percent=99)\n",
    "ax2.imshow(Cutout_FUV.data,norm=norm,origin='lower',cmap=plt.cm.Blues)\n",
    "ax2.axis('off')\n",
    "\n",
    "ax2.imshow(Cutout_mask.data,origin='lower',cmap=plt.cm.gray,alpha=0.6)\n",
    "\n",
    "contours = []\n",
    "region_ID = np.unique(Cutout_mask.data[~np.isnan(Cutout_mask.data)])\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(Cutout_mask.data)\n",
    "    blank_mask[Cutout_mask.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "for coords in contours:\n",
    "    ax2.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label='association')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.5*np.log10(np.sqrt(2*np.pi))+2.5*np.log10(std) + 2.5/np.log(10)*(x-mu)**2/(2*std**2)-13.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.5e-17,1.5e-17)\n",
    "plt.plot(-2.5*np.log10(x)-13.74,norm.pdf(x,1e-17,1e-18),color='blue')\n",
    "plt.xlim((28,29.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    EBV_err = np.atleast_1d(EBV_err)\n",
    "    sample_size = 100000\n",
    "    \n",
    "    ext = pn.RedCorr(R_V=3.1,E_BV=EBV,law='CCM89 oD94').getCorr(wavelength)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = pn.RedCorr(R_V=3.1,E_BV=EBV_rand,law='CCM89 oD94').getCorr(wavelength)\n",
    "    \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    if plot:\n",
    "        fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(6,6/2))\n",
    "        ax1.hist(EBV_rand[:,0],bins=100)\n",
    "        ax1.axvline(EBV[0],color='black')\n",
    "        ax1.set(xlabel='E(B-V)')\n",
    "        ax2.hist(ext_arr[:,0],bins=100)\n",
    "        ax2.axvline(ext[0],color='black')\n",
    "        ax2.set(xlabel='extinction')\n",
    "        plt.show()\n",
    " \n",
    "    return ext,ext_err\n",
    "\n",
    "extinction(EBV=[0.089,0.3],EBV_err=[0.01,0.01],wavelength=5007,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping Circles\n",
    "\n",
    "https://math.stackexchange.com/questions/3543367/area-of-overlap-of-two-circles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(r1,r2,x):\n",
    "    h=1/(2*x)*np.sqrt(2*x**2*r1**2+2*x**2*r2**2+2*r1**2*r2**2-x**4-r1**4-r2**4)\n",
    "\n",
    "    return r1**2*np.arcsin(h/r1)+r2**2*np.arcsin(h/r2)-x*h\n",
    "\n",
    "\n",
    "def draw_overlap(n1,n2,n_overlap):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if n1<n2:\n",
    "        n1,n2=n2,n1\n",
    "    if n_overlap>n2/2:\n",
    "        raise ValueError('overlap can not be larger than individual samples')\n",
    "    \n",
    "    r1 = 1\n",
    "    r2 = np.sqrt(n2/n1)\n",
    "\n",
    "    separation = np.linspace(r1+r2,r1)\n",
    "    area = overlap(r1,r2,separation)\n",
    "    x = np.interp(np.sqrt(n_overlap/n1),area,separation)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "    patch = plt.Circle((0,0 ),r1,alpha=0.5,color='tab:red')\n",
    "    ax.add_artist(patch)\n",
    "    patch = plt.Circle((x,0),r2,alpha=0.5,color='tab:blue') \n",
    "    ax.add_artist(patch)\n",
    "\n",
    "    ax.set_aspect(1) \n",
    "    ax.axis('off')\n",
    "    ax.set(xlim=[-1.2,x+r2+0.2],ylim=[-1.2,1.2])\n",
    "    plt.show()\n",
    "    \n",
    "draw_overlap(23000,17000,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = assoc_tmp[assoc_tmp['gal_name']=='NGC2835']\n",
    "\n",
    "print(f'1to1: {np.sum(tmp[\"1to1\"])}')\n",
    "print(f'contained: {np.sum(tmp[\"overlap\"]==\"contained\")}')\n",
    "print(f'partial: {np.sum(tmp[\"overlap\"]==\"partial\")}')\n",
    "print(f'isolated: {np.sum(tmp[\"overlap\"]==\"isolated\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from astrotools.plot.utils import bin_stat\n",
    "from astrotools.plot.corner import corner_binned_percentile\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax1=plt.subplots(nrows=1,figsize=(single_column,single_column/1.618))\n",
    "\n",
    "# ----------------------- ax1 ------------------------------------------\n",
    "tmp = catalogue[catalogue['gal_name']=='NGC2835']\n",
    "\n",
    "q1 = 68\n",
    "q2 = 98\n",
    "\n",
    "xlim =[8e2,1e5]\n",
    "ylim = [1e4,5e7]\n",
    "ylim = [1e36,8e39]\n",
    "vmin,vmax = 0,15\n",
    "nbins = 6\n",
    "bins = np.logspace(2.7,5.2,nbins)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('plasma',6)\n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "rho = []\n",
    "age_bins = [0,5,10,15]\n",
    "for i in range(len(age_bins)-1):\n",
    "\n",
    "    sub = tmp[(tmp['age']>=age_bins[i]) & (tmp['age']<age_bins[i+1])]\n",
    "    x,y = sub['mass'],sub['HA6562_LUM_CORR']\n",
    "    x,mean,std = bin_stat(x,y,bins=bins,statistic='median')\n",
    "    ax1.errorbar(x,mean,fmt='o-',ms=1.5,color=cmap(norm(age_bins[i]+1)))\n",
    "\n",
    "x,y = tmp['mass'],tmp['HA6562_LUM_CORR']\n",
    "x,y = x[~np.isnan(y) & np.isfinite(y)],y[~np.isnan(y) & np.isfinite(y)]\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=68,color='gray',alpha=0.6)\n",
    "corner_binned_percentile(x,y,ax1,nbins=bins,n=98,color='gray',alpha=0.3)\n",
    "\n",
    "rho = spearmanr(x,y,nan_policy='omit')[0]\n",
    "label = r'$\\rho'+f'={np.nanmean(rho):.2f}$'\n",
    "t = ax1.text(0.05,0.93,label,transform=ax1.transAxes,ha='left',va='top')\n",
    "t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax1.set(xscale='log',yscale='log',xlim=xlim,ylim=ylim,\n",
    "        xlabel=r'mass / M$_\\odot$',ylabel=r'$F(\\mathrm{H}\\alpha)$ / erg s$^{-1}$')\n",
    "\n",
    "# we need a scatter plot instance for the color bar\n",
    "sc = ax1.scatter(19*[1],19*[1],c=19*[1],cmap=cmap,vmin=vmin,vmax=vmax)\n",
    "#divider = make_axes_locatable(ax1)\n",
    "#cax = divider.append_axes('top', size=\"10%\", pad=0.3)\n",
    "#cbar = fig.colorbar(sc,label='age / Myr',cax=cax,orientation='horizontal')\n",
    "#cbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "cbar = fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old vs new nebulae catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original catalogue from Francesco\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'Nebulae_catalogue_v2.fits') as hdul:\n",
    "    nebulae_old = Table(hdul[1].data)\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v3' /'Nebulae_Catalogue_v3.fits') as hdul:\n",
    "    nebulae_new = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_eq.fits') as hdul:\n",
    "    eq_width = Table(hdul[1].data)\n",
    "tmp = join(nebulae_new,eq_width,keys=['gal_name','region_ID'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "lim = [0,100]\n",
    "ax.scatter(tmp['eq_width'],tmp['EW_HA'])\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.set(xlim=lim,ylim=lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "lim = [0,500]\n",
    "ax.scatter(nebulae_old['HA6562_FLUX']/nebulae_old['HA6562_FLUX_ERR'],nebulae_new['HA6562_FLUX']/nebulae_new['HA6562_FLUX_ERR'])\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.set(xlim=lim,ylim=lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tmp #[tmp['HA6562_FLUX']>tmp['HA6562_FLUX_ERR']]\n",
    "f\"{np.mean(t['eq_width']-t['EW_HA']):.2f}+-{np.std(t['eq_width']-t['EW_HA']):.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(r0,r1,x0,x1):\n",
    "    \n",
    "    fig,ax=plt.subplots()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_widths = Table.read(basedir/'data'/'interim'/'Nebulae_Catalogue_v2p1_EW.fits')\n",
    "eq_widths['EW_HA'] = eq_widths['Halpha']/eq_widths['continuum_Halpha']\n",
    "eq_widths['EW_HA_corr'] = eq_widths['Halpha']/(eq_widths['continuum_Halpha']-eq_widths['continuum_Halpha_bkg'])\n",
    "\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v3.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(figsize=(two_column,two_column/2),ncols=2)\n",
    "\n",
    "lim = [0,300]\n",
    "\n",
    "ax1.scatter(nebulae['EW_HA'],eq_widths['EW_HA'])\n",
    "ax1.plot(lim,lim,color='black')\n",
    "ax1.set(xlim=lim,ylim=lim,xlabel='EW Kathryn',ylabel='EW Fabian')\n",
    "\n",
    "tmp = eq_widths[((eq_widths['continuum_Halpha']-eq_widths['continuum_Halpha_bkg'])>eq_widths['continuum_Halpha_error'])]\n",
    "print(len(tmp))\n",
    "ax2.scatter(tmp['EW_HA'],tmp['EW_HA_corr'])\n",
    "ax2.plot(lim,lim,color='black')\n",
    "ax2.set(xlim=lim,ylim=[0,3000],xlabel='EW original',ylabel='EW corrected')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'ew_corrected.png',dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "bins = np.logspace(2,5,16)\n",
    "ax.hist(fuv['FUV_FLUX'],bins=bins,alpha=0.5)\n",
    "ax.hist(fuv['FUV_BKG_FLUX'],bins=bins,alpha=0.5)\n",
    "ax.set(xscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"EW: {np.mean(catalogue['continuum_Halpha_bkg']/catalogue['continuum_Halpha'])*100:.1f}\")\n",
    "print(f\"FUV: {np.mean(catalogue['FUV_BKG_FLUX']/catalogue['FUV_FLUX'])*100:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"EW: {np.mean(nebulae['continuum_Halpha_bkg']/nebulae['continuum_Halpha'])*100:.1f} %\")\n",
    "print(f\"FUV: {np.mean(nebulae['FUV_BKG_FLUX']/nebulae['FUV_FLUX'])*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    EBV : array\n",
    "\n",
    "    EBV_err : array\n",
    "\n",
    "    wavelength : float\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    EBV_err = np.atleast_1d(EBV_err)\n",
    "    sample_size = 50000\n",
    "    \n",
    "    ext = pn.RedCorr(R_V=3.1,E_BV=EBV,law='CCM89 oD94').getCorr(wavelength)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = pn.RedCorr(R_V=3.1,E_BV=EBV_rand,law='CCM89 oD94').getCorr(wavelength)\n",
    "    \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    return ext,ext_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.RedCorr(R_V=3.1,E_BV=0.044,law='CCM89 oD94').getCorr(1481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn\n",
    "from tqdm import tqdm\n",
    "\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v3.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "fuv = Table.read(basedir/'data'/'interim'/'Nebulae_Catalogue_v2p1_FUV_reproj_old.fits')\n",
    "fuv = join(fuv,nebulae[['gal_name','region_ID','EBV','EBV_ERR']],keys=['gal_name','region_ID'])\n",
    "\n",
    "# Milky Way E(B-V) from  Schlafly & Finkbeiner (2011)\n",
    "EBV_MW = {'IC5332': 0.015,'NGC0628': 0.062,'NGC1087': 0.03,'NGC1300': 0.026,\n",
    "          'NGC1365': 0.018,'NGC1385': 0.018,'NGC1433': 0.008,'NGC1512': 0.009,\n",
    "          'NGC1566': 0.008,'NGC1672': 0.021,'NGC2835': 0.089,'NGC3351': 0.024,\n",
    "          'NGC3627': 0.037,'NGC4254': 0.035,'NGC4303': 0.02,'NGC4321': 0.023,\n",
    "          'NGC4535': 0.017,'NGC5068': 0.091,'NGC7496': 0.008}\n",
    "\n",
    "#fuv['FUV_FLUX_CORR2'] = np.nan\n",
    "#fuv['FUV_FLUX_CORR2_ERR'] = np.nan\n",
    "\n",
    "for gal_name in tqdm(np.unique(fuv['gal_name'])):\n",
    "    \n",
    "    tmp = fuv[fuv['gal_name']==gal_name]\n",
    "\n",
    "    EBV_correction = 1\n",
    "    rc_MW = pn.RedCorr(R_V=3.1,E_BV=EBV_correction*EBV_MW[gal_name],law='CCM89 oD94')\n",
    "    \n",
    "    ext_int,ext_int_err = extinction(EBV_correction*tmp['EBV'],tmp['EBV_ERR'],wavelength=1481)    \n",
    "\n",
    "    fuv['FUV_FLUX_CORR'][fuv['gal_name']==gal_name] = tmp['FUV_FLUX'] * ext_int \n",
    "    fuv['FUV_FLUX_CORR_ERR'][fuv['gal_name']==gal_name] =  fuv['FUV_FLUX_CORR'][fuv['gal_name']==gal_name] *np.sqrt((fuv['FUV_FLUX_ERR'][fuv['gal_name']==gal_name]/fuv['FUV_FLUX'][fuv['gal_name']==gal_name])**2 + (ext_int_err/ext_int)**2)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "    \n",
    "fuv[['EBV','EBV_ERR']]\n",
    "\n",
    "doc = f'''this catalogue contains the FUV fluxes for the objects in the nebula \n",
    "catalogue, measured from the Astrosat data (using the F148W filter for\n",
    "all galaxies except for NGC1433 and NGC1512, for which the F154W filter\n",
    "was used). All fluxes are in [f]=1e-20 erg s-1 cm-2 AA-1 and corrected \n",
    "for Milky Way foreground extinction (with the extinction curve from \n",
    "O'Donnell (1994) and E(B-V) from Schlafly & Finkbeiner (2011)). The \n",
    "columns ending with _CORR are also corrected for internal extinction, \n",
    "based on the E(B-V) from the nebula catalogue. \n",
    "Based on the nebula catalogue v2p0. \n",
    "This catalogue was created with the following script:\n",
    "https://github.com/fschmnn/cluster/blob/master/scripts/measure_FUV.py\n",
    "last update: {date.today().strftime(\"%b %d, %Y\")}\n",
    "'''\n",
    "\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "for i,comment in enumerate(doc.split('\\n')):\n",
    "    if i==0:\n",
    "        primary_hdu.header['COMMENT'] = comment\n",
    "    else:\n",
    "        primary_hdu.header[''] = comment\n",
    "table_hdu   = fits.BinTableHDU(fuv)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto('Nebulae_Catalogue_v2p1_FUV_reproj.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn\n",
    "\n",
    "Av1 = pn.RedCorr(R_V=3.1,E_BV=nebulae['EBV_balmer'],law='CCM89 oD94').getCorr(1481)\n",
    "Av044 = pn.RedCorr(R_V=3.1,E_BV=0.44*nebulae['EBV_balmer'],law='CCM89 oD94').getCorr(1481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.RedCorr(R_V=3.1,E_BV=0.44*2,law='CCM89 oD94').getCorr(1481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.plot import compare\n",
    "\n",
    "ax=compare(Av1,Av044,lim=[0.1,1e5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "ax.plot(nebulae['EBV_balmer'],Av1,label=r'$X=1$')\n",
    "ax.plot(nebulae['EBV_balmer'],Av044,label=r'$X=0.44$')\n",
    "ax.legend()\n",
    "#ax.scatter(nebulae['EBV_balmer'],Av044)\n",
    "ax.set(yscale='log',xlim=[0,1],ylim=[0.8,8e3],xlabel=r'$E(B-V)$',ylabel='RedCorr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuv = Table.read(basedir/'data'/'interim'/'Nebulae_Catalogue_v2p1_FUV_bkg.fits')\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.hist(100*fuv['FUV_BKG_FLUX']/fuv['FUV_FLUX'],bins=np.linspace(0,100,20))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "450.85px",
    "left": "38px",
    "top": "110.133px",
    "width": "267.633px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
