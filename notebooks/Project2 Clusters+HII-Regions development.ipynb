{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions development <a class=\"tocSkip\">\n",
    "\n",
    "this notebook contains small code snippets that were used to test ideas and validate concepts that are used in the main notebook. Each section should be self contained and run without the other sections (except for the first few sections, which are used throughout the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pnlf.packages import *\n",
    "\n",
    "from pnlf.constants import tab10, single_column, two_column\n",
    "from pnlf.plot import quick_plot\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout,\n",
    "                    #format='(levelname)s %(name)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# first we need to specify the path to the raw data\n",
    "basedir = Path('..')  # where we save stuff (and )\n",
    "data_ext = Path('a:')/'Archive' # raw data\n",
    "\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])\n",
    "sample_table['power_index'] = 2.3\n",
    "sample_table['power_index'][sample_table['AO'].mask]=2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE DAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# galaxies with astrosat\n",
    "hst_sample      = set(['NGC0628', 'NGC1433', 'NGC1566', 'NGC3351', 'NGC3627', 'NGC4535'])\n",
    "astrosat_sample = set([x.stem for x in (data_ext/'Astrosat').iterdir() if x.stem !='images'])\n",
    "muse_sample     = set(sample_table['Name'])\n",
    "complete_sample = hst_sample & astrosat_sample & muse_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.io import ReadLineMaps\n",
    "\n",
    "name = 'NGC0628'\n",
    "\n",
    "p = {x:sample_table.loc[name][x] for x in sample_table.columns}\n",
    "\n",
    "# read in the data we will be working with and print some information\n",
    "galaxy = ReadLineMaps(data_ext/'MUSE'/'DR2.1'/'copt',**p)\n",
    "#galaxy.Ebv = sample_table.loc[name]['E(B-V)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nebulae Catalogues with masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import filter_table\n",
    "\n",
    "filename = next((data_ext/'MUSE'/'DR2.1'/'copt').glob(f'{name}*.fits'))\n",
    "copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v2p1.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "print(f'{name}: {len(nebulae)} nebulae in initial catalogue')\n",
    "\n",
    "nebulae.rename_columns(['cen_x','cen_y'],['x','y'])\n",
    "\n",
    "# we remove some objects that are not HII-regions\n",
    "nebulae = filter_table(nebulae,gal_name=name,BPT_NII=0,BPT_SII=0,BPT_OI=0)\n",
    "with np.errstate(divide='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = (nebulae['SIII6312_FLUX']+nebulae['SIII9068_FLUX']) / (nebulae['SII6716_FLUX']+nebulae['SII6730_FLUX'])\n",
    "\n",
    "#nebulae = nebulae[['region_ID','x','y','met_scal','Delta_met','logq_D91']]\n",
    "nebulae.rename_columns(['region_ID','logq_D91'],['ID','ionization'])\n",
    "#nebulae['SkyCoord'] = SkyCoord.from_pixel(nebulae['x'],nebulae['y'],galaxy.wcs)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "\n",
    "filename = data_ext/'Products'/'Nebulae catalogue' /'spatial_masks'/f'{name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "    \n",
    "print(f'{name}: {len(nebulae)} nebulae in final catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### HST\n",
    "\n",
    "* Use clusters with 'PHANGS_CLUSTER_CLASS' == 1, 2, and 3. Objects with 'PHANGS_CLUSTER_CLASS' >= 4 are actually not clusters but artifacts, individual stars, etc. Objects with no classification have not been classified yet so don't use those either. They will be classified by our machine learning in the near future though.\n",
    "* If you are using the cluster properties, please use the chi2 minimized results (PHANGS_AGE_MINCHISQ, PHANGS_AGE_MINCHISQ_ERR, PHANGS_MASS_MINCHISQ, PHANGS_MASS_MINCHISQ_ERR, PHANGS_EBV_MINCHISQ, PHANGS_EBV_MINCHISQ_ERR) rather than the Bayesian estimates for now. This will change at some point in the future once the issues with the Bayesian analysis are resolved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# whitelight image\n",
    "with fits.open(data_ext / 'HST' / 'white_light' / name / f'{name.lower()}_white_24rgb.fits') as hdul:\n",
    "    hst_whitelight = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    hst_whitelight.data[hst_whitelight.data==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cluster catalogues\n",
    "filename = data_ext / 'HST' / 'cluster catalogue' / f'{name}_phangshst_base_catalog.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    clusters = Table(hdul[1].data)\n",
    "\n",
    "print(f'initial catalogue has {len(clusters)} entries')    \n",
    "# only CLASS 1,2 and 3 are classified as clusters\n",
    "clusters = clusters[np.isin(clusters['PHANGS_CLUSTER_CLASS'],[1,2,3])]\n",
    "# remove LEGUS columns\n",
    "clusters = clusters[[x for x in clusters.columns if 'LEGUS' not in x]]\n",
    "# remove the PHANGS label from the column names\n",
    "clusters.rename_columns([x for x in clusters.columns],[x.replace('PHANGS_','') for x in clusters.columns])\n",
    "# add SkyCoord to match with MUSE data\n",
    "clusters['SkyCoord'] = SkyCoord(clusters['RA']*u.degree,clusters['DEC']*u.degree)\n",
    "print(f'final catalogue has {len(clusters)} entries')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "\n",
    "cutout = Cutout2D(hst_whitelight.data,clusters['SkyCoord'][10],64*u.pix,wcs=hst_whitelight.wcs)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(projection=cutout.wcs)\n",
    "\n",
    "norm = simple_norm(cutout.data,clip=False)\n",
    "ax.imshow(cutout.data,norm=norm,cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Astrosat\n",
    "\n",
    "https://uvit.iiap.res.in/Instrument/Filters\n",
    "\n",
    "Ffilter = CPS x UC (ergs s-1 cm-2 A-1)\n",
    "mAB = -2.5 log10 (CPS) + ZP\n",
    " \n",
    "* CPS : Counts per second, the source count-rate\n",
    "* UC : Unit Conversion for the filter (see the table below)\n",
    "* ZP: Zero point for the filter (see the table below) \n",
    "\n",
    "the resolution is 0.4\" per pixel. With a PSF resolution of 1.8\" this leads to fwhm ~ 4.5 px. This corresponds to a std = 1.91 px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitelight image\n",
    "astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F148W_flux_reproj.fits'\n",
    "\n",
    "if not astro_file.is_file():\n",
    "    astro_file = data_ext / 'Astrosat' / name / f'{name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        print(f'no astrosat file for {name}')\n",
    "    \n",
    "with fits.open(astro_file) as hdul:\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_plot(astrosat.data,wcs=astrosat.wcs,xlim=[2100,2700],ylim=[1800,2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_exact\n",
    "\n",
    "FUV, footprint = reproject_exact(astrosat,galaxy.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot.plot import create_RGB\n",
    "\n",
    "path = data_ext / 'MUSE_DR2' / 'filterImages'  \n",
    "\n",
    "sdss_g, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_g_WCS_Pall_mad.fits',header=True)\n",
    "sdss_r, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_r_WCS_Pall_mad.fits',header=True)\n",
    "sdss_i, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_i_WCS_Pall_mad.fits',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gri_composite = create_RGB(sdss_i,sdss_r,sdss_g,percentile=98)\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1  = fig.add_subplot(121,projection=WCS(h))\n",
    "ax1.imshow(gri_composite)\n",
    "\n",
    "#rgb = make_rgb_image(galaxy.HA6562,galaxy.OIII5006,FUV,vmin=0,vmax=300)\n",
    "\n",
    "rgb = create_RGB(galaxy.HA6562,galaxy.OIII5006,FUV,percentile=98,weights=[1,1,0.8])\n",
    "rgb[np.isnan(galaxy.HA6562)] = (1,1,1)\n",
    "\n",
    "ax2  = fig.add_subplot(122,projection=galaxy.wcs)\n",
    "ax2.imshow(rgb)\n",
    "\n",
    "labels=[r'H$\\alpha$','[OIII]','FUV']\n",
    "handles = 3*[mpl.patches.Rectangle((0, 0), 0, 0, alpha=0.0)]\n",
    "leg = ax2.legend(handles,labels, frameon=True,framealpha=0.7,handlelength=0,prop={'size': 6},loc=3)\n",
    "for color,text in zip(['red','green','blue'],leg.get_texts()):\n",
    "    text.set_color(color)\n",
    "        \n",
    "plt.savefig(basedir/'reports'/f'{name}_astrosat_rgb.pdf',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving to different PSF\n",
    "\n",
    "https://www.astro.princeton.edu/~ganiano/Kernels/Ker_2018/Kernel_FITS_Files/Hi_Resolution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import create_matching_kernel, TopHatWindow, CosineBellWindow\n",
    "from astropy.modeling.models import Gaussian2D, Moffat2D\n",
    "from astropy.convolution import convolve\n",
    "from astropy.stats import gaussian_sigma_to_fwhm, gaussian_fwhm_to_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the fwhm varies between 3 and 4.2 ($\\gamma=2.8$ bis $3.9$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_to_moffat(std,alpha,gamma,size=32,window=None):\n",
    "    '''create a kernel to convolve from moffat to gaussian'''\n",
    "\n",
    "    fwhm_gauss  = std * gaussian_sigma_to_fwhm\n",
    "    fwhm_moffat = gamma * (2*np.sqrt(2**(1/alpha)-1))\n",
    "        \n",
    "    if fwhm_gauss > fwhm_moffat:\n",
    "        logger.warning('fwhm of input is larger than output')\n",
    "        \n",
    "    y, x = np.mgrid[0:2*size+1, 0:2*size+1]\n",
    "\n",
    "    gm1 = Gaussian2D(1, size, size, std, std)\n",
    "    g1  = gm1(x, y)\n",
    "    g1 /= g1.sum()\n",
    "    \n",
    "    gm2 = Moffat2D(1,size,size,alpha=alpha,gamma=gamma)\n",
    "    g2  = gm2(x,y)\n",
    "    g2 /= g2.sum()\n",
    "\n",
    "    return create_matching_kernel(g1, g2,window=window)\n",
    "    \n",
    "def moffat_to_gaussian(std,alpha,gamma,size=32,window=None):\n",
    "    '''create a kernel to convolve from gaussian to moffat'''\n",
    "    \n",
    "    fwhm_gauss  = std * gaussian_sigma_to_fwhm\n",
    "    fwhm_moffat = gamma * (2*np.sqrt(2**(1/alpha)-1))\n",
    "        \n",
    "    if fwhm_moffat > fwhm_gauss:\n",
    "        logger.warning('fwhm of input is larger than output')\n",
    "        \n",
    "    y, x = np.mgrid[0:2*size+1, 0:2*size+1]\n",
    "\n",
    "    gm1 = Moffat2D(1,size,size,alpha=alpha,gamma=gamma)\n",
    "    g1  = gm1(x,y)\n",
    "    g1 /= g1.sum()\n",
    "\n",
    "    gm2 = Gaussian2D(1, size, size, std, std)\n",
    "    g2  = gm2(x, y)\n",
    "    g2 /= g2.sum()\n",
    "\n",
    "    return create_matching_kernel(g1, g2,window=window)\n",
    "\n",
    "def gaussian_to_gaussian(std_in,std_out,size=32,window=None):\n",
    "    '''create a kernel to convolve from gaussian to gaussian'''\n",
    "    \n",
    "        \n",
    "    if std_in > std_out:\n",
    "        logger.warning('fwhm of input is larger than output')\n",
    "        \n",
    "    y, x = np.mgrid[0:2*size+1, 0:2*size+1]\n",
    "\n",
    "    gm1 = Gaussian2D(1, size, size, std_in, std_in)\n",
    "    g1  = gm1(x,y)\n",
    "    g1 /= g1.sum()\n",
    "\n",
    "    gm2 = Gaussian2D(1, size, size, std_out, std_out)\n",
    "    g2  = gm2(x, y)\n",
    "    g2 /= g2.sum()\n",
    "\n",
    "    return create_matching_kernel(g1, g2,window=window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a perfect 2D Moffat and convolve it to a larger Gaussian. Then we try to fit the convolved image.\n",
    "\n",
    "Astrosat has a PSF of $\\sigma=1.9px$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.photometry import growth_curve\n",
    "\n",
    "size  = 32\n",
    "std   = 2\n",
    "alpha = 2.8\n",
    "gamma = 3\n",
    "fwhm_m = gamma * (2*np.sqrt(2**(1/alpha)-1))\n",
    "\n",
    "y, x = np.mgrid[0:2*size+1, 0:2*size+1]\n",
    "gm = Moffat2D(1,size,size,alpha,gamma)\n",
    "g  = gm(x,y)\n",
    "g /= g.sum()\n",
    "\n",
    "convolved = convolve(g,moffat_to_gaussian(std,alpha,gamma,window=CosineBellWindow(0.9)),preserve_nan=True)\n",
    "\n",
    "fwhm = growth_curve(convolved,size,size,'gaussian',rmax=30,alpha=alpha,plot=True)\n",
    "print(f'fwhm_moffat={fwhm_m:.2f}, fwhm_g={gaussian_sigma_to_fwhm*std:.2f}, fwhm_measured: {fwhm[0][0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'external'/'Kernel_HiRes_Moffet_01.5_to_Gauss_02.0.fits') as hdul:\n",
    "    moffat_to_gaussian = hdul[0].data\n",
    "    moffat_to_gaussian_header = hdul[0].header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-measure H$\\alpha$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Ultraviolet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrotools.regions import Regions\n",
    "\n",
    "muse_regions = Regions(mask=nebulae_mask.data,projection=nebulae_mask.meta,bkg=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = np.array([np.sum(galaxy.HA6562[muse_regions.mask==ID]) for ID in nebulae['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(6,3))\n",
    "lim = [0,1e6]\n",
    "ax1.scatter(nebulae['HA6562_FLUX'],flux)\n",
    "ax1.plot(lim,lim)\n",
    "ax1.set(xlim=lim,ylim=lim,xlabel='HA Francesco',ylabel='HA binned')\n",
    "\n",
    "ax2.scatter(nebulae['HA6562_FLUX'],100*np.abs((nebulae['HA6562_FLUX']-flux)/nebulae['HA6562_FLUX']))\n",
    "ax2.set_ylim([-5,100])\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'HII_fluxes_native_DR1.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve MUSE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convolve the convolved MUSE maps to astrosat\n",
    "kernel = gaussian_to_gaussian(std_in  = 0.92*gaussian_fwhm_to_sigma, \n",
    "                            std_out = 1.9,\n",
    "                            window = TopHatWindow(0.2)) \n",
    "\n",
    "muse_convolved = convolve(galaxy.HA6562,\n",
    "                          kernel= kernel,\n",
    "                          preserve_nan=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the mean fwhm\n",
    "gamma  = np.nanmax(galaxy.PSF)\n",
    "kernel = moffat_to_gaussian(std = 1.8,\n",
    "                            alpha  = galaxy.power_index,\n",
    "                            gamma  = gamma,\n",
    "                            window = TopHatWindow(0.2)) \n",
    "\n",
    "muse_convolved = convolve(galaxy.HA6562,\n",
    "                          kernel= kernel,\n",
    "                          preserve_nan=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_convolved = np.array([np.sum(muse_convolved[muse_regions.mask==ID]) for ID in nebulae['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(6,3))\n",
    "lim = [0,1e6]\n",
    "ax1.scatter(nebulae['HA6562_FLUX'],flux_convolved)\n",
    "ax1.plot(lim,lim)\n",
    "ax1.set(xlim=lim,ylim=lim,xlabel='HA Francesco',ylabel='HA convolved')\n",
    "\n",
    "ax2.scatter(nebulae['HA6562_FLUX'],100*np.abs((nebulae['HA6562_FLUX']-flux_convolved)/nebulae['HA6562_FLUX']))\n",
    "ax2.set(ylim=[-5,100],xlabel='HA Francesco',ylabel='diff in %')\n",
    "\n",
    "print(f\"{100*np.sum(np.abs((nebulae['HA6562_FLUX']-flux_convolved)/nebulae['HA6562_FLUX'])<0.2)/len(nebulae):.2f}% under 20%\")\n",
    "\n",
    "#plt.savefig(basedir/'reports'/'HII_fluxes_convolved_DR1.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebin convolved MUSE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import resolution_from_wcs\n",
    "from reproject import reproject_exact, reproject_interp\n",
    "\n",
    "def block_reduce(data,header,block_size=2):\n",
    "    '''\n",
    "    \n",
    "    CRPIXi    Reference pixel on the horizonal and vertical axis\n",
    "    CRVALi    Horizonal and vertical axis WCS value at the reference pixel\n",
    "    CDi_j     Rotation and scaling matrix\n",
    "    '''\n",
    "    \n",
    "    new_header = header.copy()\n",
    "    \n",
    "    for i in [1,2]:\n",
    "        new_header[f'CD{i}_{i}'] = header[f'CD{i}_{i}'] * block_size\n",
    "        new_header[f'NAXIS{i}'] = int(header[f'NAXIS{i}'] / block_size)\n",
    "        new_header[f'CRPIX{i}'] = header[f'CRPIX{i}'] / block_size\n",
    "    \n",
    "    new_data, footprint = reproject_exact((data, header), new_header)\n",
    "    \n",
    "    return new_data, new_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_binned, muse_binned_header = block_reduce(muse_convolved,galaxy.header,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot.plot import quick_plot\n",
    "\n",
    "quick_plot(muse_binned,wcs=WCS(muse_binned_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_regions = muse_regions.reproject(muse_binned_header)\n",
    "flux_binned = np.array([np.sum(muse_binned[binned_regions.mask==ID]) for ID in nebulae['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(6,3))\n",
    "lim = [0,1e6]\n",
    "ax1.scatter(nebulae['HA6562_FLUX'],4*flux_binned)\n",
    "ax1.plot(lim,lim)\n",
    "ax1.set(xlim=lim,ylim=lim,xlabel='HA Francesco',ylabel='HA binned')\n",
    "\n",
    "ax2.scatter(nebulae['HA6562_FLUX'],100*np.abs((nebulae['HA6562_FLUX']-4*flux_binned)/nebulae['HA6562_FLUX']))\n",
    "ax2.set(ylim=[-5,100],xlabel='HA Francesco',ylabel='diff in %')\n",
    "\n",
    "print(f\"{100*np.sum(np.abs((nebulae['HA6562_FLUX']-4*flux_binned)/nebulae['HA6562_FLUX'])<0.2)/len(nebulae):.2f}% under 20%\")\n",
    "\n",
    "plt.savefig(basedir/'reports'/'HII_fluxes_binned_DR1.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_regions.plot(muse_binned,filename=basedir/'reports'/'binned_regions.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "\n",
    "pos  = pn['SkyCoord'][6]\n",
    "size = 20*u.arcsecond\n",
    "\n",
    "cutout1 = Cutout2D(galaxy.OIII5006,pos,size,wcs=galaxy.wcs)\n",
    "cutout2 = Cutout2D(convolved,pos,size,wcs=galaxy.wcs)\n",
    "cutout3 = Cutout2D(convolved_rebin,pos,size,wcs=WCS(new_header))\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "ax1  = fig.add_subplot(131,projection=cutout1.wcs)\n",
    "norm = simple_norm(cutout1.data,clip=False,percent=99)\n",
    "ax1.imshow(cutout1.data,norm=norm,cmap=plt.cm.gray)\n",
    "\n",
    "ax2  = fig.add_subplot(132,projection=cutout2.wcs)\n",
    "norm = simple_norm(cutout2.data,clip=False,percent=99)\n",
    "ax2.imshow(cutout2.data,norm=norm,cmap=plt.cm.gray)\n",
    "\n",
    "ax3  = fig.add_subplot(133,projection=cutout2.wcs)\n",
    "norm = simple_norm(cutout2.data,clip=False,percent=99)\n",
    "ax3.imshow(cutout2.data,norm=norm,cmap=plt.cm.gray)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/f'{name}_muse_convolved.pdf',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reproject MUSE convolved to astrosat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_astrosat, footprint = reproject_exact((muse_binned,muse_binned_header),astrosat.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astrosat_regions = muse_regions.reproject(astrosat.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_astrosat = np.array([np.sum(muse_astrosat[astrosat_regions.mask==ID]) for ID in nebulae['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(6,3))\n",
    "lim = [1e6,1e7]\n",
    "ax1.scatter(nebulae['HA6562_FLUX'],4*flux_astrosat)\n",
    "ax1.plot(lim,lim)\n",
    "ax1.set(xlim=lim,ylim=lim,xlabel='HA Francesco',ylabel='HA binned')\n",
    "\n",
    "ax2.scatter(nebulae['HA6562_FLUX'],100*((nebulae['HA6562_FLUX']-4*flux_astrosat)/nebulae['HA6562_FLUX']))\n",
    "ax2.set(ylim=[-100,100],xlabel='HA Francesco',ylabel='diff in %')\n",
    "\n",
    "print(f\"{100*np.sum(np.abs((nebulae['HA6562_FLUX']-4*flux_astrosat)/nebulae['HA6562_FLUX'])<0.1)/len(nebulae):.2f}% under 10%\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or directly reproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_astrosat2, footprint = reproject_exact((galaxy.HA6562,galaxy.wcs),astrosat.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_astrosat2 = np.array([np.sum(muse_astrosat2[astrosat_regions.mask==ID]) for ID in nebulae['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(6,3))\n",
    "lim = [1e6,1e7]\n",
    "ax1.scatter(nebulae['HA6562_FLUX'],4*flux_astrosat2)\n",
    "ax1.plot(lim,lim)\n",
    "ax1.set(xlim=lim,ylim=lim,xlabel='HA Francesco',ylabel='HA binned')\n",
    "\n",
    "ax2.scatter(nebulae['HA6562_FLUX'],100*np.abs((nebulae['HA6562_FLUX']-4*flux_astrosat2)/nebulae['HA6562_FLUX']))\n",
    "ax2.set(ylim=[-5,100],xlabel='HA Francesco',ylabel='diff in %')\n",
    "\n",
    "print(f\"{100*np.sum(np.abs((nebulae['HA6562_FLUX']-4*flux_astrosat2)/nebulae['HA6562_FLUX'])<0.1)/len(nebulae):.2f}% under 10%\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Measure FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pnlf.regions import Regions\n",
    "\n",
    "muse_regions = Regions(mask=HII_mask.data,projection=HII_mask.meta,bkg=-1)\n",
    "astrosat_regions = muse_regions.reproject(astrosat.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from regions import PixCoord, RectanglePixelRegion\n",
    "\n",
    "# test if all regions fall in the astrosat image\n",
    "x,y = astrosat.data.shape\n",
    "image_reg = RectanglePixelRegion(PixCoord(x/2,y/2),x,y)\n",
    "image_reg_sky = image_reg.to_sky(astrosat.wcs)\n",
    "\n",
    "np.all(image_reg_sky.contains(nebulae['SkyCoord'],wcs=astrosat.wcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "\n",
    "# find the contours around the image to use as vertices for the PixelRegion\n",
    "contours = find_contours(HII_mask.mask.astype(int),0.5)\n",
    "# we use the region with the most vertices\n",
    "coords = max(contours,key=len)\n",
    "# the coordinates from find_counters are switched compared to astropy\n",
    "reg_muse_pix  = PolygonPixelRegion(vertices = PixCoord(*coords.T[::-1])) \n",
    "reg_muse_sky  = reg_muse_pix.to_sky(HII_mask.wcs)\n",
    "\n",
    "reg_muse_astro  = reg_muse_sky.to_pixel(astrosat.wcs)\n",
    "\n",
    "# plot image\n",
    "ax = quick_plot(astrosat,cmap=plt.cm.gray)\n",
    "#reg_muse_hst.plot(ax=ax,color='tab:red',label='MUSE')\n",
    "#x,y = nebulae['SkyCoord'].to_pixel(astrosat.wcs)\n",
    "#ax.scatter(x,y,s=2,color='tab:blue',label='nebula')\n",
    "ax.imshow(astrosat_regions.mask,alpha=1,cmap=plt.cm.Reds)\n",
    "ax.set(xlim=(1000,3000),ylim=(1000,2700))\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_astrosat_regions.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we only calculate sigma for the part of the image that is actually used\n",
    "from reproject import reproject_interp\n",
    "muse_reproj, footprint = reproject_interp((nebulae_mask.mask,nebulae_mask.wcs),astrosat.meta)\n",
    "\n",
    "mean,median,std=sigma_clipped_stats(astrosat.data[footprint.astype(bool)])\n",
    "print(mean,median,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from photutils import aperture_photometry\n",
    "\n",
    "tmp = nebulae[nebulae['gal_name']==name]\n",
    "\n",
    "flux = np.array([np.sum(astrosat.data[astrosat_regions.mask==ID]) for ID in tmp['region_ID']])\n",
    "err = np.array([np.sqrt(median**2 * len(astrosat_regions.coords[astrosat_regions.labels.index(ID)][0])) for ID in tmp['region_ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "\n",
    "extinction_model = O94(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    if True:\n",
    "        fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "        ax1.hist(EBV_rand[:,0],bins=100)\n",
    "        ax1.axvline(EBV[0],color='black')\n",
    "        ax1.set(xlabel='E(B-V)')\n",
    "        ax2.hist(ext_arr[:,0],bins=100)\n",
    "        ax2.axvline(ext[0],color='black')\n",
    "        ax2.set(xlabel='extinction')\n",
    "        plt.show()\n",
    " \n",
    "    return ext,ext_err\n",
    "\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. FUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(0.5*tmp['EBV'],tmp['EBV_ERR'],wavelength=1481*u.angstrom)\n",
    "\n",
    "tmp['FUV'] = 1e20*flux / extinction_mw / ext_int \n",
    "tmp['FUV_err'] = tmp['FUV']*np.sqrt((err/flux)**2 + (ext_int_err/ext_int)**2)  \n",
    "\n",
    "\n",
    "ext_int,ext_int_err = extinction(tmp['EBV'],tmp['EBV_ERR'],wavelength=6562*u.angstrom)\n",
    "\n",
    "tmp['HA6562_FLUX_CORR2'] = tmp['HA6562_FLUX'] / ext_int \n",
    "tmp['HA6562_FLUX_CORR2_ERR'] = tmp['HA6562_FLUX_CORR2']*np.sqrt((tmp['HA6562_FLUX_ERR']/tmp['HA6562_FLUX'])**2 + (ext_int_err/ext_int)**2)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Now do it for all galaxies\n",
    "\n",
    "!!!deprecated!!! measure_FUV.py is now used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pnlf.regions import Regions\n",
    "from reproject import reproject_interp\n",
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "\n",
    "extinction_model = O94(Rv=3.1)\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(data_ext / 'MUSE_DR2' / 'Nebulae catalogue' / 'Nebulae_Catalogue_DR2_native.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['FUV'] = np.nan\n",
    "nebulae['FUV_ERR'] = np.nan\n",
    "    \n",
    "nebulae['gal_name'][nebulae['gal_name']=='NGC628'] = 'NGC0628'\n",
    "\n",
    "\n",
    "for name in ['IC5332','NGC0628','NGC1300','NGC1365','NGC1433',\n",
    "             'NGC1512','NGC1566','NGC2835','NGC3351','NGC3627','NGC4254']:\n",
    "    \n",
    "    print(f'start with {name}')\n",
    "    p = {x:sample_table.loc[name][x] for x in sample_table.columns}\n",
    "\n",
    "    filename = data_ext / 'MUSE_DR2' / 'Nebulae catalogue' /'spatial_masks'/f'{name}_HIIreg_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        HII_mask = NDData(hdul[0].data-1,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        HII_mask.data[HII_mask.data==-1] = np.nan\n",
    "        \n",
    "    print(f'read in nebulae catalogue')\n",
    "    \n",
    "    # whitelight image\n",
    "    astro_file = data_ext / 'Astrosat' / name / f'{name}_FUV_F148W_flux_reproj.fits'\n",
    "\n",
    "    if not astro_file.is_file():\n",
    "        astro_file = data_ext / 'Astrosat' / name / f'{name}_FUV_F154W_flux_reproj.fits'\n",
    "        if not astro_file.is_file():\n",
    "            print(f'no astrosat file for {name}')\n",
    "\n",
    "    with fits.open(astro_file) as hdul:\n",
    "        d = hdul[0].data\n",
    "        astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    print(f'read in astrosat data')\n",
    "    \n",
    "    muse_regions = Regions(mask=HII_mask.data,projection=HII_mask.meta,bkg=-1)\n",
    "    astrosat_regions = muse_regions.reproject(astrosat.meta)\n",
    "    print('regions reprojected')\n",
    "    \n",
    "    muse_reproj, footprint = reproject_interp((HII_mask.mask,HII_mask.wcs),astrosat.meta)\n",
    "    mean,median,std=sigma_clipped_stats(astrosat.data[footprint.astype(bool)])\n",
    "    print('measuring sigma_clipped_stats')\n",
    "    \n",
    "    tmp = nebulae[nebulae['gal_name']==name]\n",
    "\n",
    "    flux = np.array([np.sum(astrosat.data[astrosat_regions.mask==ID]) for ID in tmp['region_ID']])\n",
    "    err = np.array([np.sqrt(median**2 * len(astrosat_regions.coords[astrosat_regions.labels.index(ID)][0])) for ID in tmp['region_ID']])\n",
    "    print('measuring flux')\n",
    "    \n",
    "    extinction_mw  = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "    extinction_int = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*tmp['EBV'])\n",
    "\n",
    "    nebulae['FUV'][nebulae['gal_name']==name] = 1e20*flux / extinction_mw / extinction_int \n",
    "    nebulae['FUV_ERR'][nebulae['gal_name']==name] = 1e20*err  \n",
    "    print('extinction correction and write to catalogue\\n')\n",
    "    \n",
    "# write to file\n",
    "primary_hdu = fits.PrimaryHDU()\n",
    "table_hdu   = fits.BinTableHDU(nebulae)\n",
    "hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "hdul.writeto(basedir/'data'/'interim'/'Nebulae_Catalogue_with_FUV.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure NUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import InverseVariance\n",
    "\n",
    "gal_name = 'NGC0628'\n",
    "\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "error_file = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_err-drc-wht.fits'\n",
    "\n",
    "if not filename.is_file():\n",
    "    print(f'no NUV data for {gal_name}')\n",
    "else:\n",
    "    with fits.open(filename) as hdul:\n",
    "        F275 = NDData(hdul[0].data,\n",
    "                        mask=hdul[0].data==0,\n",
    "                        meta=hdul[0].header,\n",
    "                        wcs=WCS(hdul[0].header))\n",
    "        with fits.open(error_file) as hdul:\n",
    "            F275.uncertainty = InverseVariance(hdul[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "HSTband = 'nuv'\n",
    "scalepc = 64\n",
    "\n",
    "associations, associations_mask = read_associations(folder=data_ext/'HST'/'stellar_associations',target=gal_name.lower(),scalepc=scalepc,\n",
    "                                                    HSTband='nuv',version=version,data='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure flux in mask and covert to physical units\n",
    "std_err_map = np.sqrt(1/F275.uncertainty.array)\n",
    "\n",
    "NUV = [np.sum(F275.data[associations_mask.data==assoc_ID]) for assoc_ID in associations['assoc_ID']]\n",
    "NUV_err = [np.sqrt(np.sum(std_err_map[associations_mask.data==assoc_ID]**2)) for assoc_ID in associations['assoc_ID']]\n",
    "\n",
    "NUV_mJy = 1e3*np.array(NUV)* F275.meta['PHOTFNU']*u.mJy\n",
    "NUV_mJy_err = 1e3*np.array(NUV_err)* F275.meta['PHOTFNU']*u.mJy\n",
    "\n",
    "NUV_flam = np.array(NUV)* F275.meta['PHOTFLAM']\n",
    "NUV_flam_err = np.array(NUV_err)* F275.meta['PHOTFLAM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(associations['NUV_dolflux_mjy'],NUV_mJy,s=1)\n",
    "ax1.plot([5e-5,1],[5e-5,1],color='black')\n",
    "ax1.set(xlim=[5e-5,1e0],ylim=[5e-5,1e0],xscale='log',yscale='log',\n",
    "        xlabel='NUV / mJy from catalogue',ylabel='NUV / mJy from image')\n",
    "ax1.set_title(r'using \\texttt{PHOTFNU}')\n",
    "\n",
    "\n",
    "# we are missing a factor 100 in the untis here\n",
    "ax2.scatter(associations['NUV_FLUX'],NUV_flam,s=1)\n",
    "ax2.plot([5e-19,5e-13],[5e-19,5e-13],color='black')\n",
    "ax2.set(xlim=[5e-19,5e-15],ylim=[5e-19,5e-15],xscale='log',yscale='log',\n",
    "        xlabel=r'NUV / erg s$^{-1}$ cm$^{-2}$ \\AA$^{-1}$ from catalogue',ylabel=r'NUV / erg s$^{-1}$ cm$^{-2}$ \\AA$^{-1}$ from image')\n",
    "ax2.set_title(r'using \\texttt{PHOTFLAM}')\n",
    "\n",
    "#fig.suptitle(f'{gal_name}, NUV (F275W) for {scalepc}pc associations')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(basedir/'reports'/f'remeasure_NUV_{gal_name}_{scalepc}pc.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the calibrated HST data contains two keywords to convert the counts from the image to physical units. Here we use the `spectral_density` function from astropy to see if both keywords behave the way we think they should (they should give the same result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(F275.meta['PHOTFNU']*u.Jy).to(u.erg/u.s/u.cm**2/u.Angstrom,equivalencies=u.spectral_density(2704*u.AA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F275.meta['PHOTFLAM']*u.erg/u.s/u.cm**2/u.Angstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.hist(associations['NUV_dolflux_mjy']/associations['NUV_dolflux_mjy_err'],bins=np.logspace(-1,4,20),\n",
    "        label='from catalogue',alpha=0.6)\n",
    "ax.hist(NUV_flam/NUV_flam_err,bins=np.logspace(-1,4,20),label='measured from mask',alpha=0.6)\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xscale='log',xlabel='NUV / mJy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compare Nebulae with Cluster catalogue and FUV\n",
    "\n",
    "### first we create regions to see how MUSE and HST overlapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "\n",
    "# find the contours around the image to use as vertices for the PixelRegion\n",
    "contours = find_contours(HII_mask.mask.astype(int),0.5)\n",
    "# we use the region with the most vertices\n",
    "coords = max(contours,key=len)\n",
    "# the coordinates from find_counters are switched compared to astropy\n",
    "reg_muse  = PolygonPixelRegion(vertices = PixCoord(*coords.T[::-1])) \n",
    "reg_sky  = reg_muse.to_sky(galaxy.wcs)\n",
    "reg_hst  = reg_sky.to_pixel(hst_whitelight.wcs)\n",
    "\n",
    "# check which objects are within another \n",
    "clusters['in_image']=reg_sky.contains(clusters['SkyCoord'],hst_whitelight.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot image\n",
    "ax = quick_plot(hst_whitelight.data,wcs=hst_whitelight.wcs,cmap=plt.cm.gray)\n",
    "# plot \n",
    "ax.scatter(clusters['X'][clusters['in_image']],clusters['Y'][clusters['in_image']],s=5,color='green')\n",
    "reg_hst.plot(ax=ax,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### now we can match the catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "\n",
    "# it makes a different if we match the clusters to the nebulae or the other way around\n",
    "matchcoord   = nebulae.copy()\n",
    "catalogcoord = clusters.copy()\n",
    "\n",
    "idx, sep, _ = match_coordinates_sky(matchcoord['SkyCoord'],catalogcoord['SkyCoord'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_name, y_name = 'AGE_MINCHISQ', 'ionization'\n",
    "\n",
    "matchcoord[y_name] = catalogcoord[idx][y_name]\n",
    "criteria  = sep.__lt__(Angle('1\"')) & ~np.isnan(matchcoord[x_name]) & ~np.isnan(matchcoord[y_name])\n",
    "catalogue = matchcoord[criteria]\n",
    "print(f'{len(criteria)} objects in catalogue ({np.sum(~criteria)} objects rejected)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic, pearsonr\n",
    "\n",
    "mean, bin_edges, binnumber = binned_statistic(catalogue[x_name],\n",
    "                                              catalogue[y_name],\n",
    "                                              statistic='mean',\n",
    "                                              bins=20,\n",
    "                                              range=[0,100])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "ax.errorbar((bin_edges[1:]+bin_edges[:-1])/2,mean,yerr=std,fmt='o')\n",
    "ax.set(xlabel=f'{x_name.replace(\"_\",\"\")}',ylabel=f'{y_name}')\n",
    "ax.set_title('binned statistics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky, search_around_sky\n",
    "#idxc, idxn, d2d, d3d = search_around_sky(nebulae['SkyCoord'],clusters['SkyCoord'],max_sep_ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat = clusters[(clusters['age']>0) & (clusters['mass']>5e3)]\n",
    "\n",
    "idx, sep2d, _ = match_coordinates_sky(nebulae['SkyCoord'],cat['SkyCoord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "catalogue = clusters[(clusters['age']>0) & (clusters['age']<10) & (clusters['mass']>0)]\n",
    "\n",
    "# define the maximum seperation\n",
    "max_sep_dis = 30 * u.pc\n",
    "max_sep_ang = 206265 * (max_sep_dis / Distance(distmod=galaxy.mu)).decompose() * u.arcsec\n",
    "\n",
    "# the new columns we want to calculate\n",
    "nearby_cluster = []       # list of clusters within max_sep_ang\n",
    "N_nearby_cluster = []     # number of nearby clusters\n",
    "sep = []                  # distance to the nearest cluster\n",
    "age = []                  # age of nearest cluster (not NaN)\n",
    "mass = []                 # mass of nearest cluster (not NaN)\n",
    "mass_dis = []             # distance weighted mass\n",
    "for row in nebulae:\n",
    "    \n",
    "    separation = row['SkyCoord'].separation(catalogue['SkyCoord'])\n",
    "    # mask for all clusters within max_sep_dis\n",
    "    mask = separation.__lt__(max_sep_ang)\n",
    "\n",
    "    cluster_sub = catalogue[mask]\n",
    "    \n",
    "    N_nearby_cluster.append(np.sum(mask))\n",
    "    nearby_cluster.append('')\n",
    "    \n",
    "    if len(cluster_sub) > 0:\n",
    "        sep.append(np.min(separation[mask]))\n",
    "        age.append(np.mean(cluster_sub['age']))\n",
    "        mass.append(np.sum(cluster_sub['mass']))\n",
    "        mass_dis.append(np.sum(cluster_sub['mass']/separation[mask]**2).value)\n",
    "    else:\n",
    "        sep.append(np.inf)\n",
    "        age.append(np.inf)\n",
    "        mass.append(0)\n",
    "        mass_dis.append(0)\n",
    "        \n",
    "#catalogue['nearby_clusters'] = nearby_cluster\n",
    "nebulae['N_nearby_cluster'] = N_nearby_cluster\n",
    "nebulae['sep'] = sep\n",
    "nebulae['age'] = age\n",
    "nebulae['mass'] = mass\n",
    "nebulae['mass_dis'] = mass_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nebulae[nebulae['N_nearby_cluster']>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_corner(table,rows,columns):\n",
    "    '''plot rows against columns'''\n",
    "    \n",
    "    if set(rows)-set(table.columns):\n",
    "        raise ValueError(f'missing column: {set(rows)-set(table.columns)}')\n",
    "    if set(columns)-set(table.columns):\n",
    "        raise ValueError(f'missing column: {set(columns)-set(table.columns)}')\n",
    "        \n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=len(rows),ncols=len(columns),\n",
    "                             sharex='col',sharey='row',\n",
    "                             figsize=(2*two_column,2*two_column*len(rows)/len(columns)))\n",
    "    \n",
    "    for i,col in enumerate(columns):\n",
    "        for j,row in enumerate(rows):\n",
    "            tmp = table[(table[col]>0)]\n",
    "            \n",
    "            ax = axes[j,i]\n",
    "            ax.scatter(tmp[col],tmp[row])\n",
    "                \n",
    "            if j==len(rows)-1:\n",
    "                ax.set_xlabel(col.replace('_',''))\n",
    "                ax.set_xscale('log')\n",
    "            if i%len(columns)==0:\n",
    "                ax.set_ylabel(row.replace('_',''))\n",
    "                #ax.set_ylim([np.nanpercentile(tmp[row],1),np.nanpercentile(tmp[row],99)])\n",
    "                \n",
    "    plt.subplots_adjust(hspace = .001)      \n",
    "    plt.subplots_adjust(wspace = .001)      \n",
    "\n",
    "    plt.show()\n",
    "            \n",
    "    \n",
    "plot_corner(nebulae[nebulae['N_nearby_cluster']>0],rows=['Z_local','ionization'],columns=['N_nearby_cluster','mass','age','mass_dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y=clusters['SkyCoord'].to_pixel(galaxy.wcs)\n",
    "clusters['x']=x\n",
    "clusters['y']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pnlf.detection import match_catalogues\n",
    "\n",
    "idx, sep = match_catalogues(clusters[['x','y']],nebulae[['x','y']])\n",
    "\n",
    "tolerance = 5\n",
    "within_tolerance = len(sep[sep<tolerance])\n",
    "\n",
    "print(f'{within_tolerance} of {len(sep)} clusters have an HII-region within {tolerance} pixel ({within_tolerance / len(sep)*100:.1f} %)')\n",
    "print(f'mean seperation is {sep[sep<tolerance].mean():.2f} pixel')\n",
    "\n",
    "clusters['sep'] = sep\n",
    "clusters['Z'] = nebulae[idx]['Z']\n",
    "clusters['ionization'] = nebulae[idx]['Z']\n",
    "clusters['Z_without_radial'] = nebulae[idx]['Z_local']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pnlf.plot.plot import create_RGB\n",
    "\n",
    "#HII = nebulae[nebulae['HII_region_class']==1]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = fig.add_subplot(projection=galaxy.wcs)\n",
    "\n",
    "norm = simple_norm(galaxy.OIII5006_DAP,clip=False,percent=99)\n",
    "#img = create_RGB(galaxy.HA6562,galaxy.OIII5006_DAP,galaxy.SII6716,weights=[0.6,1,0.6],percentile=[95,99.,95])\n",
    "#ax.imshow(galaxy.OIII5006_DAP,norm=norm,cmap=plt.cm.Blues)\n",
    "#ax.imshow(img)\n",
    "\n",
    "ax.scatter(clusters['x'],clusters['y'],marker='o',s=3,lw=0.3,color='black',alpha=0.9,label='cluster')\n",
    "im = ax.scatter(nebulae['x'],nebulae['y'],marker='o',s=5,lw=0.3,\n",
    "           c=nebulae['Delta_met'], cmap=plt.cm.coolwarm,alpha=0.9,label='nebulae')\n",
    "plt.colorbar(im)\n",
    "plt.legend()\n",
    "print(f'{len(nebulae)} HII regions\\n{len(clusters)} Clusters')\n",
    "\n",
    "filename = basedir / 'reports' / 'cluster' / 'clusters+HII.pdf'\n",
    "#plt.savefig(filename,bbox_inches='tight',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### combine regions with HST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from reproject import reproject_interp\n",
    "\n",
    "target = catalogue['SkyCoord'][0]\n",
    "\n",
    "cutout = Cutout2D(hst_whitelight.data,target,100*u.pix,wcs=hst_whitelight.wcs)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(projection=cutout.wcs)\n",
    "\n",
    "# the HST whitelight image\n",
    "norm = simple_norm(cutout.data,clip=False)\n",
    "ax.imshow(cutout.data,norm=norm,cmap=plt.cm.gray_r)\n",
    "\n",
    "# the mask from the nebulae catalogue\n",
    "hst_mask, _ = reproject_interp(HII_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')\n",
    "for i in np.unique(hst_mask[~np.isnan(hst_mask)]):\n",
    "    if i in nebulae['ID']:\n",
    "        hst_mask[hst_mask==i] = nebulae['ionization'][nebulae['ID']==i]\n",
    "    else:\n",
    "        hst_mask[hst_mask==i] = 0\n",
    "ax.imshow(hst_mask,alpha=0.5,cmap=plt.cm.Reds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Starburst99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from starburst99.main import Cluster  \n",
    "    from starburst99.io import read_starburst99\n",
    "except ImportError:\n",
    "    print('Package `starburst99` not installed')\n",
    "    \n",
    "star_path = Path('../../Starburst99/data')\n",
    "noutput = 'cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_cluster(folder,**kwargs):\n",
    "\n",
    "    parameters = {\n",
    "    \"name\" : 'standard',\n",
    "    \"isf\" : -1,\n",
    "    \"mass\" : 1.,\n",
    "    \"sfr\" : 1,\n",
    "    \"ninterv\" : 2,\n",
    "    \"xponent\" : '1.3,2.3',\n",
    "    \"xmaslim\" : '0.1,0.5,120',\n",
    "    \"sncut\" : 8.,\n",
    "    \"bhcut\" : 120.,\n",
    "    \"model\" : 64,\n",
    "    \"wind_model\" : 0,\n",
    "    \"tinitial\" : 0.01,\n",
    "    \"time_scale\" : 0,\n",
    "    \"time_step\" : 0.1,\n",
    "    \"n_steps\" : 1000,\n",
    "    \"tmax\" : 50,\n",
    "    \"jmg\" : 3,\n",
    "    \"atmos\" : 5,\n",
    "    \"metallicity\" : 3,\n",
    "    \"uvline\" : 1}\n",
    "\n",
    "    # assign the new parameters\n",
    "    for k,v in kwargs.items():\n",
    "        if k in parameters:\n",
    "            parameters[k] = v\n",
    "    \n",
    "    # open tempalte\n",
    "    with open(basedir/'data'/'input.template') as f:\n",
    "        template = f.read()\n",
    "    \n",
    "    template = template.format(**parameters)\n",
    "\n",
    "    #write to templae\n",
    "    with open(folder/'input.out','w') as f:\n",
    "        f.write(template)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from starburst99.main import Cluster\n",
    "        \n",
    "cluster = Cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from starburst99.io import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_cluster(filename,**kwargs):\n",
    "\n",
    "\n",
    "    parameters = {\n",
    "    \"name\" : 'standard',\n",
    "    \"isf\" : -1,\n",
    "    \"mass\" : 1.,\n",
    "    \"sfr\" : 1,\n",
    "    \"ninterv\" : 2,\n",
    "    \"xponent\" : '1.3,2.3',\n",
    "    \"xmaslim\" : '0.1,0.5,120',\n",
    "    \"sncut\" : 8.,\n",
    "    \"bhcut\" : 120.,\n",
    "    \"model\" : 64,\n",
    "    \"wind_model\" : 0,\n",
    "    \"tinitial\" : 0.01,\n",
    "    \"time_scale\" : 0,\n",
    "    \"time_step\" : 0.1,\n",
    "    \"n_steps\" : 1000,\n",
    "    \"tmax\" : 50,\n",
    "    \"jmg\" : 3,\n",
    "    \"atmos\" : 5,\n",
    "    \"metallicity\" : 3,\n",
    "    \"uvline\" : 1}\n",
    "\n",
    "    # assign the new parameters\n",
    "    for k,v in kwargs.items():\n",
    "        if k in parameters:\n",
    "            parameters[k] = v\n",
    "    \n",
    "    # open tempalte\n",
    "    with open(star_path/'input.template') as f:\n",
    "        template = f.read()\n",
    "    \n",
    "    template = template.format(**parameters)\n",
    "\n",
    "    #write to templae\n",
    "    with open(star_path/'input.out','w') as f:\n",
    "        f.write(template)\n",
    "        \n",
    "create_cluster(None,model=34,sncut=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster1e4 = Cluster(star_path / 'GENEVA_v40_0.014_1e4Msun' / noutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster1e4 = Cluster(star_path / 'GENEVA_v40_0.014_1e4Msun' / noutput)\n",
    "cluster1e6 = Cluster(star_path / 'GENEVA_v40_0.014_1e6Msun' / noutput)\n",
    "cluster_new = cluster1e4.scale(1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Compare results of the scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "tablename = 'yields'\n",
    "\n",
    "tbl1 = getattr(cluster1e4,tablename)\n",
    "tbl2 = getattr(cluster1e6,tablename)\n",
    "tbl3 = getattr(cluster_new,tablename)\n",
    "\n",
    "for col in tbl1.columns[1:]:\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    \n",
    "    ax.plot(tbl1['Time'],tbl1[col],color=color,ls='--',label=col.replace('_',''))\n",
    "    ax.plot(tbl2['Time'],tbl2[col],color=color,ls=':',lw=2)\n",
    "    ax.plot(tbl3['Time'],tbl3[col],color=color,lw=0.5)\n",
    "\n",
    "print('-- : 1e4, .. : 1e6, __ : scaled')\n",
    "#ax.set_yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "tablename = 'yields'\n",
    "column = 'O'\n",
    "tbl1 = getattr(cluster1e4,tablename)\n",
    "tbl2 = getattr(cluster1e6,tablename)\n",
    "tbl3 = getattr(cluster_new,tablename)\n",
    "\n",
    "\n",
    "\n",
    "color = next(ax._get_lines.prop_cycler)['color']\n",
    "    \n",
    "ax.plot(tbl1['Time'],tbl1[column],color=color,ls='--',label='1e4')\n",
    "ax.plot(tbl2['Time'],tbl2[column],color=color,ls=':',lw=2,label='1e6')\n",
    "ax.plot(tbl3['Time'],tbl3[column],color=color,lw=0.5,label='scaled')\n",
    "\n",
    "plt.legend()\n",
    "ax.set_yscale('log')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## IZI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from izi.izi import izi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flux=[3.44,2.286, 1, 0.553, 0.698, 2.83]\n",
    "error=[0.17, 0.07,0.05, 0.023, 0.027, 0.15]\n",
    "ID=['oii3726;oii3729','oiii4959;oiii5007', 'hbeta' , 'nii6548;nii6584', \\\n",
    "       'sii6717;sii6731', 'halpha']\n",
    "   \n",
    "out2=izi(flux, error, ID,\\\n",
    "    intergridfile='interpolgrid_50_50d13_kappa20',\n",
    "    epsilon=0.1, quiet=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flux=[3.44,2.286, 1]\n",
    "error=[0.17, 0.07,0.05]\n",
    "id=['oii3726;oii3729','oiii4959;oiii5007', 'hbeta' ]\n",
    "   \n",
    "out2=izi(flux, error, id,\\\n",
    "    intergridfile='interpolgrid_50_50d13_kappa20',\n",
    "    epsilon=0.1, quiet=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flux=[3.44,0.553]\n",
    "error=[0.17, 0.023]\n",
    "id=['oii3726;oii3729', 'nii6548;nii6584']\n",
    "   \n",
    "out2=izi(flux, error, id,\\\n",
    "    intergridfile='interpolgrid_50_50d13_kappa20',\n",
    "    epsilon=0.1, quiet=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flux=[0.553,  2.83]\n",
    "error=[0.023,  0.15]\n",
    "id=[ 'nii6548;nii6584', 'halpha']\n",
    "   \n",
    "out2=izi(flux, error, id,\\\n",
    "    intergridfile='interpolgrid_50_50d13_kappa20',\n",
    "    epsilon=0.1, quiet=False, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### IZI_mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from izi.izi_MCMC import izi_MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flux=[3.44,2.286, 1, 0.553, 0.698, 2.83]\n",
    "error=[0.17, 0.07,0.05, 0.023, 0.027, 0.15]\n",
    "id=['oii3726;oii3729','oiii4959;oiii5007', 'hbeta' , 'nii6548;nii6584', \\\n",
    "       'sii6717;sii6731', 'halpha']\n",
    "\n",
    "out2=izi_MCMC(flux, error, id,\\\n",
    "     intergridfile='interpolgrid_100_100d13_kappaINF', epsilon=0.1, quiet=False, \n",
    "     plot=True,plot_z_steps=False,plot_q_steps=False,\n",
    "     logqprior=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out2=izi_MCMC(flux, error, id,\n",
    "    intergridfile='interpolgrid_100_100d13_kappaINF', epsilon=0.1, quiet=False, \n",
    "    plot=False,logqprior=[7.1,0.5],plot_q_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "samples = out2.samples\n",
    "ndim=3\n",
    "\n",
    "# This is the empirical mean of the sample:\n",
    "value0 = np.median(samples, axis=0)\n",
    "valueup = np.percentile(samples,  66, axis=0)\n",
    "valuedown = np.percentile(samples, 34, axis=0)\n",
    "print(value0, valueup, valuedown)\n",
    "\n",
    "# Make the base corner plot\n",
    "figure = corner.corner(samples, \\\n",
    "                labels=[r\"$\\rm 12+log(O/H)$\", r\"$\\rm q \\ [cm^{-2}]$\",  \\\n",
    "                        r\"$ \\rm E(B-V)  \\ [mag]$\"],\\\n",
    "                show_titles=True,\n",
    "                title_kwargs={\"fontsize\": 10},\\\n",
    "                label_kwargs={\"fontsize\": 14},\\\n",
    "                data_kwargs={\"ms\": 0.6})\n",
    "\n",
    "# Extract the axes\n",
    "\n",
    "axes = np.array(figure.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over the diagonal\n",
    "for i in range(ndim):\n",
    "    ax = axes[i, i]\n",
    "\n",
    "    ax.axvline(value0[i], color=\"r\")\n",
    "    ax.axvline(valueup[i], color=\"r\", ls='--')\n",
    "    ax.axvline(valuedown[i], color=\"r\", ls='--')\n",
    "#    ax.axhline(value1[i], color=\"b\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### use on Nebulae catalogue\n",
    "\n",
    "the following two lines are not supported by IZI\n",
    " 'OI6363_FLUX', 'OII7330_FLUX''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from izi.izi import izi\n",
    "from izi.izi_MCMC import izi_MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names = {\n",
    " 'HB4861':'hbeta',\n",
    " 'OIII5006':'oiii5007',\n",
    " 'HA6562':'halpha',\n",
    " 'NII6583':'nii6584',\n",
    " 'SII6716':'sii6717',\n",
    " 'SII6730':'sii6731',\n",
    " 'SIII9068':'siii9068',\n",
    " 'OI6300':'oi6300',\n",
    " 'NII5754':'nii5755',\n",
    " 'HEI5875':'hei5875',\n",
    " 'SIII6312':'siii6312',\n",
    " 'OII7319':'oii7318',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_lines = ['HB4861','OIII5006','NII5754','OI6300','SIII6312',\n",
    "             'HA6562','NII6583','SII6716','SII6730','OII7319','SIII9068']\n",
    "small = ['HB4861','OIII5006','HA6562','NII6583','SII6716','SII6730','SIII9068']\n",
    "\n",
    "lines = small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ID    = [names[line] for line in lines]\n",
    "\n",
    "for num in range(5):\n",
    "    flux  = np.array(list(nebulae[[line+'_FLUX_CORR' for line in lines]][num]))\n",
    "    error = np.array(list(nebulae[[line+'_FLUX_CORR_ERR' for line in lines]][num]))\n",
    "\n",
    "    out=izi(flux,error, ID,plot=False,quiet=True)\n",
    "    print(f'NC: Z={nebulae[\"Z\"][num]:.2f} vs IZI: Z={out.sol[\"Z_joint\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ID = [names[line] for line in lines]\n",
    "\n",
    "for num in range(5):\n",
    "\n",
    "    flux  = np.array(list(nebulae[[line+'_FLUX' for line in lines]][num]))\n",
    "    error = np.array(list(nebulae[[line+'_FLUX_ERR' for line in lines]][num]))\n",
    "    \n",
    "    out_mcmc = izi_MCMC(flux,error,ID,plot=False,quiet=True)\n",
    "    print(f'NC: Z={nebulae[\"Z\"][num]:.2f} vs IZI: Z={out_mcmc.sol[\"Z\"]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MUSE HII-regions over HST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "muse_regions = Regions(mask=HII_mask.data,projection=HII_mask.meta,bkg=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hst_regions = muse_regions.reproject(hst_whitelight.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from skimage.measure import regionprops, find_contours\n",
    "from reproject import reproject_exact\n",
    "\n",
    "from pnlf.auxiliary import nanunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bkg = -1\n",
    "\n",
    "coords = []\n",
    "labels  = []\n",
    "for prop in regionprops(mask.data.astype(int)-bkg):\n",
    "    x,y = prop.coords.T\n",
    "    coords.append((x,y))\n",
    "    labels.append(prop.label+bkg)\n",
    "\n",
    "contours = find_contours(mask.data,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wcs = hst.wcs\n",
    "\n",
    "world = [mask.wcs.all_pix2world(np.array(c[::-1]).T,0) for c in coords]\n",
    "output_pix = []\n",
    "for c in world:\n",
    "    x,y = np.unique(wcs.all_world2pix(c,0).astype(int),axis=0).T[::-1]\n",
    "    output_pix.append((x,y))\n",
    "    \n",
    "hst_mask = np.zeros(hst.shape)*np.nan\n",
    "for r,l in zip(output_pix,labels):\n",
    "    hst_mask[r] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Plot one HII-region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos  = tbl['SkyCoord'][20]\n",
    "size = 20*u.arcsec\n",
    "\n",
    "HII  = Cutout2D(galaxy.HA6562,pos,size=size,wcs=galaxy.wcs)\n",
    "mask = Cutout2D(HII_mask.data,pos,size=size,wcs=HII_mask.wcs)\n",
    "mask.data[mask.data==-1]=np.nan\n",
    "hst  = Cutout2D(hst_whitelight.data,pos,size=size,wcs=hst_whitelight.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_exactp\n",
    "\n",
    "new_mask, footprint = reproject_exact((mask.data,mask.wcs),output_projection=hst.wcs,shape_out=hst.shape)\n",
    "hst_mask = np.zeros_like(new_mask)*np.nan\n",
    "for l in nanunique(mask.data):\n",
    "    hst_mask[new_mask==l] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "\n",
    "norm = simple_norm(HII.data,'linear',clip=False,percent=99)\n",
    "ax1  = fig.add_subplot(131,projection=HII.wcs)\n",
    "ax1.imshow(HII.data,norm=norm,cmap=plt.cm.hot)\n",
    "\n",
    "ax2  = fig.add_subplot(132,projection=mask.wcs)\n",
    "ax2.imshow(mask.data)\n",
    "\n",
    "norm = simple_norm(hst.data,'linear',clip=False,percent=99)\n",
    "ax3  = fig.add_subplot(133,projection=hst.wcs)\n",
    "ax3.imshow(hst.data,norm=norm,cmap=plt.cm.gray_r)\n",
    "ax3.imshow(hst_mask,alpha=0.3,cmap=plt.cm.Reds_r)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "filename = basedir / 'data' / 'interim' / f'Nebulae_Catalogue_with_FUV_DR2.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_catalogue = Table(hdul[1].data)\n",
    "\n",
    "# we remove some objects that are not HII-regions\n",
    "with np.errstate(divide='ignore'):\n",
    "    nebulae_catalogue['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae_catalogue['SII6716_FLUX_CORR']+nebulae_catalogue['SII6730_FLUX_CORR']\n",
    "    SIII = nebulae_catalogue['SIII6312_FLUX_CORR']+nebulae_catalogue['SIII9068_FLUX_CORR']\n",
    "    nebulae_catalogue[SII>0]['[SIII]/[SII]'] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae_catalogue['HA/FUV'] = nebulae_catalogue['HA6562_FLUX_CORR']/nebulae_catalogue['FUV_FLUX_CORR']\n",
    "    \n",
    "hst_sample      = set(['NGC0628', 'NGC1433', 'NGC1566', 'NGC3351', 'NGC3627', 'NGC4535'])\n",
    "muse_sample     = set(sample_table['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "from cluster.regions import find_sky_region\n",
    "from pnlf.auxiliary import filter_table\n",
    "\n",
    "def get_value(data,x,y):\n",
    "    shape = data.shape\n",
    "    \n",
    "    out = []\n",
    "    for j,i in zip(x,y):\n",
    "        if 0<i<shape[0] and 0<j<shape[1]:\n",
    "            out.append(data[int(i),int(j)])\n",
    "        else:\n",
    "            out.append(np.nan)\n",
    "    return out\n",
    "\n",
    "data_ext = Path('a:') \n",
    "\n",
    "for name in hst_sample:\n",
    "    \n",
    "    print(name)\n",
    "    nebulae = filter_table(nebulae_catalogue,gal_name=name)\n",
    "    HII_regions = filter_table(nebulae,BPT_NII=0,BPT_SII=0,BPT_OI=0)\n",
    "    \n",
    "    filename = data_ext / 'MUSE_DR2' / 'MUSEDAP' / f'{name}_MAPS.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                        meta=hdul['HA6562_FLUX'].header,\n",
    "                        wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "        OIII = NDData(data=hdul['OIII5006_FLUX'].data,\n",
    "                        uncertainty=StdDevUncertainty(hdul['OIII5006_FLUX_ERR'].data),\n",
    "                        mask=np.isnan(hdul['OIII5006_FLUX'].data),\n",
    "                        meta=hdul['OIII5006_FLUX'].header,\n",
    "                        wcs=WCS(hdul['OIII5006_FLUX'].header)) \n",
    "\n",
    "    filename = data_ext / 'MUSE_DR2' / 'Nebulae catalogue' /'spatial_masks'/f'{name}_HIIreg_mask.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        nebulae_mask = NDData(hdul[0].data-1,mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "        \n",
    "        \n",
    "    # cluster catalogues\n",
    "    filename = data_ext / 'HST' / 'cluster catalogue' / f'{name}_phangshst_base_catalog.fits'\n",
    "    with fits.open(filename) as hdul:\n",
    "        hst_all_objects = Table(hdul[1].data)\n",
    "\n",
    "    # remove LEGUS columns\n",
    "    hst_all_objects = hst_all_objects[[x for x in hst_all_objects.columns if 'LEGUS' not in x]]\n",
    "    # remove the PHANGS label from the column names\n",
    "    hst_all_objects.rename_columns([x for x in hst_all_objects.columns],[x.replace('PHANGS_','') for x in hst_all_objects.columns])\n",
    "    # add SkyCoord to match with MUSE data\n",
    "    hst_all_objects['SkyCoord'] = SkyCoord(hst_all_objects['RA']*u.degree,hst_all_objects['DEC']*u.degree)\n",
    "\n",
    "    reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "\n",
    "    hst_all_objects['in_frame'] = reg_muse_sky.contains(hst_all_objects['SkyCoord'],nebulae_mask.wcs)\n",
    "    hst_all_objects['MUSE_X'],hst_all_objects['MUSE_Y'] = hst_all_objects['SkyCoord'].to_pixel(nebulae_mask.wcs)\n",
    "\n",
    "    hst_all_objects['region_ID'] = np.nan\n",
    "    hst_all_objects['region_ID'][hst_all_objects['in_frame']] = get_value(nebulae_mask.data,x=hst_all_objects['MUSE_X'][hst_all_objects['in_frame']],y=hst_all_objects['MUSE_Y'][hst_all_objects['in_frame']])\n",
    "    # we exclude clusters in PNe and SNRs \n",
    "    hst_all_objects['region_ID'][~np.isin(hst_all_objects['region_ID'],HII_regions['region_ID'])] = np.nan\n",
    "\n",
    "    region_ID = hst_all_objects['region_ID']\n",
    "    print(f'{np.sum(~np.isnan(region_ID))} inside and {np.sum(np.isnan(region_ID))} outside of nebulae')\n",
    "    unique, counts = np.unique(region_ID[~np.isnan(region_ID)],return_counts=True)\n",
    "    isolated_clusters = filter_table(hst_all_objects[np.isin(hst_all_objects['region_ID'],unique[counts==1])],CLUSTER_CLASS=[1,2,3])\n",
    "    print(f'{len(isolated_clusters)} isolated clusters (no other objects in nebulae)')\n",
    "\n",
    "    isolated_nebulae = nebulae[np.isin(nebulae['region_ID'],isolated_clusters['region_ID'])]\n",
    "\n",
    "    catalogue = join(isolated_clusters,isolated_nebulae,keys='region_ID')\n",
    "\n",
    "    del catalogue['SkyCoord']\n",
    "\n",
    "    # write to file\n",
    "    primary_hdu = fits.PrimaryHDU()\n",
    "    table_hdu   = fits.BinTableHDU(catalogue)\n",
    "    hdul = fits.HDUList([primary_hdu, table_hdu])\n",
    "    hdul.writeto(basedir/'data'/'interim'/f'{name}_Clusters+Nebulae.fits',overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new table\n",
    "if 'catalogue' in locals(): del catalogue\n",
    "    \n",
    "for filename in [x for x in (basedir/'data'/'interim').iterdir() if  x.stem.endswith('Clusters+Nebulae')]:\n",
    "    \n",
    "    # cluster catalogues\n",
    "    with fits.open(filename) as hdul:\n",
    "        tbl = Table(hdul[1].data) \n",
    " \n",
    "    if 'catalogue' in locals():\n",
    "        catalogue = vstack([catalogue,tbl])\n",
    "    else:\n",
    "        catalogue = tbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D histogram with log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interpn\n",
    "\n",
    "# nebulae catalogue from Francesco (mostly HII-regions)\n",
    "with fits.open(data_ext / 'Products' / 'Nebulae catalogue' / 'Nebulae_catalogue_v2.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "    \n",
    "nebulae['[SIII]/[SII]'] = np.nan\n",
    "SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "SIII = nebulae['SIII6312_FLUX_CORR']+nebulae['SIII9068_FLUX_CORR']\n",
    "nebulae['[SIII]/[SII]'][SII>0] = SIII[SII>0]/SII[SII>0]\n",
    "nebulae['[SIII]/[SII]_err'] = np.sqrt( ( (nebulae['SIII6312_FLUX_CORR_ERR']**2+nebulae['SIII9068_FLUX_CORR_ERR']**2)*SII**2 + (nebulae['SII6716_FLUX_CORR_ERR']**2+nebulae['SII6730_FLUX_CORR_ERR']**2)*SIII**2) / SII**4)\n",
    "\n",
    "\n",
    "x = nebulae['[SIII]/[SII]']\n",
    "y = nebulae['Delta_met_scal']\n",
    "\n",
    "x,y = x[~np.isnan(x) & ~np.isnan(y)], y[~np.isnan(x) & ~np.isnan(y)]\n",
    "\n",
    "#x,y = x[~np.isnan(x) & ~np.isnan(y)], y[~np.isnan(x) & ~np.isnan(y)]\n",
    "\n",
    "xlim=[6e-2,6e-1]\n",
    "ylim=[-0.1,0.1]\n",
    "nbins=32\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column))\n",
    "\n",
    "bins = [np.logspace(*np.log10(xlim),nbins),np.linspace(*ylim,nbins)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=True)\n",
    "#hist[hist==0]=np.nan\n",
    "z = interpn((0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ),hist,np.vstack([x,y]).T,method=\"nearest\",bounds_error=False)\n",
    "idx = z.argsort()\n",
    "x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "#im = ax.imshow(hist.T,aspect='auto',origin='lower',extent=[*xlim,*ylim],cmap=plt.cm.gray_r)\n",
    "ax.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.gray_r)\n",
    "ax.scatter(x,y,c=z)\n",
    "\n",
    "\n",
    "ax.set(xscale='log',xlim=xlim,ylim=ylim)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
