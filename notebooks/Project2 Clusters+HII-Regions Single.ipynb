{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions Single <a class=\"tocSkip\">\n",
    "\n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues. \n",
    "    \n",
    "In this notebook we do the matching on a per galaxie basis. For each resolution, a set of output files is produced that matches the nebulae to the association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from astrotools.packages import *\n",
    "\n",
    "from astrotools.constants import tab10, single_column, two_column\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.basicConfig(stream=sys.stdout,datefmt='%H:%M:%S',level=logging.INFO)\n",
    "logger = logging.getLogger('pymuse')\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter(\"%(asctime)-15s %(message)s\",datefmt='%H:%M:%S')\n",
    "handler.setFormatter(fmt)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "basedir = Path('..')  # where we save stuff (and )\n",
    "data_ext = Path('a:')/'Archive' # raw data\n",
    "\n",
    "# we use the sample table for basic galaxy properties\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "the galaxies listed in `hst_sample` have a cluster catalogue. The galaxies listed in `muse_sample` have astrosat observations to measure the FUV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which version of the association catalogue to use\n",
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 32\n",
    "\n",
    "name = 'NGC7496'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE (DAP + nebulae catalogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import filter_table\n",
    "from pnlf.io import ReadLineMaps\n",
    "\n",
    "p = {x:sample_table.loc[name][x] for x in sample_table.columns}\n",
    "\n",
    "# DAP linemaps (Halpha and OIII)\n",
    "filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{name}*.fits'))\n",
    "copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    OIII = NDData(data=hdul['OIII5006_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['OIII5006_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['OIII5006_FLUX'].data),\n",
    "                    meta=hdul['OIII5006_FLUX'].header,\n",
    "                    wcs=WCS(hdul['OIII5006_FLUX'].header)) \n",
    "\n",
    "filename = data_ext/'Products'/'Nebulae_catalogs'/'Nebulae_catalogue_v2'/'spatial_masks'/f'{name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "    \n",
    "path = data_ext / 'MUSE'/'DR2.1' / 'filterImages' \n",
    "#sdss_g, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_g_WCS_Pall_mad.fits',header=True)\n",
    "#sdss_r, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_r_WCS_Pall_mad.fits',header=True)\n",
    "#sdss_i, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_i_WCS_Pall_mad.fits',header=True)\n",
    "    \n",
    "# the original catalogue from Francesco\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v3.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_dig.fits') as hdul:\n",
    "    dig = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_eq.fits') as hdul:\n",
    "    eq_width = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_age.fits') as hdul:\n",
    "    ages = Table(hdul[1].data)\n",
    "    \n",
    "nebulae = join(nebulae,fuv,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,eq_width,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,dig,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,ages,keys=['gal_name','region_ID'])\n",
    "nebulae.rename_columns(['cen_x','cen_y'],['x','y'])\n",
    "\n",
    "\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "    SIII = nebulae['SIII6312_FLUX_CORR']+nebulae['SIII9068_FLUX_CORR']\n",
    "    nebulae[SII>0]['[SIII]/[SII]'] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2+(nebulae['FUV_FLUX_CORR_ERR']/nebulae['FUV_FLUX_CORR'])**2)\n",
    "\n",
    "\n",
    "nebulae['HIIregion'] = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "HII_regions = filter_table(nebulae,gal_name=name,BPT_NII=0,BPT_SII=0,BPT_OI=0)\n",
    "nebulae = filter_table(nebulae,gal_name=name)\n",
    "nebulae.add_index('region_ID')\n",
    "HII_regions.add_index('region_ID')\n",
    "\n",
    "# WFI image (larger FOV)\n",
    "filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    WFI = NDData(data=hdul[0].data,\n",
    "                 meta=hdul[0].header,\n",
    "                 wcs=WCS(hdul[0].header))\n",
    "    \n",
    "# most of the time we do not need the datacubes\n",
    "if False:\n",
    "    #from spectral_cube import SpectralCube\n",
    "    filename = Path('g:') /'Archive'/'MUSE'/'DR2.1'/'datacubes'/f'{name}_DATACUBE_FINAL_WCS_Pall_mad.fits'\n",
    "    with fits.open(filename , memmap=True, mode='denywrite') as hdul:\n",
    "        data_cube   = hdul[1].data\n",
    "        cube_header = hdul[1].header   \n",
    "    \n",
    "print(f'{name}: {len(HII_regions)} HII-regions in final catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = next((data_ext/'MUSE'/'DR2.1'/'copt'/'MUSEDAP').glob(f'{name}*.fits'))\n",
    "copt_res = float(filename.stem.split('-')[1].split('asec')[0])\n",
    "with fits.open(filename) as hdul:\n",
    "    stellar_ebv = hdul['EBV_STARS'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stellar_ebv_muse = [stellar_ebv[int(x),int(y)] for x,y in nebulae[['x','y']]]\n",
    "stellar_ebv_muse = [np.nanmean(stellar_ebv[nebulae_mask.data==region_ID]) for region_ID in nebulae['region_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(5,5))\n",
    "\n",
    "lim = [0,1]\n",
    "ax.scatter(stellar_ebv_muse,nebulae['EBV'])\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "\n",
    "ax.set(xlim=lim,ylim=lim,xlabel=r'$E(B-V)$ stars MUSE',ylabel=r'$E(B-V)$ balmer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HST\n",
    "\n",
    "**white light + filter images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations, ReadHST\n",
    "\n",
    "target  = name.lower()\n",
    "\n",
    "# whitelight image (we set 0s to nan)\n",
    "white_light_filename = data_ext / 'HST' / 'white_light' / f'{name.lower()}_white_24rgb.fits'\n",
    "if white_light_filename.is_file():\n",
    "    with fits.open(white_light_filename) as hdul:\n",
    "        hst_whitelight = NDData(hdul[0].data,mask=hdul[0].data==0,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "        hst_whitelight.data[hst_whitelight.data==0] = np.nan\n",
    "else:\n",
    "    logging.warning('no white light image')\n",
    "    \n",
    "# filter image with uncertainties\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{name.lower()}_f275w_v1_err-drc-wht.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275.uncertainty = StdDevUncertainty(hdul[0].data)\n",
    "    \n",
    "associations, associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,\n",
    "                                                    HSTband=HSTband,scalepc=scalepc,version=version)\n",
    "\n",
    "\n",
    "print(f'{name}: {len(associations)} associations in catalogue')    \n",
    "# associations mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(data_ext/'ALMA'/'v4p0'/f'{name.lower()}_12m+7m+tp_co21_broad_tpeak.fits') as hdul:\n",
    "    CO = NDData(data=hdul[0].data,\n",
    "                meta=hdul[0].header,\n",
    "                wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Enviormental Masks\n",
    "\n",
    "```\n",
    "1 = center (small bulge, nuclear ring, nuclear disc, etc.)\n",
    "2 = bar (excluding bar ends)\n",
    "3 = bar ends (overlap of bar and spiral)\n",
    "4 = interbar (R_gal < R_bar, but outside bar footprint)\n",
    "5 = spiral arms inside interbar (R_gal < R_bar)\n",
    "6 = spiral arms (R_gal > R_bar)\n",
    "7 = interarm (only the R_gal spanned by spiral arms, and R_gal > R_bar)\n",
    "8 = outer disc (R_gal > spiral arm ends, only for galaxies with identified spirals)\n",
    "9 = interbar (“disc” where R_gal < R_bar) where no strong spiral arms were identified\n",
    "10 = disc (R_gal > R_bar) where no spiral arms were identified (e.g. flocculent spirals)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "with fits.open(data_ext/'environment_masks'/f'{name}_simple.fits') as hdul:\n",
    "    mask,footprint = reproject_interp(hdul[0],Halpha.meta,order='nearest-neighbor',return_footprint=True)\n",
    "    env_masks_neb = NDData(data=mask,\n",
    "                           meta=hdul[0].header,\n",
    "                           wcs=Halpha.wcs)\n",
    "    env_mask_org = hdul[0].data\n",
    "    \n",
    "    #mask = reproject_interp(hdul[0],associations_mask.meta,order='nearest-neighbor',return_footprint=False)\n",
    "    #env_masks_asc = NDData(data=mask,\n",
    "    #                       meta=hdul[0].header,\n",
    "    #                       wcs=associations_mask.wcs)\n",
    "    \n",
    "    \n",
    "environment_dict = {1 : 'center', 2 : 'bar', 3 : 'bar ends', 4 : 'interbar', \n",
    "                          5 : 'spiral arms',6 : 'spiral arms',7 : 'interarm' ,\n",
    "                          8 : 'outer disc', 9 : 'interbar', 10 : 'disc' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "create region objects from the mask. Slightly easier to use later but a bit complicated to construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "from functools import reduce\n",
    "\n",
    "def region_from_mask(mask):\n",
    "    \n",
    "    # otherwiese we have problems with the edge of the image\n",
    "    mask[:,0] = False\n",
    "    mask[:,-1] = False\n",
    "    mask[0,:] = False\n",
    "    mask[-1,:] = False\n",
    "    \n",
    "    contours = find_contours(mask.astype(float),level=0.5)\n",
    "    \n",
    "    regs = []\n",
    "    for contour in contours:\n",
    "        regs.append(PolygonPixelRegion(PixCoord(*contour.T[::-1])))\n",
    "     \n",
    "    return regs\n",
    "    #return reduce(lambda x,y:x&y,regs)\n",
    "\n",
    "environment_regions = {}\n",
    "for i in np.unique(env_masks_neb.data):\n",
    "    reg = region_from_mask(np.isin(env_masks_neb.data,i))\n",
    "    environment_regions[environment_dict[i]] = reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(8,8))\n",
    "\n",
    "norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "ax.imshow(Halpha.data,norm=norm,cmap=plt.cm.Greys,origin='lower')\n",
    "\n",
    "mask = (env_masks_neb.data==6).astype(float)\n",
    "mask[mask==0.] = np.nan\n",
    "ax.imshow(mask,alpha=0.5,origin='lower')\n",
    "\n",
    "for reg in environment_regions['spiral arms']:\n",
    "    patch = reg.as_artist()\n",
    "    ax.add_patch(patch)\n",
    "   \n",
    "ax.scatter(nebulae['x'],nebulae['y'],s=0.5)    \n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spiral_arms_pix = reduce(lambda x,y:x|y,environment_regions['spiral arms'])\n",
    "spiral_arms_sky = spiral_arms_pix.to_sky(env_masks_neb.wcs)\n",
    "\n",
    "nebulae['spiral_arms'] = spiral_arms_sky.contains(nebulae['SkyCoord'],wcs=Halpha.wcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Astrosat\n",
    "\n",
    "https://uvit.iiap.res.in/Instrument/Filters\n",
    "\n",
    "the resolution is 0.4\" per pixel. With a PSF resolution of 1.8\" this leads to fwhm ~ 4.5 px. This corresponds to a std = 1.91 px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitelight image\n",
    "#astrosat_int_time = ascii.read(data_ext/'Astrosat'/'integration_time.txt',delimiter='&')\n",
    "#astrosat_int_time.add_index('gal_name')\n",
    "astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F148W_flux_reproj.fits'\n",
    "astrosat_filter = 'F148W'\n",
    "if not astro_file.is_file():\n",
    "    print(f'no filter F148W for {name}')\n",
    "    astrosat_filter = 'F154W'\n",
    "    astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        astrosat_filter = None\n",
    "        print(f'no astrosat file for {name}')\n",
    "\n",
    "#astro_file = data_ext / 'Astrosat' / 'NGC4254_NUV_N242W_flux_reproj.fits'\n",
    "with fits.open(astro_file) as hdul:\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    for row in hdul[0].header['COMMENT']:\n",
    "        if row.startswith('CTSTOFLUX'):\n",
    "            _,CTSTOFLUX = row.split(':')\n",
    "            CTSTOFLUX = float(CTSTOFLUX)\n",
    "        if row.startswith('IntTime'):\n",
    "            _,IntTime = row.split(':')\n",
    "            IntTime = float(IntTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astrosat_to_muse = (astrosat.wcs.pixel_scale_matrix[0][0] / Halpha.wcs.pixel_scale_matrix[0][0])**2\n",
    "astrosat_to_muse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Equivalent Width\n",
    "\n",
    "the first step is to extract the spectra of each HII-region.\n",
    "\n",
    "Are the spectra continuum subtracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "\n",
    "filename = data_ext / 'Products' / 'Nebulae catalogue' /'spectra'/f'{name}_VorSpectra.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    spectra = Table(hdul[1].data)\n",
    "    spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    \n",
    "spectra['region_ID'] = np.arange(len(spectra))\n",
    "spectra.add_index('region_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H0 = 67 * u.km / u.s / u.Mpc\n",
    "z = (H0*Distance(distmod=p['(m-M)'])/c.c).decompose()\n",
    "lam_HA0 = 6562.8*u.Angstrom\n",
    "lam_HA = (1+z)*lam_HA0\n",
    "\n",
    "lam_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.spectrum import fit_emission_line\n",
    "\n",
    "region_ID = 10\n",
    "#lam_HA = 6595*u.Angstrom\n",
    "filename = basedir/'reports'/name/f'{name}_eqwidth.png'\n",
    "flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "fit = fit_emission_line(spectral_axis,flux,lam_HA,filename=filename)\n",
    "integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "\n",
    "print(f\"f_cat/f_fit = {nebulae.loc[region_ID]['HA6562_FLUX']/integrated_flux.value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#lam_HA = 6595*u.Angstrom\n",
    "\n",
    "HA = []\n",
    "HII_regions['eq_width'] = np.nan\n",
    "for region_ID in tqdm(HII_regions['region_ID']):\n",
    "    \n",
    "    flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "    fit = fit_emission_line(spectral_axis,flux,lam_HA,plot=False)\n",
    "    integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "    continuum = fit.c0_1 * u.erg/u.s/u.cm**2/u.Angstrom\n",
    "    eq_width = integrated_flux/continuum\n",
    "    eq_width = HII_regions.loc[region_ID]['HA6562_FLUX']/continuum\n",
    "    HII_regions.loc[region_ID]['eq_width'] = eq_width.value\n",
    "    \n",
    "    HA.append(integrated_flux)\n",
    "    #HA_cat = nebulae.loc[region_ID]['HA6562_FLUX']\n",
    "    #print(f'{integrated_flux/HA_cat:.2f}')\n",
    "    #print(f'HA = {fit.mean_0.value:.2f}')\n",
    "HA = np.array([x.value for x in HA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(HA,HII_regions['HA6562_FLUX']/2.5)\n",
    "x = np.linspace(0,2e6)\n",
    "ax.plot(x,x,color='black')\n",
    "ax.set(ylim=[0,2e6],xlim=[0,2e6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HST and MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare footprints of different observations\n",
    "\n",
    "here we compare the footprints of the observations and check which objects overlap. The FOV of astrosat is a circle with a diameter of 28'. This is much larger than HST and MUSE and both will always be covered by the astrosat observations\n",
    "\n",
    "NGC0628\n",
    "987 (of 1077) associations in MUSE FOV\n",
    "2563 (of 2869) nebulae in HST FOV\n",
    "\n",
    "NGC1365\n",
    "499 (of 518) associations in MUSE FOV\n",
    "907 (of 1455) nebulae in HST FOV\n",
    "\n",
    "NGC1433\n",
    "495 (of 496) associations in MUSE FOV\n",
    "1011 (of 1736) nebulae in HST FOV\n",
    "\n",
    "NGC1566\n",
    "1584 (of 1653) associations in MUSE FOV\n",
    "2112 (of 2404) nebulae in HST FOV\n",
    "\n",
    "NGC3351\n",
    "710 (of 836) associations in MUSE FOV\n",
    "1179 (of 1284) nebulae in HST FOV\n",
    "\n",
    "\n",
    "NGC3627\n",
    "1330 (of 1415) associations in MUSE FOV\n",
    "1635 (of 1635) nebulae in HST FOV\n",
    "\n",
    "NGC4535\n",
    "475 (of 647) associations in MUSE FOV\n",
    "1555 (of 1938) nebulae in HST FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.regions import find_sky_region\n",
    "\n",
    "reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "reg_hst_pix, reg_hst_sky = find_sky_region(hst_whitelight.mask.astype(int),wcs=hst_whitelight.wcs)\n",
    "\n",
    "# check which nebulae/clusters are within the HST/MUSE FOV\n",
    "associations['in_frame'] = reg_muse_sky.contains(associations['SkyCoord'],nebulae_mask.wcs)\n",
    "#clusters['in_frame'] = reg_muse_sky.contains(clusters['SkyCoord'],nebulae_mask.wcs)\n",
    "nebulae['in_frame']  = reg_hst_sky.contains(nebulae['SkyCoord'],nebulae_mask.wcs)\n",
    "\n",
    "print(f'{np.sum(associations[\"in_frame\"])} (of {len(associations)}) associations in MUSE FOV')\n",
    "#print(f'{np.sum(clusters[\"in_frame\"])} (of {len(clusters)}) clusters in MUSE FOV')\n",
    "print(f'{np.sum(nebulae[\"in_frame\"])} (of {len(nebulae)}) nebulae in HST FOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=6*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "ax = quick_plot(WFI_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/'footpring.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association and nebulae\n",
    "\n",
    "the association catalogue differs from the clusters in that its entries are extended. Because we match two catalogues with extended objects, we must proceed differently.\n",
    "\n",
    "In a first step we take a look at a single association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "\n",
    "assoc_ID = 10\n",
    "pos = associations['SkyCoord'][associations['assoc_ID']==assoc_ID]\n",
    "\n",
    "contours = find_contours(associations_mask.data==assoc_ID,0.5,)\n",
    "coords = max(contours,key=len)\n",
    "\n",
    "# the coordinates from find_counters are switched compared to astropy\n",
    "reg_pix  = PolygonPixelRegion(vertices = PixCoord(*coords.T[::-1])) \n",
    "reg_sky  = reg_pix.to_sky(associations_mask.wcs)\n",
    "\n",
    "mask_cutout = Cutout2D(associations_mask.data,pos,size=1*u.arcsecond,wcs=associations_mask.wcs)\n",
    "F275_cutout = Cutout2D(F275.data,pos,size=2*u.arcsecond,wcs=F275.wcs)\n",
    "\n",
    "reg_pix_cut  = reg_sky.to_pixel(mask_cutout.wcs)\n",
    "\n",
    "ax = quick_plot(F275_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "ax.imshow(mask_cutout.data,alpha=0.5)\n",
    "\n",
    "reg_pix_cut.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to match the nebulae to the associations we first reproject the mask of the nebulae to the HST image. We then scale the association mask by the number of associations (assume we have 1432 objects, then 615 becomes 0.0615). This way we can add the two masks together and infer from the resulting unique values which clusters overlap with which associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match catalogues\n",
    "\n",
    "and make some plots that showcase the position/overlap\n",
    "\n",
    "```\n",
    "# NGC3627 is too large to reproject\n",
    "#center = SkyCoord(sample_table.loc[name]['R.A.'],sample_table.loc[name]['Dec.'])\n",
    "#cutout = Cutout2D(associations_mask.data,center,size=(5.5*u.arcmin,3*u.arcmin),wcs=associations_mask.wcs)\n",
    "cutout = Cutout2D(associations_mask.data,(3000,4800),size=(9000,5500),wcs=associations_mask.wcs)\n",
    "\n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=cutout.wcs,\n",
    "                                   shape_out=cutout.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "scale = 10**np.ceil(np.log10(max(cutout.data[~np.isnan(cutout.data)])))\n",
    "s_arr = cutout.data/scale+nebulae_hst\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "# reproject nebulae mask to hst \n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=associations_mask.wcs,\n",
    "                                   shape_out=associations_mask.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "\n",
    "# we scale the associations such that the the id is in the decimal\n",
    "scale = 10**np.ceil(np.log10(max(associations_mask.data[~np.isnan(associations_mask.data)])))\n",
    "s_arr = associations_mask.data/scale+nebulae_hst\n",
    "\n",
    "header = associations_mask.wcs.to_header()\n",
    "header['scale'] = scale\n",
    "#hdu = fits.PrimaryHDU(s_arr,header=header)\n",
    "#hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_map.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids of associations, nebulae and combination (sum) of both\n",
    "a_id = np.unique(associations_mask.data[~np.isnan(associations_mask.data)]).astype(int)\n",
    "n_id = np.unique(nebulae_mask.data[~np.isnan(nebulae_mask.data)]).astype(int)\n",
    "s_id = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "# this splits the sum into two parts (nebulae and associations)\n",
    "a_modf,n_modf = np.modf(s_id)\n",
    "n_modf = n_modf.astype(int)\n",
    "a_modf = np.round(a_modf*scale).astype(int)\n",
    "\n",
    "unique_a, count_a = np.unique(a_modf,return_counts=True)\n",
    "unique_n, count_n = np.unique(n_modf,return_counts=True)\n",
    "\n",
    "nebulae_dict = {int(n) : a_modf[n_modf==n].tolist() for n in n_id}     \n",
    "associations_dict = {int(a) : n_modf[a_modf==a].tolist() for a in a_id}     \n",
    "\n",
    "\n",
    "# so far we ensured that the nebulae in unique_n have only one association,\n",
    "# but it is possible that this association goes beyond the nebulae and into\n",
    "# a second nebulae. Those objects are excluded here\n",
    "isolated_nebulae = set()\n",
    "isolated_assoc   = set()\n",
    "for n,v in nebulae_dict.items():\n",
    "    if len(v)==1:\n",
    "        if len(associations_dict[v[0]])==1:\n",
    "            isolated_nebulae.add(n)\n",
    "            isolated_assoc.add(v[0])\n",
    "            \n",
    "print(f'n_associations = {len(associations_dict)}')\n",
    "print(f'n_nebulae      = {len(nebulae_dict)}')\n",
    "print(f'1to1 match     = {len(isolated_nebulae)}')\n",
    "\n",
    "\n",
    "# we save those two dicts so we do not have to redo this everytime\n",
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_nebulae.yml','w+') as f:\n",
    "    yaml.dump(nebulae_dict,f)\n",
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations.yml','w+') as f:\n",
    "    yaml.dump(associations_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all assoc that have at least one pixel outside of the nebulae masks\n",
    "mask = associations_mask.data.copy()\n",
    "mask[~np.isnan(nebulae_hst)] = np.nan\n",
    "outside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "# find all assoc that have at least one pixel inside of the nebulea masks\n",
    "mask = associations_mask.data.copy()\n",
    "mask[np.isnan(nebulae_hst)] = np.nan\n",
    "inside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "contained = np.setdiff1d(inside,outside)\n",
    "partial   = np.intersect1d(inside,outside)\n",
    "isolated  = np.setdiff1d(outside,inside)\n",
    "\n",
    "print(f'contained: {len(contained)}\\npartial: {len(partial)}\\nisolated: {len(isolated)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_tmp = associations[['assoc_ID']].copy()\n",
    "assoc_tmp.add_index('assoc_ID')\n",
    "\n",
    "x_asc,y_asc = associations['SkyCoord'].to_pixel(env_masks_neb.wcs)\n",
    "outside = (x_asc > env_masks_neb.data.shape[1]) | (y_asc > env_masks_neb.data.shape[0])\n",
    "x_asc[outside] = 0\n",
    "y_asc[outside] = 0\n",
    "assoc_tmp['env_asc'] = [environment_dict[env_masks_neb.data[y,x]] for \n",
    "                          x,y in zip(x_asc.astype(int),y_asc.astype(int))]\n",
    "assoc_tmp[outside]['env_asc'] = ''\n",
    "\n",
    "assoc_tmp['overlap'] = np.empty(len(associations),dtype='U9')\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],contained)] = 'contained'\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],partial)]   = 'partial'\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],isolated)]  = 'isolated'\n",
    "assoc_tmp['1to1'] = False\n",
    "assoc_tmp['1to1'][np.isin(assoc_tmp['assoc_ID'],list(isolated_assoc))] = True\n",
    "assoc_tmp['Nnebulae'] = [len(associations_dict[k]) for k in assoc_tmp['assoc_ID']]\n",
    "\n",
    "assoc_tmp['region_ID'] = np.nan\n",
    "assoc_tmp['region_ID'][assoc_tmp['1to1']] = [associations_dict[k][0] for k in assoc_tmp[assoc_tmp['1to1']]['assoc_ID']]\n",
    "\n",
    "overlap = join(\n",
    "    Table(np.unique(associations_mask.data[~np.isnan(associations_mask.data)],return_counts=True),names=['assoc_ID','size']),\n",
    "    Table(np.unique(associations_mask.data[~np.isnan(nebulae_hst) & ~np.isnan(associations_mask.data)],return_counts=True),names=['assoc_ID','overlap_size']),\n",
    "    keys=['assoc_ID'],join_type='outer')\n",
    "overlap = overlap.filled(0)\n",
    "overlap['overlap_asc'] = overlap['overlap_size']/overlap['size']\n",
    "overlap['overlap_asc'].info.format = '%.2f'\n",
    "assoc_tmp = join(assoc_tmp,overlap[['assoc_ID','overlap_asc']],keys='assoc_ID')\n",
    "\n",
    "print('write to file')\n",
    "hdu = fits.BinTableHDU(assoc_tmp,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.regions import find_neighbors\n",
    "from tqdm import tqdm \n",
    "\n",
    "nebulae_tmp = nebulae[['region_ID','x','y']].copy()\n",
    "nebulae_tmp.add_index('region_ID')\n",
    "\n",
    "nebulae_tmp['env_neb'] = [environment_dict[env_masks_neb.data[y,x]] for \n",
    "                          x,y in zip(nebulae_tmp['x'].astype(int),nebulae_tmp['y'].astype(int))]\n",
    "\n",
    "nebulae_tmp['neighbors'] = np.nan\n",
    "for row in tqdm(nebulae_tmp):\n",
    "    row['neighbors'] = len(find_neighbors(nebulae_mask.data,tuple(row[['x','y']]),row['region_ID'],plot=False))\n",
    "del nebulae_tmp[['x','y']]\n",
    "\n",
    "nebulae_tmp['1to1'] = False\n",
    "nebulae_tmp['1to1'][np.isin(nebulae_tmp['region_ID'],list(isolated_nebulae))] = True\n",
    "nebulae_tmp['Nassoc'] = [len(nebulae_dict[k]) for k in nebulae_tmp['region_ID']]\n",
    "nebulae_tmp['assoc_ID'] = np.nan\n",
    "nebulae_tmp['assoc_ID'][nebulae_tmp['1to1']] = [nebulae_dict[k][0] for k in nebulae_tmp[nebulae_tmp['1to1']]['region_ID']]\n",
    "\n",
    "\n",
    "overlap = join(\n",
    "    Table(np.unique(nebulae_hst[~np.isnan(nebulae_hst)],return_counts=True),names=['region_ID','size']),\n",
    "    Table(np.unique(nebulae_hst[~np.isnan(nebulae_hst) & ~np.isnan(associations_mask.data)],return_counts=True),names=['region_ID','overlap_size']),\n",
    "    keys=['region_ID'],join_type='outer')\n",
    "overlap = overlap.filled(0)\n",
    "overlap['overlap_neb'] = overlap['overlap_size']/overlap['size']\n",
    "overlap['overlap_neb'].info.format = '%.2f'\n",
    "nebulae_tmp = join(nebulae_tmp,overlap[['region_ID','overlap_neb']],keys='region_ID')\n",
    "\n",
    "\n",
    "hdu = fits.BinTableHDU(nebulae_tmp,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_nebulae.fits',overwrite=True)\n",
    "#del nebulae_tmp['1to1']\n",
    "\n",
    "print(f'{np.sum(nebulae_tmp[\"neighbors\"]==0)} nebulae have no neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "catalogue = join(assoc_tmp,nebulae_tmp,keys=['assoc_ID','region_ID'])\n",
    "catalogue = join(catalogue,nebulae,keys='region_ID')\n",
    "catalogue = join(catalogue,associations,keys='assoc_ID')\n",
    "\n",
    "# pay attention to the order of assoc, neb\n",
    "catalogue.rename_columns(['X','Y','x','y','RA','DEC','cen_ra','cen_dec',\n",
    "                          'reg_area','region_area',\n",
    "                          'EBV_1','EBV_2','EBV_err','EBV_ERR',\n",
    "                          'SkyCoord_1','SkyCoord_2'],\n",
    "                         ['x_asc','y_asc','x_neb','y_neb','ra_asc','dec_asc','ra_neb','dec_neb',\n",
    "                          'area_asc','area_neb',\n",
    "                          'EBV_balmer','EBV_stars','EBV_balmer_err','EBV_stars_err',\n",
    "                          'SkyCoord_asc','SkyCoord_neb'])\n",
    "\n",
    "# separation to other associations and nebulae\n",
    "idx,sep_asc,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_neb,_= match_coordinates_sky(catalogue['SkyCoord_neb'],nebulae['SkyCoord'],nthneighbor=2)\n",
    "catalogue['sep_asc'] = sep_asc.to(u.arcsec)\n",
    "catalogue['sep_neb'] = sep_neb.to(u.arcsec)\n",
    "\n",
    "# select the columns of the joined catalogue\n",
    "columns = ['assoc_ID','region_ID','x_asc','y_asc','x_neb','y_neb',\n",
    "           'ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_asc','SkyCoord_neb',\n",
    "           'env_asc','env_neb','area_asc','area_neb',\n",
    "           'sep_asc','sep_neb','neighbors','Nassoc','overlap','overlap_asc','overlap_neb',\n",
    "           'age','age_err','mass','mass_err','EBV_stars','EBV_stars_err','EBV_balmer','EBV_balmer_err',\n",
    "           'met_scal','met_scal_err','logq_D91','logq_D91_err',] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR')] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR_ERR')] + \\\n",
    "            ['NUV_FLUX','NUV_FLUX_ERR','U_FLUX','U_FLUX_ERR','B_FLUX','B_FLUX_ERR',\n",
    "             'V_FLUX','V_FLUX_ERR','I_FLUX','I_FLUX_ERR'] + \\\n",
    "            ['HA/FUV','eq_width']\n",
    "catalogue = catalogue[columns]\n",
    "        \n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR')],\n",
    "                      [col.replace('FLUX_CORR','flux') for col in catalogue.columns if col.endswith('FLUX_CORR')])\n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')],\n",
    "                      [col.replace('FLUX_CORR_ERR','flux_err') for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')])\n",
    "catalogue['assoc_ID'] = catalogue['assoc_ID'].astype('int')\n",
    "catalogue['region_ID'] = catalogue['region_ID'].astype('int')\n",
    "\n",
    "catalogue.info.description = 'Joined catalogue between associations and nebulae'\n",
    "mean_sep = np.mean(catalogue['SkyCoord_asc'].separation(catalogue['SkyCoord_neb']))\n",
    "print(f'{len(catalogue)} objects in catalogue')\n",
    "print(f'the mean separation between cluster and association center is {mean_sep.to(u.arcsecond):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Write\n",
    "export parts of the joined catalogue (right now only the fully contained objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "export = catalogue.copy() #[catalogue['contained']]\n",
    "#export.add_column(export['SkyCoord_asc'].to_string(style='hmsdms',precision=2),index=6,name='RaDec_asc')\n",
    "#export.add_column(export['SkyCoord_neb'].to_string(style='hmsdms',precision=2),index=8,name='RaDec_neb')\n",
    "\n",
    "RA_asc ,DEC_asc = zip(*[x.split(' ') for x in export['SkyCoord_asc'].to_string(style='hmsdms',precision=2)])\n",
    "RA_neb ,DEC_neb = zip(*[x.split(' ') for x in export['SkyCoord_neb'].to_string(style='hmsdms',precision=2)])\n",
    "\n",
    "export.add_column(RA_asc,index=6,name='Ra_asc')\n",
    "export.add_column(DEC_asc,index=8,name='Dec_asc')\n",
    "export.add_column(RA_neb,index=10,name='Ra_neb')\n",
    "export.add_column(DEC_neb,index=12,name='Dec_neb')\n",
    "\n",
    "for col in export.columns:\n",
    "    if col not in ['Ra_asc','Dec_asc','Ra_neb','Dec_neb','region_ID','cluster_ID','overlap','env_asc','env_neb']:\n",
    "        export[col].info.format = '%.2f'\n",
    "\n",
    "del export[['ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_neb','SkyCoord_asc','HA/FUV']]\n",
    "\n",
    "hdu = fits.BinTableHDU(export,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations_and_nebulae_joined.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_nebulae.yml') as f:\n",
    "    nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "    associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    \n",
    "filename = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations.fits'\n",
    "assoc_tmp = Table(fits.getdata(filename,ext=1))\n",
    "associations=join(associations,assoc_tmp,keys='assoc_ID')\n",
    "associations.add_index('assoc_ID')\n",
    "\n",
    "filename = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_nebulae.fits'\n",
    "nebulae_tmp = Table(fits.getdata(filename,ext=1))\n",
    "nebulae=join(nebulae,nebulae_tmp,keys='region_ID')\n",
    "nebulae.add_index('region_ID')\n",
    "# read in existing catalogues\n",
    "filename = basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations_and_nebulae_joined.fits'\n",
    "catalogue = Table(fits.getdata(filename,ext=1))\n",
    "\n",
    "catalogue['SkyCoord_asc'] = SkyCoord(catalogue['Ra_asc'],catalogue['Dec_asc'])\n",
    "catalogue['SkyCoord_neb'] = SkyCoord(catalogue['Ra_neb'],catalogue['Dec_neb'])\n",
    "#catalogue['HA/FUV'] = catalogue['HA6562_flux']/catalogue['FUV_flux']\n",
    "#catalogue['HA/FUV_err'] = catalogue['HA/FUV']*np.sqrt((catalogue['HA6562_flux_err']/catalogue['HA6562_flux'])**2+(catalogue['FUV_flux_err']/catalogue['FUV_flux'])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save cutout for each region in seperate fits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp \n",
    "\n",
    "def save_cutouts(position,size=4*u.arcsec):\n",
    "    \n",
    "    header = fits.Header()\n",
    "    header['gal_name'] = name\n",
    "    header['RA'] = row['SkyCoord_asc'].ra.to(u.degree).value\n",
    "    header['DEC'] = row['SkyCoord_asc'].dec.to(u.degree).value\n",
    "    header['RADESYS'] = 'ICRS'\n",
    "    header['regionID'] = row['region_ID']\n",
    "    header['assocID'] = row['cluster_ID']\n",
    "\n",
    "    hdul = fits.HDUList([fits.PrimaryHDU(header=header)])\n",
    "\n",
    "    # save HST image\n",
    "    cutout = Cutout2D(F275.data,position=position,size=size,wcs=F275.wcs)\n",
    "    hdul.append(fits.ImageHDU(cutout.data,header=cutout.wcs.to_header(),name='F275'))\n",
    "\n",
    "    # save Halpha\n",
    "    Halpha_cutout, _  = reproject_interp(Halpha,output_projection=cutout.wcs,shape_out=cutout.shape,order='bilinear')    \n",
    "    hdul.append(fits.ImageHDU(Halpha_cutout,header=cutout.wcs.to_header(),name='Halpha'))\n",
    "\n",
    "\n",
    "    # save OIII\n",
    "    OIII_cutout, _  = reproject_interp(OIII,output_projection=cutout.wcs,shape_out=cutout.shape,order='bilinear')    \n",
    "    hdul.append(fits.ImageHDU(OIII_cutout,header=cutout.wcs.to_header(),name='OIII'))\n",
    "\n",
    "\n",
    "    # save nebulae mask\n",
    "    nebulae_cutout, _  = reproject_interp(nebulae_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')    \n",
    "    hdul.append(fits.ImageHDU(nebulae_cutout,header=cutout.wcs.to_header(),name='nebulae'))\n",
    "\n",
    "\n",
    "    # save association mask\n",
    "    assoc_cutout, _  = reproject_interp(associations_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')    \n",
    "    hdul.append(fits.ImageHDU(assoc_cutout,header=cutout.wcs.to_header(),name='assoc'))\n",
    "\n",
    "    hdul.writeto(basedir/'data'/'cutouts'/f'{name}_region{row[\"region_ID\"]}.fits',overwrite=True,checksum=True)\n",
    "    \n",
    "    \n",
    "row = catalogue[0]\n",
    "position = row['SkyCoord_neb']\n",
    "\n",
    "save_cutouts(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple associations inside one nebulae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'nebulae in galaxy: {len(nebulae)}')\n",
    "print(f'HII regions in galaxy: {len(HII_regions)}')\n",
    "print(f'associations in galaxy: {len(associations)}')\n",
    "\n",
    "print(f'nebulae that overlap with association: {np.sum(nebulae[\"overlap_neb\"]>0)}')\n",
    "print(f'associations that overlap with nebulae: {np.sum(associations[\"overlap_asc\"]>0)}')\n",
    "print(f'one-to-one relation: {np.sum(associations[\"1to1\"])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_nebulae.yml') as f:\n",
    "    nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "with open(basedir/'data'/'map_nebulae_association'/version/HSTband/f'{scalepc}pc'/f'{name}_{HSTband}_{scalepc}pc_associations.yml') as f:\n",
    "    associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    \n",
    "age_diff = []\n",
    "ovl_diff = []\n",
    "mass_diff = []\n",
    "mass_div = []\n",
    "\n",
    "candidates = []\n",
    "\n",
    "# we select each nebulae with more than one HII region\n",
    "for region_ID in nebulae_tmp[nebulae_tmp['Nassoc']>1]['region_ID']:\n",
    "    for assoc_ID in nebulae_dict[region_ID]:\n",
    "        \n",
    "        # if one of the associations also overlaps with another nebulae we reject it\n",
    "        if len(associations_dict[assoc_ID])>1:\n",
    "            break\n",
    "        # we want a certain amount of overlap\n",
    "        if associations.loc[assoc_ID]['overlap_asc'] <0.0:\n",
    "            break\n",
    "        #if associations.loc[assoc_ID]['mass'] <1e4:\n",
    "        #    break\n",
    "    else:\n",
    "        candidates.append(region_ID)\n",
    "        if len(nebulae_dict[region_ID])==2:\n",
    "            sub = associations.loc[nebulae_dict[region_ID]]\n",
    "            sub.sort('mass')\n",
    "            \n",
    "            age_diff.append(np.diff(sub['age'])[0])\n",
    "            mass_diff.append(np.diff(sub['mass'])[0])\n",
    "            mass_div.append(sub['mass'][1]/sub['mass'][0])\n",
    "\n",
    "print(f'{len(candidates)} of {np.sum(nebulae_tmp[\"Nassoc\"]>1)} match the criteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "\n",
    "# we select each nebulae with more than one HII region\n",
    "for region_ID in nebulae_tmp[nebulae_tmp['Nassoc']>1]['region_ID']:\n",
    "    sub = associations.loc[nebulae_dict[region_ID]]\n",
    "    # first we check if we can ignore the other associations in the nebulae\n",
    "    if np.sum((sub['age']<10) & (sub['mass']>1e4)) == 1:\n",
    "        # then we make sure that the one associations also overlaps sufficiently\n",
    "        if np.sum((sub['age']<10) & (sub['mass']>1e4) & (sub['overlap_asc']>0.5)) == 1:\n",
    "            candidates.append(region_ID)\n",
    "        \n",
    "print(f'{len(candidates)} of {np.sum(nebulae_tmp[\"Nassoc\"]>1)} match the criteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(7, 2), height_ratios=(2, 7),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.0, hspace=0.0)\n",
    "\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "\n",
    "xlim = [1e2,1e5]\n",
    "ylim = [-12,12]\n",
    "\n",
    "x,y = mass_diff,age_diff\n",
    "nbins=11\n",
    "bins = [np.logspace(*np.log10(xlim),12),np.linspace(*ylim,13)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=False)\n",
    "im = ax.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.Reds)\n",
    "\n",
    "ax_histx.hist(x,bins[0],color='#af1d1d')\n",
    "ax_histy.hist(y,bins[1],orientation='horizontal',color='#af1d1d')\n",
    "\n",
    "ax_histx.set_yticks([400])\n",
    "ax_histy.set_xticks([1000])\n",
    "ax.plot([1e2,1e5],[0,0],color='black')\n",
    "\n",
    "ax.set(xlim=xlim,xscale='log',xlabel=r'$\\Delta$ mass / M$_\\odot$',\n",
    "       ylim=ylim,ylabel='$\\Delta$ age / Myr')\n",
    "fig.colorbar(im,label='count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(single_column,single_column/1.618))\n",
    "\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(7, 2), height_ratios=(2, 7),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.0, hspace=0.0)\n",
    "\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "print(f'{np.sum(np.array(mass_div)>10)} objects differ by a factor >10')\n",
    "xlim = [1,10]\n",
    "ylim = [-12,12]\n",
    "\n",
    "x,y = mass_div,age_diff\n",
    "nbins=11\n",
    "bins = [np.linspace(*xlim,10),np.linspace(*ylim,13)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=False)\n",
    "im = ax.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.Reds)\n",
    "\n",
    "ax_histx.hist(x,bins[0],color='#af1d1d')\n",
    "ax_histy.hist(y,bins[1],orientation='horizontal',color='#af1d1d')\n",
    "\n",
    "ax_histx.set_yticks([400])\n",
    "ax_histy.set_xticks([1000])\n",
    "ax.plot([1,200],[0,0],color='black')\n",
    "\n",
    "ax.set(xlim=xlim,xscale='linear',xlabel=r'mass0 / mass1',\n",
    "       ylim=ylim,ylabel='$\\Delta$ age / Myr')\n",
    "fig.colorbar(im,label='count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(single_column,single_column/1.618))\n",
    "xlim = [10,1e5]\n",
    "ylim = [0,10]\n",
    "\n",
    "x,y = mass_diff, age_diff\n",
    "nbins=11\n",
    "bins = [np.logspace(*np.log10(xlim),16),np.linspace(*ylim,9)]\n",
    "hist, x_e, y_e = np.histogram2d(x,y,bins=bins,density=False)\n",
    "im = ax.pcolormesh(x_e,y_e,hist.T,cmap=plt.cm.Reds)\n",
    "\n",
    "ax.set(xlim=xlim,xscale='log',xlabel=r'$\\Delta$ mass / M$_\\odot$',\n",
    "       ylim=ylim,yscale='linear',ylabel='$\\Delta$ age / Myr')\n",
    "fig.colorbar(im,label='count')\n",
    "#plt.savefig(basedir/'reports'/'tmp_catalogue_properties_2D_hist.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "xlim = [1e2,1e5]\n",
    "ylim = [1,20]\n",
    "\n",
    "for region_ID in nebulae_tmp[nebulae_tmp['Nassoc']>1]['region_ID']:\n",
    "    if len(nebulae_dict[region_ID])==2:\n",
    "        sub = associations.loc[nebulae_dict[region_ID]]\n",
    "        ax.errorbar(sub['mass'],sub['age'],fmt='o-',color='black')\n",
    "\n",
    "ax.set(xlim=xlim,xscale='log',xlabel=r'$\\Delta$ mass / M$_\\odot$',\n",
    "       ylim=ylim,yscale='linear',ylabel='$\\Delta$ age / Myr')\n",
    "#plt.savefig(basedir/'reports'/'tmp_catalogue_properties_2D_hist.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 445, 375\n",
    "sub = associations.loc[nebulae_dict[223]]\n",
    "sub[['mass','age','overlap_asc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at cutouts of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout\n",
    "\n",
    "filename = basedir/'reports'/'multiple_associations.pdf'\n",
    "\n",
    "sample = nebulae.loc[candidates]\n",
    "positions = sample['SkyCoord']\n",
    "labels = sample['region_ID']\n",
    "\n",
    "multi_page_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 10*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.plot(cluster.quanta['Time']/1e6,cluster.quanta['HI_rate']/cluster.quanta['HI_rate'][0])\n",
    "ax.set(yscale='log',xlim=[1,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_modf,n_modf = np.modf(s_id)\n",
    "n_modf = n_modf.astype(int)\n",
    "a_modf = np.round(a_modf*scale).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_mod_arr,n_mod_arr = np.modf(s_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_ID = 24\n",
    "a_id,a_count = np.unique(a_mod_arr[n_mod_arr==region_ID]*scale,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nebulae.index_mode('discard_on_copy'):\n",
    "    nebulae_ages = nebulae[['gal_name','region_ID','Nassoc']].copy()\n",
    "nebulae_ages.add_index('region_ID')\n",
    "\n",
    "min_overlap = 0\n",
    "\n",
    "nebulae_ages['age_mw']   = np.nan   # mass weighted\n",
    "nebulae_ages['age_lw']   = np.nan   # light weighted\n",
    "nebulae_ages['age_sw']   = np.nan   # size weighted\n",
    "nebulae_ages['age_mean'] = np.nan\n",
    "nebulae_ages['age_min']  = np.nan\n",
    "nebulae_ages['age_max']  = np.nan\n",
    "nebulae_ages['mass_sum'] = np.nan\n",
    "\n",
    "for region_ID in nebulae_ages['region_ID']:\n",
    "    #region_ID = row['region_ID']\n",
    "    assoc_IDs = nebulae_dict[region_ID]\n",
    "    if len(assoc_IDs)==0:\n",
    "        continue\n",
    "    sub = associations.loc[assoc_IDs]\n",
    "        \n",
    "    nebulae_ages.loc[region_ID]['age_mw'] = np.average(sub['age'],weights=sub['mass'])\n",
    "    nebulae_ages.loc[region_ID]['age_lw'] = np.average(sub['age'],weights=sub['NUV_FLUX'])\n",
    "    nebulae_ages.loc[region_ID]['age_mean'] = np.mean(sub['age'])\n",
    "    nebulae_ages.loc[region_ID]['age_min'] = np.min(sub['age'])\n",
    "    nebulae_ages.loc[region_ID]['age_max'] = np.max(sub['age'])\n",
    "    nebulae_ages.loc[region_ID]['mass_sum'] = np.sum(sub['mass'])\n",
    "\n",
    "    \n",
    "    a_id,a_count = np.unique(a_mod_arr[n_mod_arr==region_ID]*scale,return_counts=True)\n",
    "    nebulae_ages.loc[region_ID]['age_sw'] = np.average(np.atleast_1d(sub['age']),weights=a_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from astrotools.plot import corner\n",
    "\n",
    "lim = [0,20]\n",
    "\n",
    "print(f'1to1: {np.sum(nebulae[\"1to1\"])}\\nnow: {np.sum(~np.isnan(nebulae_ages[\"age_sw\"]))}')\n",
    "\n",
    "columns = ['age_mean','age_mw','age_lw','age_sw']\n",
    "def plt_func(x,y,ax,xlim=None): \n",
    "    ax.scatter(x,y)\n",
    "    ax.plot(lim,lim,color='black')\n",
    "limits  = {key:lim for key in columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    corner(nebulae_ages,columns=columns,limits=limits,function=plt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(5,5))\n",
    "\n",
    "tmp = nebulae_ages[nebulae['1to1']]\n",
    "ax.scatter(tmp['age_mw'],tmp['age_sw'])\n",
    "ax.plot([0,100],[0,100],color='black')\n",
    "ax.set(xlim=[0,100],ylim=[0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolated HII regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is useful to only include HII regions in the overlapping regions\n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_in_frame.fits') as hdul:\n",
    "    in_frame = Table(hdul[1].data)\n",
    "nebulae = join(nebulae,in_frame,keys=['gal_name','region_ID'])\n",
    "distance = Distance(distmod=sample_table.loc[name]['(m-M)'])\n",
    "nebulae['distance'] = distance\n",
    "nebulae['HA6562_LUM_CORR'] = (nebulae['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*(distance)**2).to(u.erg/u.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "tmp = nebulae[nebulae['overlap_neb']==0]\n",
    "ax.hist(tmp['region_area'],bins=np.linspace(50,500,10),label='isolated',alpha=0.5)\n",
    "\n",
    "tmp = nebulae[nebulae['overlap_neb']>0]\n",
    "ax.hist(tmp['region_area'],bins=np.linspace(50,500,10),label='overlapping',alpha=0.5)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'external'/'PHANGS_IR3_ngc2835_phangs-hst_v1p1_ml_class12.fits') as hdul:\n",
    "    compact_clusters = Table(hdul[1].data)\n",
    "compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(data_ext/'Products'/'stellar_associations'/'DOLPHOT'/f'{name.lower()}_uvis_dolphot.fits') as hdul:\n",
    "    dolphot_peaks = Table(hdul[1].data)\n",
    "#compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "dolphot_peaks['SkyCoord'] = SkyCoord.from_pixel(dolphot_peaks['Dolphot_x'],dolphot_peaks['Dolphot_y'],wcs=F275.wcs)\n",
    "\n",
    "#dolphot_peaks = dolphot_peaks[np.isin(dolphot_peaks['Dolphot_ID'],bright_peaks)]\n",
    "#dolphot_peaks = dolphot_peaks[dolphot_peaks['S2N']>20]\n",
    "#dolphot_peaks['label']=dolphot_peaks['Dolphot_ID']\n",
    "\n",
    "print(len(dolphot_peaks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout\n",
    "\n",
    "sample = nebulae[(nebulae['overlap_neb']==0) & (nebulae['in_frame'])][:59]\n",
    "#sample = nebulae[np.isin(nebulae['region_ID'],[2,4,5,6,10,11,12,14,15,18,19,24,27,28,29,31,39,40,41,42,51,56,57,59,60,66,67,70,72,74,77,79,82])]\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_HIIregions_F275'\n",
    "positions = sample['SkyCoord']\n",
    "labels = [f'{ri}' for ri in sample['region_ID']]\n",
    "\n",
    "multi_page_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             points= dolphot_peaks,\n",
    "             #points2= compact_clusters,\n",
    "             labels= labels,\n",
    "             size = 6*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from reproject import reproject_interp\n",
    "\n",
    "cutout_mask = reproject_interp(nebulae_mask,output_projection=F275.wcs,shape_out=F275.data.shape,order='nearest-neighbor',return_footprint=False)    \n",
    "neb_contours = []\n",
    "for i in np.unique(nebulae['region_ID']):\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    neb_contours += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "cutout_mask = reproject_interp(associations_mask,output_projection=F275.wcs,shape_out=F275.data.shape,order='nearest-neighbor',return_footprint=False)    \n",
    "assoc_contours = []\n",
    "for i in np.unique(associations['assoc_ID']):\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    assoc_contours += find_contours(blank_mask, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(20,20))\n",
    "norm = simple_norm(F275.data,clip=False,stretch='linear',percent=99.5)\n",
    "ax.imshow(F275.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "x,y = compact_clusters['SkyCoord'].to_pixel(F275.wcs)\n",
    "ax.scatter(x,y,marker='o',facecolors='none',s=20,lw=0.4,color='tab:green',label='cluster')\n",
    "\n",
    "for coords in neb_contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label='nebulae')\n",
    "for coords in assoc_contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:blue',lw=1,label='association')\n",
    "\n",
    "ax.set(xlim=[2000,6000],ylim=[2000,6000])\n",
    "plt.savefig(basedir/'reports'/name/'full_image_with_peaks.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dolphot_peaks[np.isin(dolphot_peaks['Dolphot_ID'],bright_peaks)]\n",
    "tmp.sort('Mag_WFC3_F275W')\n",
    "tmp[['Mag_WFC3_F275W','Mag_WFC3_F336W','Mag_WFC3_F438W']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the HST sources inside the isolated HII regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(matrix, index, default_value=np.nan):\n",
    "    '''\n",
    "    The `to_pixel` method returns the x,y coordinates. However in the \n",
    "    image they correspond to img[y,x]\n",
    "    '''\n",
    "    result = np.zeros(len(index))+default_value\n",
    "    mask = (index[:,1] < matrix.shape[0]) & (index[:,0] < matrix.shape[1])\n",
    "    valid = index[mask]\n",
    "    result[mask] = matrix[valid[:,1], valid[:,0]]\n",
    "    return result\n",
    "\n",
    "isolated = nebulae[nebulae['overlap_neb']==0]\n",
    "\n",
    "with fits.open(basedir/'data'/'external'/f'PHANGS_IR3_{name.lower()}_phangs-hst_v1p1_ml_class12.fits') as hdul:\n",
    "    compact_clusters = Table(hdul[1].data)\n",
    "compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "compact_clusters.add_index('ID_PHANGS_CLUSTERS')\n",
    "\n",
    "with fits.open(basedir/'data'/'external'/f'{name.lower()}_uvis_dolphot.fits') as hdul:\n",
    "    dolphot_peaks = Table(hdul[1].data)\n",
    "dolphot_peaks.add_index('Dolphot_ID')\n",
    "dolphot_peaks['SkyCoord'] = SkyCoord.from_pixel(dolphot_peaks['Dolphot_x'],dolphot_peaks['Dolphot_y'],wcs=F275.wcs)\n",
    "dolphot_peaks = dolphot_peaks[(dolphot_peaks['S2N']>20) & (dolphot_peaks['Mag_WFC3_F275W']<25)]\n",
    "print(f'{len(dolphot_peaks)} peaks in catalogue')\n",
    "\n",
    "dolphot_peaks['region_ID'] = get_value(nebulae_mask.data,np.array(dolphot_peaks['SkyCoord'].to_pixel(nebulae_mask.wcs)).T.astype(int))\n",
    "compact_clusters['region_ID'] = get_value(nebulae_mask.data,np.array(compact_clusters['SkyCoord'].to_pixel(nebulae_mask.wcs)).T.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(isolated)} of {len(nebulae)} HII regions are isolated')\n",
    "print(f'{np.sum(np.isin(compact_clusters[\"region_ID\"],isolated[\"region_ID\"]))} of {len(compact_clusters)} compact clusters fall inside one of them')\n",
    "print(f'{np.sum(np.isin(dolphot_peaks[\"region_ID\"],isolated[\"region_ID\"]))} of {len(dolphot_peaks)} DOLPHOT peaks fall inside one of them\\n')\n",
    "\n",
    "print(f'{np.sum(np.isin(isolated[\"region_ID\"],compact_clusters[\"region_ID\"]))} isolated HII regions contain compact cluster')\n",
    "print(f'{np.sum(np.isin(isolated[\"region_ID\"],dolphot_peaks[\"region_ID\"]))} isolated HII regions contain DOLPHOT peak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at color-color and CMD diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([[-0.33 , -1.662],[-0.307, -1.577],[-0.25 , -1.468],[-0.218, -1.394],[-0.186, -1.321],\n",
    "                   [-0.138, -1.235],[-0.076, -1.213],[-0.007, -1.206],[ 0.065, -1.197],[ 0.135, -1.187],\n",
    "                   [ 0.203, -1.198],[ 0.276, -1.201],[ 0.347, -1.209],[ 0.418, -1.219],[ 0.488, -1.233],\n",
    "                   [ 0.559, -1.251],[ 0.63 , -1.27 ],[ 0.706, -1.264],[ 0.771, -1.287],[ 0.648, -1.119],\n",
    "                   [ 0.626, -1.026],[ 0.565, -0.989],[ 0.559, -0.87 ],[ 0.502, -0.807],[ 0.481, -0.706],\n",
    "                   [ 0.457, -0.546],[ 0.505, -0.449],[ 0.487, -0.384],[ 0.488, -0.328],[ 0.468, -0.239],\n",
    "                   [ 0.517, -0.109],[ 0.555, -0.015],[ 0.6  ,  0.041],[ 0.65 ,  0.132],[ 0.724,  0.174],\n",
    "                   [ 0.864,  0.169],[ 0.948,  0.166],[ 1.02 ,  0.152],[ 1.138,  0.182],[ 1.203,  0.194],\n",
    "                   [ 1.253,  0.282],[ 1.331,  0.378],[ 1.354,  0.469]])\n",
    "\n",
    "x,y=points.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "tmp = compact_clusters[np.isin(compact_clusters[\"region_ID\"],isolated[\"region_ID\"])]\n",
    "U = tmp['PHANGS_F336W_VEGA_TOT']\n",
    "B = tmp['PHANGS_F438W_VEGA_TOT']\n",
    "V = tmp['PHANGS_F555W_VEGA_TOT']\n",
    "I = tmp['PHANGS_F814W_VEGA_TOT']\n",
    "\n",
    "ax1.scatter(B-V,V)\n",
    "ax1.set(xlim=[-1,2.5],ylim=[18,28],\n",
    "      ylabel=r'F555W',xlabel=r'F438W$-$F555W')\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title('compact clusters')\n",
    "\n",
    "tmp = dolphot_peaks[np.isin(dolphot_peaks[\"region_ID\"],isolated[\"region_ID\"])]\n",
    "U = tmp['Mag_WFC3_F336W']\n",
    "B = tmp['Mag_WFC3_F438W']\n",
    "V = tmp['Mag_WFC3_F555W']\n",
    "I = tmp['Mag_WFC3_F814W']\n",
    "\n",
    "ax2.scatter(B-V,V)\n",
    "ax2.set(xlim=[-1,2.5],ylim=[19,28],\n",
    "      ylabel=r'F555W',xlabel=r'F438W$-$F555W')\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_title('DOLPHOT peaks')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8))=plt.subplots(figsize=(two_column,two_column*2),ncols=2,nrows=4)\n",
    "\n",
    "tmp = compact_clusters[np.isin(compact_clusters[\"region_ID\"],isolated[\"region_ID\"])]\n",
    "U = tmp['PHANGS_F336W_VEGA_TOT']\n",
    "B = tmp['PHANGS_F438W_VEGA_TOT']\n",
    "V = tmp['PHANGS_F555W_VEGA_TOT']\n",
    "I = tmp['PHANGS_F814W_VEGA_TOT']\n",
    "\n",
    "ax1.scatter(V-I,U-B)\n",
    "ax1.plot(x,y,color='black')\n",
    "ax1.set(xlim=[-1,2],ylim=[-2.5,1],\n",
    "      ylabel=r'F336W$-$F438W',xlabel=r'F555W$-$F814W')\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title('compact clusters')\n",
    "\n",
    "ax3.scatter(B-V,V)\n",
    "ax3.set(xlim=[-1,2.5],ylim=[18,28],\n",
    "      ylabel=r'F555W',xlabel=r'F438W$-$F555W')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "tmp = dolphot_peaks[np.isin(dolphot_peaks[\"region_ID\"],isolated[\"region_ID\"])]\n",
    "U = tmp['Mag_WFC3_F336W']\n",
    "B = tmp['Mag_WFC3_F438W']\n",
    "V = tmp['Mag_WFC3_F555W']\n",
    "I = tmp['Mag_WFC3_F814W']\n",
    "\n",
    "ax2.scatter(V-I,U-B)\n",
    "ax2.plot(x,y,color='black')\n",
    "ax2.set(xlim=[-1,2],ylim=[-2.5,1],\n",
    "      ylabel=r'F336W$-$F438W',xlabel=r'F555W$-$F814W')\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_title('DOLPHOT peaks')\n",
    "\n",
    "ax4.scatter(B-V,V)\n",
    "ax4.set(xlim=[-1,2.5],ylim=[19,28],\n",
    "      ylabel=r'F555W',xlabel=r'F438W$-$F555W')\n",
    "ax4.invert_yaxis()\n",
    "\n",
    "lim = [20.5,26.5]\n",
    "tmp = compact_clusters[np.isin(compact_clusters['region_ID'],isolated['region_ID'])]\n",
    "ax5.hist(tmp['PHANGS_F275W_VEGA_TOT'],bins=np.linspace(*lim,13),label='NUV',alpha=0.5,color='tab:purple')\n",
    "ax5.hist(tmp['PHANGS_F555W_VEGA_TOT'],bins=np.linspace(*lim,13),label='V',alpha=0.5,color='tab:green')\n",
    "ax5.legend()\n",
    "ax5.set(xlabel='vega mag',xlim=lim)\n",
    "\n",
    "tmp = dolphot_peaks[np.isin(dolphot_peaks['region_ID'],isolated['region_ID'])]\n",
    "ax6.hist(tmp['Mag_WFC3_F275W'],bins=np.linspace(*lim,13),label='NUV',alpha=0.5,color='tab:purple')\n",
    "ax6.hist(tmp['Mag_WFC3_F555W'],bins=np.linspace(*lim,13),label='V',alpha=0.5,color='tab:green')\n",
    "ax6.legend()\n",
    "ax6.set(xlabel='vega mag',xlim=lim)\n",
    "\n",
    "bins = np.logspace(36,38,9)\n",
    "\n",
    "tmp = isolated[np.isin(isolated['region_ID'],compact_clusters['region_ID'])]\n",
    "ax7.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.9,label='')\n",
    "ax7.set(xscale='log',xlabel=r'H$\\alpha$ / erg s$^{-1}$',xlim=[1e36,1e38])\n",
    "\n",
    "tmp = isolated[np.isin(isolated['region_ID'],dolphot_peaks['region_ID'])]\n",
    "ax8.hist(tmp['HA6562_LUM_CORR'],bins=bins,alpha=0.9,label='DOLPHOT peaks')\n",
    "ax8.set(xscale='log',xlabel=r'H$\\alpha$ / erg s$^{-1}$',xlim=[1e36,1e38])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'sources_in_isolated_HIIregions.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### how much flux do we expect from a single O star?\n",
    "\n",
    "we can use the stellar atmosphere models from cloudy\n",
    "https://gitlab.nublado.org/cloudy/cloudy/-/wikis/StellarAtmospheres\n",
    "\n",
    "they use the following models\n",
    "http://tlusty.oca.eu/Tlusty2002/tlusty-frames-OS02.html\n",
    "\n",
    "see Figure 1 to estimate radius based on log g\n",
    "https://ui.adsabs.harvard.edu/abs/2003ApJS..146..417L/abstract\n",
    "\n",
    "Example: G35000g400v10. The first letter indicates the composition (G=solar), followed by the effective temperature, the gravity and the turbulent velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speclite.filters\n",
    "import re\n",
    "bessell = speclite.filters.load_filters('bessell-U','bessell-B','bessell-V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = np.array([15,120]) * u.Msun\n",
    "logg = [400,300]\n",
    "temp = [55000,30000]\n",
    "\n",
    "\n",
    "def AB_mag_from_spec(mass,logg,temp,mu):\n",
    "    \n",
    "    # https://www.astronomy.ohio-state.edu/martini.10/usefuldata.html\n",
    "    AB_to_vega = {\n",
    "        'bessell-U' : 0.79,\n",
    "        'bessell-B' : -0.09,\n",
    "        'bessell-V' : 0.02\n",
    "    }\n",
    "    \n",
    "    model = f'G{temp}g{logg}v10'\n",
    "    result = re.search('([A-Z])(\\d+)g(\\d+)',model)\n",
    "    Z,temp,logg = result.groups()\n",
    "    g = 10**(float(logg)/100) * u.cm/u.s**2\n",
    "    radius = np.sqrt(c.G*mass / g).to(u.Rsun)\n",
    "\n",
    "    OBstars = ascii.read(basedir/'data'/'external'/'OBstars'/f'{model}.vis.17',\n",
    "                         data_start=0,names=['wavelength','flux'])\n",
    "    wlen = OBstars['wavelength'] * u.Angstrom\n",
    "    flux = OBstars['flux'] * u.erg / (u.cm**2 * u.s * u.Angstrom)\n",
    "    \n",
    "    mags = bessell.get_ab_magnitudes(flux*(radius/u.pc)**2,wlen)\n",
    "    print(f\"M={mass:>5}, R={radius:>5.1f}, T={temp} K, g={g:>8.2g}: M_NUV={mags['bessell-U'][0]+ AB_to_vega['bessell-U']:>5.1f}, m_NUV={mags['bessell-U'][0]+mu+AB_to_vega['bessell-U']:.1f}\")\n",
    "    \n",
    "mu = sample_table.loc[name]['(m-M)']\n",
    "AB_mag_from_spec(120*u.Msun,400,55000,mu)\n",
    "AB_mag_from_spec(120*u.Msun,300,30000,mu)\n",
    "AB_mag_from_spec(40*u.Msun,400,40000,mu)\n",
    "AB_mag_from_spec(40*u.Msun,300,27500,mu)\n",
    "AB_mag_from_spec(20*u.Msun,450,35000,mu)\n",
    "AB_mag_from_spec(20*u.Msun,350,27500,mu)\n",
    "AB_mag_from_spec(15*u.Msun,450,27500,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'G50000g400v10'\n",
    "OBstars = ascii.read(basedir/'data'/'external'/'OBstars'/f'{model}.vis.7',\n",
    "                     data_start=0,names=['wavelength','flux'])\n",
    "    \n",
    "fig,ax=plt.subplots(figsize=(8,3))\n",
    "ax.plot(OBstars['wavelength'],OBstars['flux'])\n",
    "\n",
    "ax.set(xscale='log',yscale='log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcase the matched catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "size = (4*u.arcmin,4*u.arcmin)\n",
    "b_image = Cutout2D(F275.data,position=p['SkyCoord'],size=size,wcs=F275.wcs)\n",
    "r_image = reproject_interp(CO,output_projection=b_image.wcs,shape_out=b_image.shape,return_footprint=False) \n",
    "g_image = reproject_interp(Halpha,output_projection=b_image.wcs,shape_out=b_image.shape,return_footprint=False) \n",
    "\n",
    "mask = np.isnan(r_image) | np.isnan(g_image) | np.isnan(b_image.data)\n",
    "\n",
    "r_image[mask] = np.nan\n",
    "g_image[mask] = np.nan\n",
    "b_image.data[mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "cutout_mask = reproject_interp(nebulae_mask,output_projection=b_image.wcs,shape_out=b_image.shape,order='nearest-neighbor',return_footprint=False)    \n",
    "neb_contours = []\n",
    "for i in np.unique(nebulae['region_ID']):\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    neb_contours += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "cutout_mask = reproject_interp(associations_mask,output_projection=b_image.wcs,shape_out=b_image.shape,order='nearest-neighbor',return_footprint=False)    \n",
    "assoc_contours = []\n",
    "for i in np.unique(associations['assoc_ID']):\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    assoc_contours += find_contours(blank_mask, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.plot import add_scale, create_RGB\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(15,15))\n",
    "\n",
    "rgb = create_RGB(r_image.data,g_image.data,b_image.data,\n",
    "                percentile=[98,98,99.1],weights=[0.7,0.6,1])\n",
    "\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "for coords in neb_contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=0.2,label='HII-region')\n",
    "\n",
    "for coords in assoc_contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='blue',lw=0.2,label='association')\n",
    "    \n",
    "ax.axis('off')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_rgb.pdf',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cutouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the joined catalogue (containing only nebulae and clusters with a 1 to 1 relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import single_cutout\n",
    "\n",
    "\n",
    "region_ID = 418\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "#position = SkyCoord(53.42400094559648*u.deg,-36.13390066270671*u.deg)\n",
    "fig,ax=plt.subplots(figsize=(8,8))\n",
    "single_cutout(ax=ax,position = position,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             #label= f'{region_ID}/{nebulae_dict[region_ID][0]}',\n",
    "             size = 4*u.arcsecond)\n",
    "ax.axis('off')\n",
    "#plt.savefig(basedir/'reports'/name/'single_region.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=19.588447*u.Mpc\n",
    "size = 4*u.arcsec\n",
    "cutout_size = (d*size.to(u.radian).value).to(u.pc)\n",
    "100*u.pc/cutout_size\n",
    "\n",
    "# the positions in NGC1365 to showcase the overlap\n",
    "#sample = associations[np.searchsorted(associations['assoc_ID'],[63,26,40,17,4,490,343])]\n",
    "positions = SkyCoord(np.array([53.42906393,53.394168064868886,53.37350396732387,\n",
    "                   53.38195235,53.42400094559648,53.403615497790064,\n",
    "                   53.37770988890464,53.39356368])*u.deg,\n",
    "         np.array([-36.13802943,-36.161250435035704,-36.15798912343546,\n",
    "                   -36.14863033,-36.13390066270671,-36.122456347335785,\n",
    "                   -36.14803247549895,-36.13742958])*u.deg)\n",
    "labels = ['partial','partial 1to1','contained','contained 1to1','isolated','neighbors=0','neighbors=1','Nassoc=0',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_cutout\n",
    "\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_associations_F275.pdf'\n",
    "filename = basedir/'reports'/'overlap.pdf'\n",
    "#filename=None\n",
    "\n",
    "sample = catalogue[catalogue['overlap_asc']==1][:12]\n",
    "#positions = sample['SkyCoord_asc']\n",
    "#labels = [f'{ri}/{ci}' for ri, ci in sample[['region_ID','assoc_ID']]]\n",
    "\n",
    "multi_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             width=1.2*single_column,\n",
    "             filename=filename,\n",
    "             ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot.cutouts import multi_cutout_hst\n",
    "from cluster.io import read_associations, ReadHST\n",
    "\n",
    "#hst_images = ReadHST(name,data_ext / 'HST' / 'filterImages' )\n",
    "#hst_images.Halpha = Halpha\n",
    "\n",
    "filename = basedir/'reports'/'overlap_rgb.pdf'\n",
    "\n",
    "multi_cutout_hst(positions = positions,\n",
    "             images=hst_images,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             width = 1.2*single_column,\n",
    "             filename=filename,\n",
    "             ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi cutout rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot.cutouts import multi_cutout_rgb\n",
    "\n",
    "sample = associations[(associations['overlap_asc']==1) & (associations['1to1']==False)][:10]\n",
    "\n",
    "filename = None #basedir/'reports'/name/f'{name}_isolated_associations_F275'\n",
    "positions = sample['SkyCoord']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in sample[['region_ID','assoc_ID']]]\n",
    "\n",
    "multi_cutout_rgb(positions = positions,\n",
    "             r=CO,g=Halpha,b=F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a multi page pdf with all isolated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_{scalepc}pc_isolated_associations_F275'\n",
    "positions = catalogue['SkyCoord_neb'][catalogue['overlap']=='contained']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in catalogue[['region_ID','assoc_ID']][catalogue['overlap']=='contained']]\n",
    "\n",
    "filename = basedir/'reports'/'low_age_low_HaFUV_cutouts'\n",
    "sub = catalogue[(catalogue['age']<=1) & (catalogue['HA/FUV']<10)]\n",
    "positions = sub['SkyCoord_neb']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in sub[['region_ID','assoc_ID']]]\n",
    "\n",
    "multi_page_cutout(positions = positions[:59],\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout_hst\n",
    "from cluster.io import read_associations, ReadHST\n",
    "\n",
    "hst_images = ReadHST(name,data_ext / 'HST' / 'filterImages' )\n",
    "hst_images.Halpha = Halpha\n",
    "\n",
    "filename = basedir/'reports'/name/'nebulae_hst_images.pdf'\n",
    "\n",
    "sample = nebulae[nebulae['overlap_neb']>0]\n",
    "positions = sample['SkyCoord']\n",
    "labels = sample['region_ID']\n",
    "\n",
    "multi_page_cutout_hst(positions = positions,\n",
    "             images = hst_images,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=4*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column),subplot_kw={'projection': WFI_cutout.wcs})\n",
    "norm = simple_norm(WFI_cutout.data,clip=False,percent=96)\n",
    "ax.imshow(WFI_cutout.data,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "x,y = catalogue['SkyCoord_neb'][catalogue['contained']].to_pixel(WFI_cutout.wcs)\n",
    "ax.scatter(x,y,marker='s',facecolors='none',s=30,lw=1,color='tab:red')\n",
    "\n",
    "ax.coords[0].set_ticks_visible(False)\n",
    "ax.coords[1].set_ticks_visible(False)\n",
    "ax.coords[0].set_ticklabel_visible(False)\n",
    "ax.coords[1].set_ticklabel_visible(False)\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_location_in_galaxy.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "better 3 color composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = SkyCoord(np.array([53.39025716779467,53.394168064868886,53.37350396732387,\n",
    "                   53.420723078529065,53.4185425611773,53.403615497790064,\n",
    "                   53.39997682588791,53.3960053])*u.deg,\n",
    "         np.array([-36.154129419403674,-36.161250435035704,-36.15798912343546,\n",
    "                   -36.16514590146428,-36.16888805165907,-36.122456347335785,\n",
    "                   -36.13526671075946,-36.13620865])*u.deg)\n",
    "labels = ['partial','partial 1to1','contained','contained 1to1','isolated','neighbors=0','neighbors=1','Nassoc=0',]\n",
    "\n",
    "from cluster.io import ReadHST\n",
    "\n",
    "hst_images = ReadHST(name,data_ext/'HST'/'filterImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multicolorfits as mcf\n",
    "from reproject import reproject_interp\n",
    "region_ID = 1\n",
    "position = positions[region_ID]\n",
    "size = (4*u.arcsec,4*u.arcsec)\n",
    "\n",
    "f275w_cutout = Cutout2D(hst_images.f275w.data,position,size=size,wcs=hst_images.f275w.wcs)\n",
    "f336w_cutout,_  = reproject_interp(hst_images.f336w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape)    \n",
    "f438w_cutout,_  = reproject_interp(hst_images.f438w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape)    \n",
    "f555w_cutout,_  = reproject_interp(hst_images.f555w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape)    \n",
    "f814w_cutout,_  = reproject_interp(hst_images.f814w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape)    \n",
    "halpha_cutout,_ = reproject_interp(Halpha,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multicolorfits as mcf\n",
    "from reproject import reproject_interp\n",
    "region_ID = 1\n",
    "position = positions[region_ID]\n",
    "size = (4*u.arcsec,4*u.arcsec)\n",
    "\n",
    "halpha_cutout = Cutout2D(Halpha.data,position,size=size,wcs=Halpha.wcs)\n",
    "f275w_cutout,_  = reproject_interp(hst_images.f275w,output_projection=halpha_cutout.wcs,shape_out=halpha_cutout.shape)    \n",
    "f336w_cutout,_  = reproject_interp(hst_images.f336w,output_projection=halpha_cutout.wcs,shape_out=halpha_cutout.shape)    \n",
    "f438w_cutout,_  = reproject_interp(hst_images.f438w,output_projection=halpha_cutout.wcs,shape_out=halpha_cutout.shape)    \n",
    "f555w_cutout,_  = reproject_interp(hst_images.f555w,output_projection=halpha_cutout.wcs,shape_out=halpha_cutout.shape)    \n",
    "f814w_cutout,_  = reproject_interp(hst_images.f814w,output_projection=halpha_cutout.wcs,shape_out=halpha_cutout.shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(value):\n",
    "    value = value.lstrip('#')\n",
    "    lv = len(value)\n",
    "    return tuple(int(value[i:i + lv // 3], 16)/255 for i in range(0, lv, lv // 3))\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % rgb\n",
    "\n",
    "def create_multi_rgb(images,\n",
    "                   colors=['#ff0000','#00ff00','#0000ff'],\n",
    "                   percentile=None,weights=None):\n",
    "    '''combie three arrays to one RGB image\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    \n",
    "    images : list of images\n",
    "    \n",
    "    colors : the colors to which the images are assigned (default r,g,b)\n",
    "    \n",
    "    percentile : percentile that is used \n",
    "    '''\n",
    "    \n",
    "    if not percentile:\n",
    "        percentile = [(0,100) for i in range(len(images))]\n",
    "  \n",
    "    if not weights:\n",
    "        weights = [1 for i in range(len(images))]\n",
    "        \n",
    "    colors_rgb = np.vstack([hex_to_rgb(x) for x in colors])\n",
    "\n",
    "    # create an empty array with the correct size\n",
    "    rgb = np.zeros((*images[0].shape,3))\n",
    "    \n",
    "    for i,image in enumerate(images):\n",
    "        lim = np.nanpercentile(image,percentile[i])\n",
    "        scaled_img = (image-lim[0]) / (lim[1]-lim[0])\n",
    "        \n",
    "        rgb[...,0] += weights[0] * scaled_img * colors_rgb[i,0] / np.sum(colors_rgb[...,0]) * colors_rgb[i,0] / np.sum(colors_rgb[i,...])\n",
    "        rgb[...,1] += weights[1] * scaled_img * colors_rgb[i,1] / np.sum(colors_rgb[...,1]) * colors_rgb[i,1] / np.sum(colors_rgb[i,...])\n",
    "        rgb[...,2] += weights[2] * scaled_img * colors_rgb[i,2] / np.sum(colors_rgb[...,2]) * colors_rgb[i,2] / np.sum(colors_rgb[i,...])\n",
    "\n",
    "    # clip values (we use percentile for the normalization) and fill nan\n",
    "    rgb = np.clip(np.nan_to_num(rgb,nan=1),a_min=0,a_max=1)\n",
    "    \n",
    "    return rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "rgb = create_multi_rgb([halpha_cutout,f555w_cutout,f275w_cutout.data],\n",
    "                     colors=['#ff0000','#00ff00','#0000ff'],\n",
    "                     percentile=[(0,99),(0,99),(0,99)],\n",
    "                     weights=[1,0.6,1])\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the nebulae catalogue and associations\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='red',lw=0.8,label='HII-region')\n",
    "\n",
    "# 32 pc\n",
    "cutout_32,_ = reproject_interp(associations_mask,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_32[~np.isnan(cutout_32)])\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_32)\n",
    "    blank_mask[cutout_32==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='blue',lw=0.8,label='32pc assoc.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multicolorfits as mcf\n",
    "from reproject import reproject_interp\n",
    "\n",
    "f275w_reproject,_ = reproject_interp(hst_images.f275w,output_projection=Halpha.wcs,shape_out=Halpha.data.shape) \n",
    "f555w_reproject,_ = reproject_interp(hst_images.f555w,output_projection=Halpha.wcs,shape_out=Halpha.data.shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "rgb = create_multi_rgb([Halpha.data,f555w_reproject,f275w_reproject],\n",
    "                     colors=['#ff0000','#00ff00','#0000ff'],\n",
    "                     percentile=[(10,95),(10,95),(10,95)])\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = fits.Header()\n",
    "header['gal_name'] = name\n",
    "header['RA'] = position.ra.to(u.degree).value\n",
    "header['DEC'] = position.dec.to(u.degree).value\n",
    "header['RADESYS'] = 'ICRS'\n",
    "header['regionID'] = region_ID\n",
    "#header['assocID'] = row['cluster_ID']\n",
    "\n",
    "hdul = fits.HDUList([fits.PrimaryHDU(header=header)])\n",
    "\n",
    "# save HST image\n",
    "hdul.append(fits.ImageHDU(f275w_cutout.data,header=f275w_cutout.wcs.to_header(),name='f275w'))\n",
    "hdul.append(fits.ImageHDU(f336w_cutout,header=f275w_cutout.wcs.to_header(),name='f336w'))\n",
    "hdul.append(fits.ImageHDU(f438w_cutout,header=f275w_cutout.wcs.to_header(),name='f438w'))\n",
    "hdul.append(fits.ImageHDU(f555w_cutout,header=f275w_cutout.wcs.to_header(),name='f555w'))\n",
    "hdul.append(fits.ImageHDU(f814w_cutout,header=f275w_cutout.wcs.to_header(),name='f814w'))\n",
    "hdul.append(fits.ImageHDU(halpha_cutout,header=f275w_cutout.wcs.to_header(),name='Halpha'))\n",
    "\n",
    "hdul.writeto(basedir/'data'/'cutouts'/f'{name}_region{region_ID}.fits',overwrite=True,checksum=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HST narrowband Halpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from cluster.io import read_associations, ReadHST\n",
    "\n",
    "hst_images = ReadHST(name,data_ext / 'HST' / 'filterImages' )\n",
    "\n",
    "filename = data_ext / 'HST' / 'narrowband_Halpha' / f'{name.lower()}_ha_sub.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    hst_halpha = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "region_ID = 536\n",
    "position = nebulae[(nebulae['gal_name']==name) & (nebulae['region_ID']==region_ID)]['SkyCoord']\n",
    "size = 4*u.arcsecond\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "cutout_image = Cutout2D(hst_images.f275w.data,position,size=size,wcs=hst_images.f275w.wcs)\n",
    "\n",
    "norm = simple_norm(cutout_image.data,clip=False,stretch='linear',percent=99.5)\n",
    "ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "\n",
    "# plot the nebulae catalogue\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_image.wcs,shape_out=cutout_image.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label=r'H\\textsc{ii} region')\n",
    "\n",
    "\n",
    "ax.axis('off')\n",
    "#plt.savefig(basedir/'reports'/name/'single_region.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f275w_cutout  = Cutout2D(hst_images.f275w.data,position,size=size,wcs=hst_images.f275w.wcs)\n",
    "f336w_cutout = reproject_interp(hst_images.f336w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "f438w_cutout = reproject_interp(hst_images.f438w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "f555w_cutout = reproject_interp(hst_images.f555w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "f814w_cutout = reproject_interp(hst_images.f814w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "halpha_cutout  = reproject_interp(hst_halpha,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    \n",
    "halpha_cutout_muse  = reproject_interp(Halpha,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multicolorimage(*args):\n",
    "    '''combine multiple images to a three color image'''\n",
    "    \n",
    "    color_images = []\n",
    "    for arg in args:\n",
    "        color = arg.pop('color')\n",
    "        greyscale = mcf.greyRGBize_image(**arg)\n",
    "        color_images.append(mcf.colorize_image(greyscale,color, colorintype='hex',gammacorr_color=2.2))\n",
    "    \n",
    "    return mcf.combine_multicolor(color_images, gamma=2.2)\n",
    "\n",
    "p1,p2=np.nanpercentile(f275w_cutout.data,[5,100])\n",
    "f275_white  = {'color':'#d5c5e3','datin':f275w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.9]}\n",
    "f336_purple = {'color':'#9C4FFF','datin':f336w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.9]}\n",
    "f438_blue   = {'color':'#61b3cf','datin':f438w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.8]}\n",
    "f555_green  = {'color':'#00e32d','datin':f555w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.8]}\n",
    "f814_yellow = {'color':'#e08e1b','datin':f814w_cutout.data,'rescalefn':'linear','scaletype':'perc','min_max':[15,99.8]}\n",
    "ha_red_peak = {'color':'#ed7f5a','datin':halpha_cutout_muse.data,'rescalefn':'linear','scaletype':'perc','min_max':[10,99.8]}\n",
    "ha_red_dif  = {'color':'#ed5a95','datin':halpha_cutout_muse.data,'rescalefn':'log','scaletype':'abs','min_max':[100,1000]}\n",
    "\n",
    "rgb = create_multicolorimage(f275_white,f336_purple,f438_blue,f555_green,f814_yellow,ha_red_peak,ha_red_dif)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "# plot the nebulae catalogue\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_image.wcs,shape_out=cutout_image.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='#eb6060',lw=1,label=r'H\\textsc{ii} region')\n",
    "\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f275_greyscale=mcf.greyRGBize_image(f275_cutout.data, rescalefn='sqrt', scaletype='perc', min_max=[0,99.9], gamma=2.2, checkscale=False) #[70,99.7]\n",
    "f275_purple = mcf.colorize_image(f275_greyscale, '#e31ec9', colorintype='hex', gammacorr_color=2.2)#24d1b4#24d1d1\n",
    "\n",
    "f435_greyscale=mcf.greyRGBize_image(f435w_cutout.data, rescalefn='linear', scaletype='perc', min_max=[0.01,99.5], gamma=2.2, checkscale=False) #[70,99.7]\n",
    "f435_blue = mcf.colorize_image(f435_greyscale, '#61b3cf', colorintype='hex', gammacorr_color=2.2)#24d1b4#24d1d1\n",
    "\n",
    "f435_greyscale=mcf.greyRGBize_image(f435w_cutout.data, rescalefn='linear', scaletype='perc', min_max=[70,99], gamma=2.2, checkscale=False) #[70,99.7]\n",
    "f435_purple = mcf.colorize_image(f435_greyscale, '#9C4FFF', colorintype='hex', gammacorr_color=2.2)#24d1b4#24d1d1\n",
    "\n",
    "f814_greyscale=mcf.greyRGBize_image(f814w_cutout.data, rescalefn='linear', scaletype='perc', min_max=[10,99.9], gamma=2.2, checkscale=False) #[70,99.7]\n",
    "f814_yellow = mcf.colorize_image(f814_greyscale, '#e7eb21', colorintype='hex', gammacorr_color=2.2)#24d1b4#24d1d1\n",
    "\n",
    "ha_greyscale=mcf.greyRGBize_image(halpha_cutout_muse.data, rescalefn='linear', scaletype='perc', min_max=[20,99.8], gamma=2.2, checkscale=False) #[70,99.7]\n",
    "ha_red_peak = mcf.colorize_image(ha_greyscale, '#ed7f5a', colorintype='hex', gammacorr_color=2.2)#ff8400#ffae00\n",
    "\n",
    "ha_greyscale=mcf.greyRGBize_image(halpha_cutout_muse.data, rescalefn='log', scaletype='abs', min_max=[100,2000], gamma=2.2, checkscale=False) #[70,99.7]\n",
    "ha_red_dif = mcf.colorize_image(ha_greyscale, '#ed5a95', colorintype='hex', gammacorr_color=2.2)#ff8400#ffae00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations in joined catalogue\n",
    "\n",
    "first we create a mask to select a subset of objects (e.g. based on mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "# separation to other associations\n",
    "idx,sep_others,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_int,_= match_coordinates_sky(catalogue['SkyCoord_asc'],catalogue['SkyCoord_neb'])\n",
    "\n",
    "# size of the association compared to the HII-region\n",
    "small_HII = (catalogue['area_neb']/0.039) / (catalogue['area_asc']/11.95) > 2\n",
    "\n",
    "# distance to centre of galaxy\n",
    "galactic_center = SkyCoord(ra=p['R.A.'],dec=p['Dec.'])\n",
    "catalogue['galactic_radius'] = catalogue['SkyCoord_neb'].separation(galactic_center).to(u.arcmin)\n",
    "\n",
    "# define the criteria which objects we use in the plot\n",
    "criteria = (catalogue['mass']>1e3) & (catalogue['overlap']=='contained') & (catalogue['age']<10) \n",
    "\n",
    "#& catalogue['contained'] #& (catalogue['galactic_radius']>1*u.arcmin)\n",
    "\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HA/FUV vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.errorbar(tmp['age'],tmp['HA/FUV'],fmt='o')\n",
    "            #xerr=tmp['age_err'],yerr=tmp['HA/FUV_err'])\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equivalent width vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(tmp['age'],tmp['eq_width']) #,c=tmp['mass'],vmin=1e5,vmax=3e5)\n",
    "#fig.colorbar(sc,label='mass / Msun')\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['eq_width'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='equivalent width / Angstrom',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_eq_width_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,100]\n",
    "criteria = HII_regions['HA/FUV']<150\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria])\n",
    "\n",
    "x,mean,std = bin_stat(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='equivalent width / Angstrom',ylabel='Halpha / FUV',xlim=xlim,ylim=(0,70))\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_eq_width.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extinction from stars and from nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# we expect EBV_balmer = 2 EBV_stars\n",
    "\n",
    "fig = plt.figure(figsize=(single_column,single_column/1.1))\n",
    "ax = fig.add_subplot()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=\"10%\", pad=0.2,)\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=plt.cm.viridis_r)\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel='E(B-V) stars',ylabel='E(B-V) Balmer')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/f'{name}_EBV_Balmer_vs_Stars.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Plot\n",
    "\n",
    "are there any obvious differences between eg the old, high EW regions versus young, low EW regions?\n",
    "\n",
    "Likewise for the HA/FUV vs HA EQW plot, how much of this correlation is driven by Halpha, rather than a general trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import corner\n",
    "\n",
    "catalogue['HA/NUV'] = catalogue['HA6562_flux'] / catalogue['NUV_FLUX'] / 1e12\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_corner'\n",
    "columns  = ['age','HA/FUV','met_scal','logq_D91']\n",
    "limits   = {'age':(0,10),'eq_width':(0,100),'HA/FUV':(0,50),'HA/NUV':(0,20),'met_scal':(8.4,8.7),'logq_D91':(6,8)}\n",
    "\n",
    "tmp = catalogue[(catalogue['overlap']=='contained') & (catalogue['mass']>1e3)]\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "corner(tmp,columns,limits,nbins=5,filename=filename,vmin=1000,vmax=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "\n",
    "lines = [col for col in catalogue.columns if col.endswith('_flux')]\n",
    "print(f'{len(lines)} different lines with {comb(len(lines),2)} possible combinations')\n",
    "\n",
    "correlation = []\n",
    "for pair in itertools.combinations(lines,2):\n",
    "    not_nan = ~np.isnan(catalogue[pair[0]]) & ~np.isnan(catalogue[pair[1]])\n",
    "    r,p = spearmanr(catalogue['age'],catalogue[pair[0]][not_nan]/catalogue[pair[1]][not_nan])\n",
    "    correlation.append((r,pair))\n",
    "a = [x for x in correlation if np.abs(x[0])>0.15]\n",
    "a.sort(key=lambda x: np.abs(x[0]),reverse=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "rho,(line1,line2) = a[-1]\n",
    "ax.scatter(catalogue['age'],catalogue[line1]/catalogue[line2])\n",
    "ax.set(xlabel='age',ylabel=f\"{line2.split('_')[0]}/{line1.split('_')[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['HA/SII'] = catalogue['HA6562_flux'] / catalogue['SII6716_flux']\n",
    "catalogue['HA/OI'] = catalogue['HA6562_flux'] / catalogue['OI6300_flux']\n",
    "catalogue['HA/SII'][~np.isfinite(catalogue['HA/SII'])] = np.nan\n",
    "catalogue['HA/OI'][~np.isfinite(catalogue['HA/OI'])] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halpha luminosity vs mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1e4,5e5]\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] \n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['mass'],tmp['HA6562_flux'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['HA6562_flux'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='Halpha',\n",
    "       xlim=xlim,ylim=[1e4,1e6],xscale='log',yscale='log')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1,2e5]\n",
    "criteria = (catalogue['age']<20) #& (sep>Angle('3\"'))\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(tmp['mass'],tmp['region_area'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['region_area'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='HII-region area',xlim=xlim)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associated vs isolated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "tmp = associations[(associations['mass']>1e3) & (associations['age']<10)]\n",
    "\n",
    "ages_con = tmp[tmp['overlap']=='contained']['age']\n",
    "ages_par = tmp[tmp['overlap']=='partial']['age']\n",
    "ages_iso = tmp[tmp['overlap']=='isolated']['age']\n",
    "\n",
    "print(f'ages: con={np.mean(ages_con):.2f}, par={np.mean(ages_par):.2f}, iso={np.mean(ages_iso):.2f}')\n",
    "\n",
    "ax1.hist(ages_con,bins=bins,histtype='step',label='contained')\n",
    "ax2.hist(ages_par,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages_iso,bins=bins,histtype='step',label='isolated')\n",
    "\n",
    "ax1.set_title(f'contained ({np.mean(ages_con):.2f} Myr)')\n",
    "ax2.set_title(f'partially ({np.mean(ages_par):.2f} Myr)')\n",
    "ax3.set_title(f'isolated ({np.mean(ages_iso):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,120],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_age_hist_contained.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "tmp = associations[(associations['mass']>1e3) & (associations['age']<10)]\n",
    "idx,sep,_=match_coordinates_sky(tmp['SkyCoord'],nebulae['SkyCoord'])\n",
    "\n",
    "ages1 = tmp[(sep<0.4*u.arcsec)]['age']\n",
    "ages2 = tmp[(sep>0.4*u.arcsec) & (sep<0.8*u.arcsec)]['age']\n",
    "ages3 = tmp[(sep>0.8*u.arcsec)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'$s<0.4\"$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'$0.4\"<s<0.8\"$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'$0.8\"<s$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,100],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_age_hist_sep.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different resolutions\n",
    "\n",
    "the associations are for 16pc, 32pc and 64pc resolution. Here we compare them to each other to see how accurate the age dating is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "target = name\n",
    "associations16, associations_mask16 = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,scalepc=16)\n",
    "associations32, associations_mask32 = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,scalepc=32)\n",
    "associations64, associations_mask64 = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,scalepc=64)\n",
    "\n",
    "associations16.rename_columns(list(associations16.columns),[x+'_16' for x in associations16.columns])\n",
    "associations32.rename_columns(list(associations32.columns),[x+'_32' for x in associations32.columns])\n",
    "associations64.rename_columns(list(associations64.columns),[x+'_64' for x in associations64.columns])\n",
    "\n",
    "\n",
    "associations16.add_index('assoc_ID_16')\n",
    "associations32.add_index('assoc_ID_32')\n",
    "associations64.add_index('assoc_ID_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_catalogues_with_masks(mask1,mask2):\n",
    "    '''find the overlap between two masks\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    scale = 10**np.ceil(np.log10(max(mask1[~np.isnan(mask1)])))\n",
    "    s_arr = mask1.data/scale+mask2\n",
    "\n",
    "    # ids of associations, nebulae and combination (sum) of both\n",
    "    id1 = np.unique(mask1[~np.isnan(mask1)]).astype(int)\n",
    "    id2 = np.unique(mask2[~np.isnan(mask2)]).astype(int)\n",
    "    # the labels of the added masks\n",
    "    id3 = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "    # this splits the sum into two parts (nebulae and associations)\n",
    "    modf1,modf2 = np.modf(id3)\n",
    "    modf2 = modf2.astype(int)\n",
    "    modf1 = np.round(modf1*scale).astype(int)\n",
    "\n",
    "    unique1, count1 = np.unique(modf1,return_counts=True)\n",
    "    unique1, count2 = np.unique(modf2,return_counts=True)\n",
    "\n",
    "    dict1 = {int(n) : modf2[modf1==n].tolist() for n in id1}     \n",
    "    dict2 = {int(n) : modf1[modf2==n].tolist() for n in id2}     \n",
    "                \n",
    "    return dict1, dict2\n",
    "\n",
    "dict_16_32, dict_32_16 = match_catalogues_with_masks(associations_mask16.data,\n",
    "                                                     associations_mask32.data)\n",
    "dict_32_64, dict_64_32 = match_catalogues_with_masks(associations_mask32.data,\n",
    "                                                     associations_mask64.data)\n",
    "dict_64_16, dict_16_64 = match_catalogues_with_masks(associations_mask64.data,\n",
    "                                                     associations_mask16.data)\n",
    "\n",
    "isolated_16_32 = set()\n",
    "isolated_32_16 = set()\n",
    "for n,v in dict_16_32.items():\n",
    "    if len(v)==1:\n",
    "        if len(dict_32_16[v[0]])==1:\n",
    "            isolated_16_32.add(n)\n",
    "            isolated_32_16.add(v[0])\n",
    "\n",
    "isolated_32_64 = set()\n",
    "isolated_64_32 = set()\n",
    "for n,v in dict_32_64.items():\n",
    "    if len(v)==1:\n",
    "        if len(dict_64_32[v[0]])==1:\n",
    "            isolated_32_64.add(n)\n",
    "            isolated_64_32.add(v[0])\n",
    "            \n",
    "isolated_64_16 = set()\n",
    "isolated_16_64 = set()\n",
    "for n,v in dict_64_16.items():\n",
    "    if len(v)==1:\n",
    "        if len(dict_16_64[v[0]])==1:\n",
    "            isolated_64_16.add(n)\n",
    "            isolated_16_64.add(v[0])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the subsample from the association catalogues\n",
    "sub32 = associations32.copy() #[np.isin(associations32['assoc_ID_32'],list(isolated_32_16 | isolated_32_64))].copy()\n",
    "sub16 = associations16[np.isin(associations16['assoc_ID_16'],list(isolated_16_32))].copy()\n",
    "sub64 = associations64[np.isin(associations64['assoc_ID_64'],list(isolated_64_32))].copy()\n",
    "\n",
    "#del sub32['SkyCoord_32']\n",
    "del sub16['SkyCoord_16']\n",
    "del sub64['SkyCoord_64']\n",
    "\n",
    "sub16['assoc_ID_32'] = [dict_16_32[x][0] for x in sub16['assoc_ID_16']]\n",
    "sub64['assoc_ID_32'] = [dict_64_32[x][0] for x in sub64['assoc_ID_64']]\n",
    "\n",
    "sub32 = join(sub32,sub16,keys=['assoc_ID_32'],join_type='outer')\n",
    "sub32 = join(sub32,sub64,keys=['assoc_ID_32'],join_type='outer')\n",
    "sub32 = sub32.filled(np.nan)\n",
    "sub32.add_index('assoc_ID_32')\n",
    "\n",
    "# for associations that overlap with multiple associations we take the mean\n",
    "for assoc_ID_32 in sub32[np.isnan(sub32['assoc_ID_16'])]['assoc_ID_32']:\n",
    "    if len(dict_32_16[assoc_ID_32])>1:\n",
    "        ages = associations16[np.isin(associations16['assoc_ID_16'],dict_32_16[assoc_ID_32])]['age_16']\n",
    "        sub32.loc[assoc_ID_32]['age_16'] = np.mean(ages)\n",
    "        \n",
    "for assoc_ID_32 in sub32[np.isnan(sub32['assoc_ID_64'])]['assoc_ID_32']:\n",
    "    if len(dict_32_64[assoc_ID_32])>1:\n",
    "        ages = associations64[np.isin(associations64['assoc_ID_64'],dict_32_64[assoc_ID_32])]['age_64']\n",
    "        sub32.loc[assoc_ID_32]['age_64'] = np.mean(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'map_nebulae_association'/f'{name}_32pc_associations.fits') as hdul:\n",
    "    assoc_tmp = Table(hdul[1].data)\n",
    "print(f'original length: {len(assoc_tmp)}')\n",
    "#del assoc_tmp['uniform_age']\n",
    "\n",
    "sub32.rename_column('assoc_ID_32','assoc_ID')\n",
    "assoc_tmp = join(assoc_tmp,sub32['assoc_ID','assoc_ID_16','assoc_ID_64','age_16','age_64'],keys=['assoc_ID'])\n",
    "print(f'final length: {len(assoc_tmp)}')\n",
    "\n",
    "#hdu = fits.BinTableHDU(assoc_tmp,name='joined catalogue')\n",
    "#hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_32pc_associations.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = np.abs(sub32['age_32']-sub32['age_16'])>sub32['age_err_32']\n",
    "criteria |= np.abs(sub32['age_32']-sub32['age_64'])>sub32['age_err_32']\n",
    "#sub32[~criteria][['assoc_ID_16','assoc_ID_32','assoc_ID_64','age_16','age_32','age_64','age_err_32']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove associations with different ages \n",
    "uniform_age32 = []\n",
    "for k in dict_32_16.keys():\n",
    "    age     = associations32.loc[k]['age_32']\n",
    "    age_err = associations32.loc[k]['age_err_32']\n",
    "    for a16 in dict_32_16[k]:\n",
    "        if np.abs(associations16.loc[a16]['age_16']-age)>age_err:\n",
    "            break\n",
    "    else:\n",
    "        for a64 in dict_32_64[k]:\n",
    "            if np.abs(associations64.loc[a64]['age_64']-age)>age_err:\n",
    "                break\n",
    "        else:\n",
    "            uniform_age32.append(k)\n",
    "print(f'32pc: {name}: {len(uniform_age32)} of {len(dict_32_16)} associations have uniform ages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the ages from the different resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,10]\n",
    "ylim = xlim\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.scatter(sub32['age_16'],sub32['age_32'],alpha=0.2,edgecolor=None)\n",
    "ax1.set(xlim=xlim,ylim=ylim,xlabel='age / Myr (16pc resolution)',ylabel='age / Myr (32pc resolution)')\n",
    "\n",
    "ax2.scatter(sub32['age_32'],sub32['age_64'],alpha=0.2,edgecolor=None)\n",
    "ax2.set(xlim=xlim,ylim=ylim,xlabel='age / Myr (32pc resolution)',ylabel='age / Myr (64pc resolution)')\n",
    "\n",
    "ax3.scatter(sub32['age_64'],sub32['age_16'],alpha=0.2,edgecolor=None)\n",
    "ax3.set(xlim=xlim,ylim=ylim,xlabel='age / Myr (64pc resolution)',ylabel='age / Myr (16pc resolution)')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/target/f'{name}_ages_by_resolution.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-9.5,10,1)\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.hist(sub32['age_16']-sub32['age_32'],bins=bins)\n",
    "#ax1.set_title(f\"median={np.nanmedian(sub32['age_16']-sub32['age_32']):.3f}\")\n",
    "ax1.set(xlabel=r'age$_{16\\mathrm{pc}}-$age$_{32\\mathrm{pc}}$ / Myr')\n",
    "\n",
    "ax2.hist(sub32['age_32']-sub32['age_64'],bins=bins)\n",
    "#ax2.set_title(f\"median={np.nanmedian(sub32['age_32']-sub32['age_64']):.3f}\")\n",
    "ax2.set(xlabel=r'age$_{32\\mathrm{pc}}-$age$_{64\\mathrm{pc}}$ / Myr')\n",
    "\n",
    "ax3.hist(sub32['age_64']-sub32['age_16'],bins=bins)\n",
    "#ax3.set_title(f\"median={np.nanmedian(sub32['age_64']-sub32['age_16']):.3f}\")\n",
    "ax3.set(xlabel=r'age$_{64\\mathrm{pc}}-$age$_{16\\mathrm{pc}}$ / Myr')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/target/f'{name}_ages_by_resolution_hist.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age difference vs U-band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.scatter(sub32['age_16']-sub32['age_32'],sub32['U_dolmag_vega_16']-sub32['U_dolmag_vega_32'])\n",
    "ax1.set(ylabel=r'U$_{16\\mathrm{pc}}-$U$_{32\\mathrm{pc}}$',\n",
    "        xlabel=r'age$_{16\\mathrm{pc}}-$age$_{32\\mathrm{pc}}$ / Myr',\n",
    "        xlim=[-100,100])\n",
    "\n",
    "ax2.scatter(sub32['age_32']-sub32['age_64'],sub32['U_dolmag_vega_32']-sub32['U_dolmag_vega_64'])\n",
    "ax2.set(ylabel=r'U$_{32\\mathrm{pc}}-$U$_{64\\mathrm{pc}}$',\n",
    "        xlabel=r'age$_{32\\mathrm{pc}}-$age$_{64\\mathrm{pc}}$ / Myr',\n",
    "        xlim=[-100,100])\n",
    "\n",
    "ax3.scatter(sub32['age_64']-sub32['age_16'],sub32['U_dolmag_vega_64']-sub32['U_dolmag_vega_16'])\n",
    "ax3.set(ylabel=r'U$_{64\\mathrm{pc}}-$U$_{16\\mathrm{pc}}$',\n",
    "        xlabel=r'age$_{64\\mathrm{pc}}-$age$_{16\\mathrm{pc}}$ / Myr',\n",
    "        xlim=[-100,100])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/target/f'{name}_ages_by_resolution_vs_U_mag.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a cutout with all three masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations16, associations_mask16 = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,\n",
    "                                                    HSTband=HSTband,scalepc=16,version=version)\n",
    "\n",
    "associations32, associations_mask32 = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,\n",
    "                                                    HSTband=HSTband,scalepc=32,version=version)\n",
    "\n",
    "associations64, associations_mask64 = read_associations(folder=data_ext/'Products'/'stellar_associations',target=target,\n",
    "                                                    HSTband=HSTband,scalepc=64,version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "from astrotools.plot import create_RGB\n",
    "\n",
    "sub = catalogue[(catalogue['overlap']=='contained') & (catalogue['neighbors']==0)]\n",
    "position = sub['SkyCoord_neb'][4]\n",
    "#position = associations16.loc[5]['SkyCoord']\n",
    "size = 4*u.arcsec\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "cutout_F275 = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "norm = simple_norm(cutout_F275.data,stretch='linear',clip=False,percent=99.9)\n",
    "\n",
    "cutout_CO, _  = reproject_interp(CO,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "cutout_Halpha, _  = reproject_interp(Halpha,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "\n",
    "rgb = create_RGB(cutout_CO,cutout_Halpha,cutout_F275.data,\n",
    "                 percentile=[98,98,99.8],weights=[0.7,0.6,1])\n",
    "#ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "# plot the nebulae catalogue\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=1,label='HII-region')\n",
    "\n",
    "mask = np.zeros((*cutout_mask.shape,4))\n",
    "mask[~np.isnan(cutout_mask.data),:] = (0.17, 0.60, 0.17,0.05)\n",
    "#ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 32 pc\n",
    "cutout_32 = Cutout2D(associations_mask32.data,position,size=size,wcs=associations_mask32.wcs)\n",
    "region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_32.data)\n",
    "    blank_mask[cutout_32.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='blue',lw=1,label='32pc')\n",
    "\n",
    "mask = np.zeros((*cutout_32.shape,4))\n",
    "mask[~np.isnan(cutout_32.data),:] = (0.12,0.47,0.71,0.05)\n",
    "#ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "    \n",
    "#legend_without_duplicate_labels(ax)\n",
    "\n",
    "#t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "#t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "subsample = sub32[~np.isnan(sub32['assoc_ID_16']) & ~np.isnan(sub32['assoc_ID_64'])]\n",
    "position = subsample['SkyCoord_32'][31]\n",
    "position = nebulae.loc[7]['SkyCoord']\n",
    "size = 6*u.arcsec\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "cutout_image = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "norm = simple_norm(cutout_image.data,stretch='linear',clip=False,percent=99.9)\n",
    "ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "\n",
    "# plot the nebulae catalogue\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_image.wcs,shape_out=cutout_image.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    #ax.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.5,label='HII-region')\n",
    "    pass\n",
    "mask = np.zeros((*cutout_mask.shape,4))\n",
    "mask[~np.isnan(cutout_mask.data),:] = (0.84, 0.15, 0.16,0.1)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 16 pc\n",
    "cutout_16 = Cutout2D(associations_mask16.data,position,size=size,wcs=associations_mask16.wcs)\n",
    "region_ID = np.unique(cutout_16.data[~np.isnan(cutout_16.data)])\n",
    "print('16pc: ' + str(region_ID))\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_16.data)\n",
    "    blank_mask[cutout_16.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.5,label='16pc')\n",
    "\n",
    "mask = np.zeros((*cutout_16.shape,4))\n",
    "mask[~np.isnan(cutout_16.data),:] = (0.12,0.47,0.71,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 32 pc\n",
    "cutout_32 = Cutout2D(associations_mask32.data,position,size=size,wcs=associations_mask32.wcs)\n",
    "region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "print('32pc: ' + str(region_ID))\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_32.data)\n",
    "    blank_mask[cutout_32.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=0.5,label='32pc')\n",
    "\n",
    "mask = np.zeros((*cutout_32.shape,4))\n",
    "mask[~np.isnan(cutout_32.data),:] = (0.17, 0.60, 0.17,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 64 pc\n",
    "cutout_64 = Cutout2D(associations_mask64.data,position,size=size,wcs=associations_mask64.wcs)\n",
    "region_ID = np.unique(cutout_64.data[~np.isnan(cutout_64.data)])\n",
    "print('64pc: ' + str(region_ID))\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_64.data)\n",
    "    blank_mask[cutout_64.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:orange',lw=0.5,label='64pc')\n",
    "\n",
    "mask = np.zeros((*cutout_64.shape,4))\n",
    "mask[~np.isnan(cutout_64.data),:] = (0.96, 0.48, 0.053,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "    \n",
    "legend_without_duplicate_labels(ax)\n",
    "\n",
    "#t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "#t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure FUV and Halpha at association position\n",
    "\n",
    "### From point\n",
    "\n",
    "If I select an aperture smaller than a pixel, the measured flux is directly proportional to the apertuer size. Therefore it doesn't matter that the astrosat resolution is much worse than HST or MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "\n",
    "extinction_model = CCM89(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    if plot:\n",
    "        fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "        ax1.hist(EBV_rand[:,0],bins=100)\n",
    "        ax1.axvline(EBV[0],color='black')\n",
    "        ax1.set(xlabel='E(B-V)')\n",
    "        ax2.hist(ext_arr[:,0],bins=100)\n",
    "        ax2.axvline(ext[0],color='black')\n",
    "        ax2.set(xlabel='extinction')\n",
    "        plt.show()\n",
    " \n",
    "    return ext,ext_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import SkyCircularAperture,SkyCircularAnnulus,aperture_photometry\n",
    "\n",
    "criteria = np.isin(associations['cluster_ID'],isolated_assoc)\n",
    "\n",
    "aperture_size = 1*u.arcsecond\n",
    "positions = associations['SkyCoord'][criteria]\n",
    "\n",
    "aperture = SkyCircularAperture(positions,aperture_size)\n",
    "\n",
    "fluxes = associations[['cluster_ID','SkyCoord','age','age_err','mass','mass_err','EBV','EBV_err']][criteria]\n",
    "fluxes['FUV'] = 1e20*aperture_photometry(astrosat,aperture)['aperture_sum']\n",
    "fluxes['HA'] = aperture_photometry(Halpha,aperture)['aperture_sum']\n",
    "\n",
    "\n",
    "\n",
    "# because the HII-regions are sometimes extended and not circular, this is probably not sufficient\n",
    "'''\n",
    "r_in,r_out = 5*u.arcsec,8*u.arcsec\n",
    "A_circle  = np.pi*aperture_size**2\n",
    "A_annulus = np.pi*(r_out**2-r_in**2)\n",
    "annulus_aperture = SkyCircularAnnulus(positions,r_in=r_in, r_out=r_out)\n",
    "\n",
    "\n",
    "fluxes['FUV_bkg'] = 1e20*aperture_photometry(astrosat,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['HA_bkg'] = aperture_photometry(Halpha,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['FUV'] = fluxes['FUV']-fluxes['FUV_bkg']\n",
    "fluxes['HA']  = fluxes['HA']-fluxes['HA_bkg']\n",
    "'''\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. FUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=1481*u.angstrom)\n",
    "fluxes['FUV'] = fluxes['FUV'] / extinction_mw \n",
    "fluxes['FUV_CORR'] = fluxes['FUV'] / ext_int \n",
    "\n",
    "# the Halpha line maps are already MW extinction corrected\n",
    "ext_int,ext_int_err = extinction(2*associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=6562*u.angstrom)\n",
    "fluxes['HA_CORR'] = fluxes['HA'] / ext_int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA']/fluxes['FUV'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA']/fluxes['FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,125])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,70])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From mask\n",
    "\n",
    "because the resolution of MUSE and astrosat is so much worse than HST, many associations won't be resolved and hence we can not measure the fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "associations_muse, _  = reproject_interp(associations_mask,output_projection=Halpha.wcs,shape_out=Halpha.data.shape,order='nearest-neighbor') \n",
    "associations_astro, _ = reproject_interp(associations_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(set(np.unique(associations_muse[~np.isnan(associations_muse)])) & set(np.unique(associations_astro[~np.isnan(associations_astro)])))\n",
    "sample.sort()\n",
    "HA_flux = [np.sum(Halpha.data[associations_muse==cluster_ID]) for cluster_ID in sample]\n",
    "FUV_flux = [np.sum(astrosat.data[associations_astro==cluster_ID]) for cluster_ID in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "fluxes = Table([sample,HA_flux,FUV_flux],names=['cluster_ID','HA','FUV'])\n",
    "catalogue = join(associations,fluxes[(~np.isnan(HA_flux)) & (~np.isnan(FUV_flux))],keys='cluster_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "xlim=[0,100]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(catalogue['age'],catalogue['HA']/catalogue['FUV'])\n",
    "\n",
    "ax.set(xlim=xlim,xlabel='age/Myr',ylabel='Ha/FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem with the resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we compare what happens if we reproject the nebulae mask and the Halpha mask to HST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp, reproject_exact\n",
    "\n",
    "Halpha_hst = reproject_exact(Halpha,associations_mask.meta,\n",
    "                             return_footprint=False)\n",
    "nebulae_mask_hst = reproject_interp(nebulae_mask,output_projection=associations_mask.meta,\n",
    "                                    order='nearest-neighbor',return_footprint=False)\n",
    "\n",
    "Halpha_fluxes_hst = np.array([np.sum(Halpha_hst[nebulae_mask_hst==region_ID]) for region_ID in nebulae['region_ID']])\n",
    "Halpha_fluxes = np.array([np.sum(Halpha.data[nebulae_mask.data==region_ID]) for region_ID in nebulae['region_ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSE_to_HST = (associations_mask.wcs.pixel_scale_matrix[0][0] / Halpha.wcs.pixel_scale_matrix[0][0])**2\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "lim = [1e3,5e7]\n",
    "\n",
    "ax.scatter(Halpha_fluxes,MUSE_to_HST*np.array(Halpha_fluxes_hst))\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.set(xlim=lim,ylim=lim,xscale='log',yscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we reproject the association masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations_mask_muse = reproject_interp(associations_mask,output_projection=Halpha.meta,\n",
    "                                          order='nearest-neighbor',return_footprint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Halpha_fluxes_association_hst = np.array([np.sum(Halpha_hst[associations_mask.data==assoc_ID]) for assoc_ID in associations['assoc_ID']])\n",
    "Halpha_fluxes_associations_muse = np.array([np.sum(Halpha.data[associations_mask_muse==assoc_ID]) for assoc_ID in associations['assoc_ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSE_to_HST = (associations_mask.wcs.pixel_scale_matrix[0][0] / Halpha.wcs.pixel_scale_matrix[0][0])**2\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "lim = [1e3,8e7]\n",
    "\n",
    "ax.scatter(Halpha_fluxes_association_hst,Halpha_fluxes_associations_muse/MUSE_to_HST)\n",
    "ax.plot(lim,lim,color='black')\n",
    "ax.set(xlim=lim,ylim=lim,xscale='log',yscale='log',\n",
    "       xlabel=r'H$\\alpha$ reproject map',ylabel=r'H$\\alpha$ reproject mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp, reproject_exact\n",
    "from astropy.nddata import block_replicate\n",
    "\n",
    "nebulae_astrosat, _ = reproject_interp(nebulae_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor') \n",
    "astro_MUSE, _ = reproject_exact(astrosat,output_projection=Halpha.wcs,shape_out=Halpha.data.shape)    \n",
    "asttro_fine = block_replicate(astrosat,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_ID = 13\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "size = 4*u.arcsecond\n",
    "\n",
    "nebulae_cutout       = Cutout2D(nebulae_mask.data,position,size=size,wcs=nebulae_mask.wcs)\n",
    "astrosat_cutout_MUSE = Cutout2D(astro_MUSE,position,size=size,wcs=Halpha.wcs)\n",
    "\n",
    "nebulae_cutout_astrosat = Cutout2D(nebulae_astrosat,position,size=size,wcs=astrosat.wcs)\n",
    "astrosat_cutout = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)\n",
    "\n",
    "astrosat_up = block_replicate(astrosat_cutout.data,4)\n",
    "nebulae_fine, _ = reproject_interp(nebulae_mask,output_projection=astrosat_cutout.wcs,shape_out=astrosat_up.shape,order='nearest-neighbor') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "neb_contours = []\n",
    "for i in np.unique(nebulae_cutout.data[~np.isnan(nebulae_cutout.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout.data)\n",
    "    blank_mask[nebulae_cutout.data==i] = 1\n",
    "    neb_contours += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "neb_contours_astrosat = []\n",
    "for i in np.unique(nebulae_cutout_astrosat.data[~np.isnan(nebulae_cutout_astrosat.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout_astrosat.data)\n",
    "    blank_mask[nebulae_cutout_astrosat.data==i] = 1\n",
    "    neb_contours_astrosat += find_contours(blank_mask, 0.5)\n",
    "\n",
    "neb_contours_fine = []\n",
    "for i in np.unique(nebulae_fine[~np.isnan(nebulae_fine)]):\n",
    "    blank_mask = np.zeros_like(nebulae_fine)\n",
    "    blank_mask[nebulae_fine==i] = 1\n",
    "    neb_contours_fine += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "norm1 = simple_norm(astrosat_cutout.data,clip=False,percent=99)\n",
    "ax1.imshow(astrosat_cutout.data,norm=norm1)\n",
    "\n",
    "norm2 = simple_norm(astrosat_cutout_MUSE.data,clip=False,percent=99)\n",
    "ax2.imshow(astrosat_cutout_MUSE.data,norm=norm2)\n",
    "\n",
    "norm3 = simple_norm(astrosat_up,clip=False,percent=99)\n",
    "ax3.imshow(astrosat_up,norm=norm3)\n",
    "\n",
    "for coords in neb_contours:\n",
    "    ax2.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.8)\n",
    "for coords in neb_contours_astrosat:\n",
    "    ax1.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)   \n",
    "for coords in neb_contours_fine:\n",
    "    ax3.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)  \n",
    "    \n",
    "ax1.set_title('original astrosat resolution')\n",
    "ax2.set_title('interpolated to MUSE resolution')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'interpolate regions: {np.sum(astrosat_cutout.data[nebulae_cutout_astrosat.data==region_ID]):.2g}')\n",
    "print(f'interpolate astrosat: {np.sum(np.sum(astrosat_cutout_MUSE.data[nebulae_cutout.data==region_ID])):.2g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_exact\n",
    "\n",
    "ast_small , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(101,101)) \n",
    "ast_large , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(202,202)) \n",
    "ast_org = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "ax1.imshow(ast_org.data)\n",
    "ax2.imshow(ast_small)\n",
    "ax3.imshow(ast_large)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "\n",
    "\n",
    "astrosat_upsampled = block_replicate(astrosat,4,conserve_sum=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb\n",
    "\n",
    "from photutils import SkyCircularAperture, SkyCircularAnnulus, ApertureStats\n",
    "from astropy.stats import SigmaClip\n",
    "\n",
    "from photutils import SkyCircularAperture, SkyCircularAnnulus, ApertureStats\n",
    "from astropy.stats import SigmaClip\n",
    "\n",
    "# we use this function to check if a point is in the FOV\n",
    "def get_value(matrix, index, default_value=np.nan):\n",
    "    '''\n",
    "    The `to_pixel` method returns the x,y coordinates. However in the \n",
    "    image they correspond to img[y,x]\n",
    "    '''\n",
    "    result = np.zeros(len(index))+default_value\n",
    "    mask = (index[:,1] < matrix.shape[0]) & (index[:,0] < matrix.shape[1])\n",
    "    mask &= (index[:,1] >= 0) & (index[:,0] >=0)\n",
    "\n",
    "    valid = index[mask]\n",
    "    result[mask] = matrix[valid[:,1], valid[:,0]]\n",
    "    return result\n",
    "\n",
    "r     = .8*u.arcsec\n",
    "r_in  = 1.5*u.arcsec\n",
    "r_out = 2*u.arcsec\n",
    "\n",
    "# Milky Way E(B-V) from  Schlafly & Finkbeiner (2011)\n",
    "EBV_MW = {'IC5332': 0.015,'NGC0628': 0.062,'NGC1087': 0.03,'NGC1300': 0.026,\n",
    "          'NGC1365': 0.018,'NGC1385': 0.018,'NGC1433': 0.008,'NGC1512': 0.009,\n",
    "          'NGC1566': 0.008,'NGC1672': 0.021,'NGC2835': 0.089,'NGC3351': 0.024,\n",
    "          'NGC3627': 0.037,'NGC4254': 0.035,'NGC4303': 0.02,'NGC4321': 0.023,\n",
    "          'NGC4535': 0.017,'NGC5068': 0.091,'NGC7496': 0.008}\n",
    "\n",
    "sample = ['IC5332','NGC0628','NGC1087','NGC1300','NGC1365','NGC1385',\n",
    "          'NGC1433','NGC1512','NGC1566','NGC1672','NGC2835','NGC3351',\n",
    "          'NGC3627','NGC4254','NGC4303','NGC4321','NGC4535','NGC5068',\n",
    "          'NGC7496']\n",
    "\n",
    "sigclip = SigmaClip(sigma=3.0, maxiters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure NUV in nebula mask\n",
    "\n",
    "to see if we miss ionizing sources inside the nebula mask that are not part of the associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "NUV_inside_assoc = [np.sum(F275.data[(nebulae_hst.data==row['region_ID']) & (~np.isnan(associations_mask.data))]) for row in tqdm(nebulae)]\n",
    "NUV_outside_assoc = [np.sum(F275.data[(nebulae_hst.data==row['region_ID']) & (np.isnan(associations_mask.data))]) for row in tqdm(nebulae)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(NUV_inside_assoc)/np.array(NUV_outside_assoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(NUV,associations['NUV_FLUX'])\n",
    "plt.ylim([1e-10,3e-7])\n",
    "plt.xlim([0,300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starburst99\n",
    "\n",
    "compare our observations with simulated data\n",
    "\n",
    "**Note**: the GENEVAHIGH 23 (Z=0.008) model used a metallicity of 0.02 for the high resolution models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Star Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model, make_folder, list_available_models\n",
    "\n",
    "cluster_CSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'continuous_SF')\n",
    "cluster_SSF = Cluster(basedir/'..'/'starburst'/'data'/'others'/'single_burst_SF')\n",
    "cluster_SSF.measure_FUV()\n",
    "cluster_CSF.measure_FUV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Halpha/FUV\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_CSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = np.interp(cluster_CSF.ewidth['Time'],cluster_CSF.FUV['Time'],cluster_CSF.FUV['FUV'])\n",
    "ax1.plot(time,Ha/FUV,label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "Ha  = cluster_SSF.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster_SSF.FUV['FUV'].copy()\n",
    "ax1.plot(time,Ha/FUV,label='single burst')\n",
    "ax1.legend()\n",
    "ax1.set(xlim=[0,10],xlabel='age / Myr',ylabel=r'H$\\alpha$/FUV')\n",
    "\n",
    "\n",
    "time = cluster_CSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_CSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,(eq_width.value),label='continuous')\n",
    "\n",
    "time = cluster_SSF.ewidth['Time']/1e6\n",
    "eq_width  = cluster_SSF.ewidth['eq_width_H_A']\n",
    "ax2.plot(time,(eq_width.value),label='singel burst')\n",
    "ax2.legend()\n",
    "ax2.set(xlim=[0,10],yscale='linear',xlabel='age / Myr',ylabel=r'EW(H$\\alpha$)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test what multiple stellar populations would look like\n",
    "\n",
    "dT = 0.5*np.array([1]) * u.Myr\n",
    "\n",
    "time = cluster.ewidth['Time']\n",
    "Ha  = cluster.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster.FUV['FUV'].copy()\n",
    "\n",
    "fig,(ax1,ax2,ax3) =plt.subplots(ncols=3,figsize=(1.5*two_column,two_column/2))\n",
    "\n",
    "if False:\n",
    "    ax1.plot(time.value/1e6,Ha,label=f'{0*u.Myr}')\n",
    "    ax2.plot(time.value/1e6,FUV,label=f'{0*u.Myr}')   \n",
    "    ax3.plot(time.value/1e6,Ha/FUV,label=f'{0*u.Myr}')   \n",
    "\n",
    "for t in dT:\n",
    "    c0 = cluster.time_shift(t)\n",
    "    Ha_new  = np.interp(time,c0.ewidth['Time'],c0.ewidth['Luminosity_H_A'],left=0,right=0)\n",
    "    FUV_new = np.interp(time,c0.FUV['Time'],c0.FUV['FUV'],left=0,right=0)   \n",
    "     \n",
    "    ax1.axvline(t.value,color='black',ls='--')\n",
    "    ax2.axvline(t.value,color='black',ls='--')\n",
    "    #ax3.axvline(t.value,color='black',ls='--')\n",
    "    \n",
    "    if False:\n",
    "        ax1.plot(c0.ewidth['Time'].value/1e6,c0.ewidth['Luminosity_H_A'],label=f'{t}')\n",
    "        ax2.plot(c0.FUV['Time'].value/1e6,c0.FUV['FUV'],label=f'{t}')   \n",
    "        ax3.plot(c0.FUV['Time'].value/1e6,c0.ewidth['Luminosity_H_A']/c0.FUV['FUV'],label=f'{t}')    \n",
    "\n",
    "    Ha += Ha_new\n",
    "    FUV += FUV_new\n",
    "    \n",
    "ax1.plot(time.value/1e6,Ha,label='sum')\n",
    "ax2.plot(time.value/1e6,FUV,label='sum')   \n",
    "ax3.plot(time.value/1e6,Ha/FUV,label='sum')   \n",
    "    \n",
    "ax1.set(xlabel='time / Myr',ylabel='Halpha',xlim=[0,20])\n",
    "ax2.set(xlabel='time / Myr',ylabel='FUV',xlim=[0,20])\n",
    "ax3.set(xlabel='time / Myr',ylabel='Halpha / FUV',xlim=[0,20])\n",
    "\n",
    "ax1.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __add__(table1,table2):\n",
    "    '''add two clusters with different ages\n",
    "    \n",
    "    this function takes the starburst\n",
    "    '''\n",
    "    \n",
    "    time1 = table1['Time']\n",
    "    \n",
    "    time2 = table2['Time']\n",
    "    \n",
    "    HI_rate = np.interp(time1,time2,table2['HI_rate'],left=0,right=0)\n",
    "    \n",
    "    plt.plot(time1,table1['HI_rate'])\n",
    "    plt.plot(time2,table2['HI_rate'])\n",
    "    plt.plot(time1,table1['HI_rate']+HI_rate)\n",
    "    plt.plot(time1,HI_rate)\n",
    "    plt.show()\n",
    "                                           \n",
    "    return 0\n",
    "    \n",
    "__add__(cluster.quanta,c2.quanta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) =plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Equ_width_H_A'],color='black')\n",
    "ax2.set(ylabel='eq width / Angstrom',xlabel='time / Myr',xlim=[0,10])\n",
    "\n",
    "ax3.plot(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "sc = ax3.scatter(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],\n",
    "                 c=cluster.ewidth['Time']/1e6,vmin=0,vmax=10)\n",
    "fig.colorbar(sc,ax=ax3,label='age / Myr')\n",
    "ax3.set(ylabel='Halpha/ FUV',xlabel='eq width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'SB99_age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of ionizing photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue.add_index('region_ID')\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "for region_ID in catalogue['region_ID']:\n",
    "    # the masses in the catalogue are off by a factor of 10\n",
    "    mass = 0.1*catalogue['mass'][catalogue['region_ID']==region_ID]\n",
    "    age  = catalogue['age'][catalogue['region_ID']==region_ID]*u.Myr\n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    idx = np.argmin(np.abs(scaled_cluster.quanta['Time']-age))\n",
    "    catalogue.loc[region_ID]['Qpredicted'] = scaled_cluster.quanta['HI_rate'][idx].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the conversion factor is from Niederhofer+2016\n",
    "\n",
    "$$\n",
    "Q(\\mathrm{H}\\alpha) = 7.31\\cdot 10^{11} L(\\mathrm{H}\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed\n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_flux']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*Distance(distmod=p['(m-M)'])**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['age']<6) & (catalogue['age']>0)]\n",
    "# fesc = (Qpredicted-Qobserved) / Qpredicted \n",
    "fesc = (tmp['Qpredicted']-tmp['Qobserved'])/tmp['Qpredicted']\n",
    "\n",
    "print(f\"{np.sum(fesc<0)} of {len(tmp)} regions have negative fesc\")\n",
    "#Ha_from_q = (catalogue['Q']*1.37e-12*u.erg/u.s / (4*np.pi*Distance(distmod=p['(m-M)'])**2)).to(u.erg/u.s/u.cm**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "Qpredicted = np.logspace(-2,2)\n",
    "\n",
    "for f in [0.0,0.5,0.9,0.99]:\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    ax.plot(Qpredicted,Qobserved,label=f'fesc={f}',zorder=1)\n",
    "\n",
    "sc=ax.scatter(tmp['Qpredicted']/1e50,tmp['Qobserved']/1e50,\n",
    "           c=tmp['age'],cmap=plt.cm.copper,vmin=0,vmax=6,s=2,zorder=2)\n",
    "ax.legend()\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "#ax.plot([0,100],np.array([10,110]),color='gray',ls='--')\n",
    "#ax.plot([0,100],np.array([-10,90]),color='gray',ls='--')\n",
    "\n",
    "ax.set(xlabel=r'$Q_{\\mathrm{H}\\alpha}$ / $10^{50} \\mathrm{s}^{-1}$ predicted',\n",
    "       ylabel='$Q$ / $10^{50} \\mathrm{s}^{-1}$ observed',\n",
    "       xscale='log',yscale='log',xlim=[1e-2,1e2],ylim=[1e-2,1e2])\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_fesc.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.hist(fesc,bins=np.arange(0,1.1,0.05))\n",
    "ax.set(xlim=[0,1.1],xlabel='fesc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.scatter(tmp['galactic_radius'],fesc)\n",
    "ax.set(ylim=[0,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter response curve\n",
    "\n",
    "to get the FUV flux by integrating the spectrum. The curves are from the [astrosat website](https://uvit.iiap.res.in/Instrument/Filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speclite.filters import FilterResponse, load_filters, plot_filters\n",
    "\n",
    "response_curve = ascii.read(basedir/'data'/'external'/'astrosat_response_curve.txt',\n",
    "                                     names=['wavelength','EA','Filter'])\n",
    "\n",
    "F148W_mask = response_curve['Filter']=='F148W'\n",
    "F148W_lam = response_curve['wavelength'][F148W_mask]*u.angstrom\n",
    "F148W_res = response_curve['EA'][F148W_mask] / max(response_curve['EA'][F148W_mask])\n",
    "F148W = FilterResponse(F148W_lam,F148W_res,meta=dict(group_name='Astrosat',band_name='F148W'))\n",
    "\n",
    "F154W_mask = response_curve['Filter']=='F154W'\n",
    "F154W_lam  = response_curve['wavelength'][F154W_mask]*u.angstrom\n",
    "F154W_res  = response_curve['EA'][F154W_mask] / max(response_curve['EA'][F154W_mask])\n",
    "F154W = FilterResponse(F154W_lam,F154W_res,meta=dict(group_name='Astrosat',band_name='F154W'))\n",
    "\n",
    "astrosat_filter = load_filters('Astrosat-F148W', 'Astrosat-F154W')\n",
    "plot_filters(astrosat_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Halpha and FUV and EW(Halpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "FUV  = cluster.FUV['FUV']\n",
    "time_FUV = cluster.FUV['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha,color='tab:red')\n",
    "ax1.set_ylabel('Halpha/ (erg/s)',color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "FUV_int = np.interp(time_HA,time_FUV,FUV)\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='tab:blue')\n",
    "ax1.set_ylabel('Halpha / FUV',color='tab:blue')\n",
    "ax1.set(xlim=[0,3],ylim=[1e-5,0.0017],xlabel='Time/Myr')\n",
    "\n",
    "axt = ax1.twinx()\n",
    "quanta = cluster.quanta\n",
    "axt.plot(quanta['Time']/1e6,quanta['HI_rate'],color='tab:orange')\n",
    "axt.set_ylabel('ionizing photons / 1/s',color='tab:orange')\n",
    "axt.set(xlim=[0,10])\n",
    "\n",
    "HI_rate_int = np.interp(time_HA,quanta['Time'],quanta['HI_rate'])\n",
    "\n",
    "ax2.plot(HI_rate_int,Ha/FUV_int,color='black')\n",
    "sc = ax2.scatter(HI_rate_int,Ha/FUV_int,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(sc,cax=cax,label='age / Myr',pad=-1)\n",
    "\n",
    "ax2.set(xlabel='ionizing photons / 1/s',ylabel='Halpha / FUV')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster, find_model\n",
    "\n",
    "fig,(ax1,ax2) =plt.subplots(ncols=2,figsize=(two_column,two_column/2.3))\n",
    "\n",
    "\n",
    "i = 50\n",
    "\n",
    "vmin,vmax = -1.0,0.5\n",
    "cmap = mpl.cm.get_cmap('viridis_r',5)\n",
    "bins = np.array([0.001,0.002,0.008,0.014,0.04,0.05])\n",
    "bins = np.array([0,0.0015,0.005,0.011,0.032,0.048])\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=bins,ncolors=5)\n",
    "\n",
    "for i,ls in zip([60],['-']):\n",
    "    for j in [1,2,3,4,5]:\n",
    "\n",
    "        cluster = Cluster(stellar_model=i+j)\n",
    "        cluster.measure_FUV()\n",
    "        nr, model, metallicity, _ = find_model(i+j)\n",
    "        time = cluster.ewidth['Time']\n",
    "        Ha   = cluster.ewidth['Luminosity_H_A']\n",
    "        FUV  = cluster.FUV['FUV']\n",
    "        eq_HA = cluster.ewidth['eq_width_H_A']\n",
    "\n",
    "        if j==1:  \n",
    "            ax1.plot(time/1e6,Ha/FUV,color=cmap(norm((metallicity))),\n",
    "                     label=model,ls=ls)\n",
    "        else:\n",
    "            ax1.plot(time/1e6,Ha/FUV,color=cmap(norm((metallicity))),ls=ls)\n",
    "            \n",
    "        ax2.plot(time/1e6,eq_HA,color=cmap(norm((metallicity))),ls=ls)\n",
    "    \n",
    "\n",
    "ax1.set(ylabel=r'H$\\alpha$ / FUV',xlabel='time / Myr',xlim=[0,10],ylim=[-5,90])\n",
    "ax2.set(ylabel=r'EW(H$\\alpha$)',xlabel='time / Myr',xlim=[0,10],ylim=[-5,3550])\n",
    "#ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.87, 0.17, 0.02, 0.78])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),spacing='uniform',\n",
    "                    cax=cbar_ax,label=r'$Z$',\n",
    "                    ticks=(bins[1:]+bins[:-1])/2)\n",
    "cbar.ax.set_yticklabels(np.array([0.001,0.002,0.008,0.014,0.04]))\n",
    "plt.savefig(basedir/'reports'/'age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [5e2,1e3,2e3,5e3,1e4,2e4,5e4]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    cl = Cluster(stellar_model=m)\n",
    "    cl.measure_FUV()\n",
    "    clusters[m] = cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "for m in [23,53,63]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax1.legend()\n",
    "\n",
    "for m in [24,54,64]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "\n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax2.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax2.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax2.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot modeled evolution of Ha/FUV and EW(HA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    ionizing = cl.quanta['HI_rate']\n",
    "  \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(Halpha/FUV,ionizing,color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(Halpha/FUV,ionizing,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / FUV',xlabel='ionization')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare observations to simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['distance'] = np.nan\n",
    "for name in np.unique(catalogue['gal_name']):\n",
    "    catalogue['distance'][catalogue['gal_name']==name] = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [2e3,5e3,1e4,2e4,5e4,1e5,2e5,5e5,1e6,2e6,5e6]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "Halpha_FLUX = ((catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "FUV_FLUX = 5e5*((catalogue['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "ax1.scatter(np.log10(FUV_FLUX),np.log10(Halpha_FLUX))\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the grid to compare the observations to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time = len(cluster.ewidth['Time'])\n",
    "n_mass = 1000\n",
    "\n",
    "mass_min = 5e3\n",
    "mass_max = 1e6\n",
    "\n",
    "mass_grid = np.linspace(mass_min,mass_max,n_mass)\n",
    "\n",
    "HA_grid = np.zeros((n_time,n_mass))\n",
    "FUV_grid = np.zeros((n_time,n_mass))\n",
    "\n",
    "for i,mass in enumerate(mass_grid):\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    HA_grid[:,i]  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    FUV_grid[:,i] = scaled_cluster.FUV['FUV']\n",
    "\n",
    "time = scaled_cluster.FUV['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)\n",
    "\n",
    "mass, age, chi2 = [],[],[]\n",
    "for row in catalogue:\n",
    "    \n",
    "    Halpha_FLUX = ((row['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    Halpha_ERR  = ((row['HA6562_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    FUV_FLUX = 1e6*((row['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    FUV_ERR  = ((row['FUV_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    chi2_grid = (Halpha_FLUX-HA_grid)**2/Halpha_ERR**2 + (FUV_FLUX-FUV_grid)**2/FUV_ERR**2\n",
    "    \n",
    "    row,col = np.unravel_index(chi2_grid.argmin(), chi2_grid.shape)\n",
    "    mass.append(mass_grid[col])\n",
    "    age.append(time[row].value)\n",
    "    chi2.append(np.min(chi2_grid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,catalogue['AGE_MINCHISQ'])\n",
    "ax1.set(ylim=[0,30],xlim=[0,30],xlabel='age from Nebulae / Myr',ylabel='age from Cluster / Myr')\n",
    "\n",
    "ax2.scatter(np.array(mass),catalogue['MASS_MINCHISQ'])\n",
    "ax2.set(xlabel='mass from Nebulae / Msun',ylabel='mass from Cluster / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(nrows=1,ncols=1,figsize=(single_column,single_column))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,np.array(mass))\n",
    "ax1.set(xlim=[0,30],ylim=[0,5e5],xlabel='age from Nebulae / Myr',ylabel='mass / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "eqwHA  = cluster.ewidth['Equ_width_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,eqwHA,color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr',ylabel='eq / AA',xlim=[0,10])\n",
    "\n",
    "#ax2 = ax1.twinx() \n",
    "#ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "#ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "#ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    eqwHA  = cl.ewidth['Equ_width_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    #sl = cl.scale(1e5)\n",
    "    #eqwHA  = sl.ewidth['Equ_width_H_A']\n",
    "    #time_HA = sl.ewidth['Time']\n",
    "    #ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),ls='--',color=c)\n",
    "    \n",
    "    \n",
    "ax1.set(xlabel='log (Time / Myr)',ylabel=r'log (W(H$\\alpha$) / $\\AA$)',xlim=[6,7.5])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    HAFUV  = cl.ewidth['Luminosity_H_A'] / cl.FUV['FUV']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    sl = cl.scale(1e5)\n",
    "    HAFUV  = sl.ewidth['Luminosity_H_A'] / sl.FUV['FUV']\n",
    "    time_HA = sl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,ls='--',color=c)    \n",
    "    \n",
    "ax1.set(xlabel='Time / Myr',ylabel=r'HA/FUV',xlim=[0,10])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "fig = plt.figure(figsize=(two_column,two_column*2/3))\n",
    "\n",
    "for i,stellar_model in enumerate(['GENEVASTD','GENEVAHIGH','PADOVASTD','PADOVAAGB','GENEVAv00','GENEVAv40']):\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    \n",
    "    cluster = Cluster(stellar_model=stellar_model,metallicity=0.008)\n",
    "    ax.plot(cluster.quanta['Time']/1e6,cluster.quanta['HI_rate'])\n",
    "    ax.set(xlim=[0,10],ylim=[0,7e52],xlabel='age/Myr',ylabel='HI rate')\n",
    "\n",
    "    ax.set_title(stellar_model)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test mass scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "c1e5 = Cluster(basedir/'..'/'starburst'/'data'/'model54_1e5Msun'/'cluster')\n",
    "c1e6 = Cluster(basedir/'..'/'starburst'/'data'/'model54_1e6Msun'/'cluster')\n",
    "s1e6 = c1e5.scale(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "\n",
    "tname = 'ewidth'\n",
    "xname = 'Time'\n",
    "yname = 'eq_width_H_A'\n",
    "\n",
    "# get the table tname\n",
    "t_c1e5 = getattr(c1e5,tname)\n",
    "t_c1e6 = getattr(c1e6,tname)\n",
    "t_s1e6 = getattr(s1e6,tname)\n",
    "\n",
    "ax1.plot(t_c1e5[xname],t_c1e5[yname],label='c1e5')\n",
    "ax2.plot(t_c1e6[xname],t_c1e6[yname],label='c1e6')\n",
    "ax3.plot(t_s1e6[xname],t_s1e6[yname],label='s1e6')\n",
    "\n",
    "for ax,l in zip((ax1,ax2,ax3),('c1e5','c1e6','s1e6')):\n",
    "    ax.set(xlabel=xname.replace('_',''))\n",
    "    ax.set_title(l)\n",
    "ax1.set(ylabel=yname.replace('_',''))\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling of Ha/FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "Ha1e5  = c1e5.ewidth['Luminosity_H_A']\n",
    "time_HA1e5 = c1e5.ewidth['Time']\n",
    "FUV1e5  = c1e5.FUV['FUV']\n",
    "time_FUV1e5 = c1e5.FUV['Time']\n",
    "FUV_int1e5 = np.interp(time_HA1e5,time_FUV1e5,FUV1e5)\n",
    "\n",
    "Ha1e6  = c1e6.ewidth['Luminosity_H_A']\n",
    "time_HA1e6 = c1e6.ewidth['Time']\n",
    "FUV1e6  = c1e6.FUV['FUV']\n",
    "time_FUV1e6 = c1e6.FUV['Time']\n",
    "FUV_int1e6 = np.interp(time_HA1e5,time_FUV1e6,FUV1e6)\n",
    "\n",
    "\n",
    "ax1.plot(time_HA1e5/1e6,Ha1e5/FUV_int1e5,color='tab:red',label='1e5Msun')\n",
    "ax1.plot(time_HA1e6/1e6,Ha1e6/FUV_int1e6,color='tab:green',label='1e6Msun')\n",
    "ax1.set(ylabel='Halpha/FUV',xlabel='Time/Myr',xlim=[0,10])\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling of eq width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "eqw1e5  = c1e5.ewidth['eq_width_H_A']\n",
    "time_1e5 = c1e5.ewidth['Time']\n",
    "\n",
    "eqw1e6  = c1e6.ewidth['eq_width_H_A']\n",
    "time_1e6 = c1e6.ewidth['Time']\n",
    "\n",
    "\n",
    "ax1.plot(time_1e5/1e6,eqw1e5,color='tab:red',label='1e5Msun')\n",
    "ax1.plot(time_1e6/1e6,eqw1e6,color='tab:green',label='1e6Msun')\n",
    "ax1.set(ylabel='eq width',xlabel='Time/Myr',xlim=[0,10])\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import create_new_starburst99_folder, stellar_models\n",
    "\n",
    "\n",
    "create_new_starburst99_folder(basedir/'..'/'starburst'/'data'/'PADOVASTD'/'33',\n",
    "                              overwrite=True,\n",
    "                              model=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HST composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v1p2'\n",
    "HSTband = 'nuv'\n",
    "scalepc = 64\n",
    "\n",
    "name = 'NGC1672'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations, ReadHST\n",
    "\n",
    "hst_images = ReadHST('NGC1672',data_ext / 'HST' / 'filterImages' )\n",
    "\n",
    "filename = data_ext / 'HST' / 'narrowband_Halpha' / f'{name.lower()}_ha_sub.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    hst_halpha = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "    \n",
    "associations, associations_mask = read_associations(folder=data_ext/'Products'/'stellar_associations',target=name.lower(),\n",
    "                                                    HSTband=HSTband,scalepc=scalepc,version=version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from pnlf.plot import create_RGB\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "def CutoutRGB(r,g,b,mask1,mask2,position,size=(4*u.arcsec,4*u.arcsec)):\n",
    "    \n",
    "    cutout_r = Cutout2D(r.data,position,size,wcs=r.wcs)\n",
    "    cutout_g, _  = reproject_interp(g,output_projection=cutout_r.wcs,shape_out=cutout_r.shape)    \n",
    "    cutout_b, _  = reproject_interp(b,output_projection=cutout_r.wcs,shape_out=cutout_r.shape)    \n",
    "    \n",
    "    rgb = create_RGB(cutout_r.data,cutout_g,cutout_b,\n",
    "                     percentile=[98,98,98],weights=[1,0.9,0.9])\n",
    "    fig=plt.figure(figsize=(8,8))\n",
    "    ax=fig.add_subplot(projection=cutout_r.wcs)\n",
    "    ax.imshow(rgb)\n",
    "    \n",
    "    \n",
    "    # for the nebulae catalogue\n",
    "    cutout_mask, _  = reproject_interp(mask1,output_projection=cutout_r.wcs,shape_out=cutout_r.shape,order='nearest-neighbor')    \n",
    "    region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "    contours = []\n",
    "    for i in region_ID:\n",
    "        blank_mask = np.zeros_like(cutout_mask)\n",
    "        blank_mask[cutout_mask==i] = 1\n",
    "        contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "    for coords in contours:\n",
    "        ax.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label='HII-region')\n",
    "    mask = np.zeros((*cutout_mask.shape,4))\n",
    "    mask[~np.isnan(cutout_mask.data),:] = (0.84, 0.15, 0.16,0.1)\n",
    "    ax.imshow(mask)\n",
    "    \n",
    "    # for the associations\n",
    "    cutout_mask, _  = reproject_interp(mask2,output_projection=cutout_r.wcs,shape_out=cutout_r.shape,order='nearest-neighbor')    \n",
    "    region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "    contours = []\n",
    "    for i in region_ID:\n",
    "        blank_mask = np.zeros_like(cutout_mask)\n",
    "        blank_mask[cutout_mask==i] = 1\n",
    "        contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "    for coords in contours:\n",
    "        ax.plot(coords[:,1],coords[:,0],color='white',lw=1,label='association')\n",
    "\n",
    "    mask = np.zeros((*cutout_mask.shape,4))\n",
    "    mask[~np.isnan(cutout_mask.data),:] = (0.12,0.47,0.71,0.1)\n",
    "    ax.imshow(mask,origin='lower')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "position = catalogue['SkyCoord_asc'][10]\n",
    "\n",
    "#f275w, f336w, f435w, f555w, f814w\n",
    "CutoutRGB(hst_halpha,hst_images.f555w,hst_images.f275w,nebulae_mask,associations_mask,position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many matches are by change?\n",
    "\n",
    "we rotate one of the masks by 90 degree and see how many still overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "# reproject nebulae mask to hst \n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=associations_mask.wcs,\n",
    "                                   shape_out=associations_mask.data.shape,\n",
    "                                   order='nearest-neighbor') \n",
    "in_frame = Table.read(basedir/'data'/'interim'/'Nebulae_Catalogue_v2p1_in_frame.fits')\n",
    "nebulae = join(nebulae,in_frame,keys=['gal_name','region_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 = 90 deg, 2 = 180 deg ...\n",
    "k = 1\n",
    "\n",
    "for k in [0,1]:\n",
    "    print(f'{90*k} deg rotation:')\n",
    "    \n",
    "    # we scale the associations such that the the id is in the decimal\n",
    "    scale = 10**np.ceil(np.log10(max(associations_mask.data[~np.isnan(associations_mask.data)])))\n",
    "    s_arr = associations_mask.data/scale+np.rot90(nebulae_hst,k=k)\n",
    "\n",
    "    # ids of associations, nebulae and combination (sum) of both\n",
    "    a_id = np.unique(associations_mask.data[~np.isnan(associations_mask.data)]).astype(int)\n",
    "    n_id = np.unique(nebulae_mask.data[~np.isnan(nebulae_mask.data)]).astype(int)\n",
    "    s_id = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "    # this splits the sum into two parts (nebulae and associations)\n",
    "    a_modf,n_modf = np.modf(s_id)\n",
    "    n_modf = n_modf.astype(int)\n",
    "    a_modf = np.round(a_modf*scale).astype(int)\n",
    "\n",
    "    unique_a, count_a = np.unique(a_modf,return_counts=True)\n",
    "    unique_n, count_n = np.unique(n_modf,return_counts=True)\n",
    "\n",
    "    nebulae_dict = {int(n) : a_modf[n_modf==n].tolist() for n in n_id}     \n",
    "    associations_dict = {int(a) : n_modf[a_modf==a].tolist() for a in a_id}     \n",
    "\n",
    "\n",
    "    # so far we ensured that the nebulae in unique_n have only one association,\n",
    "    # but it is possible that this association goes beyond the nebulae and into\n",
    "    # a second nebulae. Those objects are excluded here\n",
    "    isolated_nebulae = set()\n",
    "    isolated_assoc   = set()\n",
    "    for n,v in nebulae_dict.items():\n",
    "        if len(v)==1:\n",
    "            if len(associations_dict[v[0]])==1:\n",
    "                isolated_nebulae.add(n)\n",
    "                isolated_assoc.add(v[0])\n",
    "\n",
    "    # find all assoc that have at least one pixel outside of the nebulae masks\n",
    "    mask = associations_mask.data.copy()\n",
    "    mask[~np.isnan(np.rot90(nebulae_hst,k=k))] = np.nan\n",
    "    outside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "    # find all assoc that have at least one pixel inside of the nebulea masks\n",
    "    mask = associations_mask.data.copy()\n",
    "    mask[np.isnan(np.rot90(nebulae_hst,k=k))] = np.nan\n",
    "    inside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "    contained = np.setdiff1d(inside,outside)\n",
    "    partial   = np.intersect1d(inside,outside)\n",
    "    isolated  = np.setdiff1d(outside,inside)\n",
    "    Ntot = len(contained) + len(partial) + len(isolated)\n",
    "    \n",
    "    print(f'contained: {len(contained)} ({100*len(contained)/Ntot:.1f}%), partial: {len(partial)}, isolated: {len(isolated)}')\n",
    "    print(f'1to1 match = {len(isolated_nebulae)}')\n",
    "    \n",
    "    nebulae_tmp = nebulae[['region_ID','x','y','in_frame']].copy()\n",
    "    nebulae_tmp.add_index('region_ID')\n",
    "\n",
    "    nebulae_tmp['1to1'] = False\n",
    "    nebulae_tmp['1to1'][np.isin(nebulae_tmp['region_ID'],list(isolated_nebulae))] = True\n",
    "\n",
    "    overlap = join(\n",
    "        Table(np.unique(nebulae_hst[~np.isnan(nebulae_hst)],return_counts=True),names=['region_ID','size']),\n",
    "        Table(np.unique(np.rot90(nebulae_hst,k=k)[~np.isnan(np.rot90(nebulae_hst,k=k)) & ~np.isnan(associations_mask.data)],return_counts=True),names=['region_ID','overlap_size']),\n",
    "        keys=['region_ID'],join_type='outer')\n",
    "    overlap = overlap.filled(0)\n",
    "    overlap['overlap_neb'] = overlap['overlap_size']/overlap['size']\n",
    "    overlap['overlap_neb'].info.format = '%.2f'\n",
    "    nebulae_tmp = join(nebulae_tmp,overlap[['region_ID','overlap_neb']],keys='region_ID')\n",
    "    n_over = np.sum((nebulae_tmp[\"overlap_neb\"]>0) & (nebulae_tmp['in_frame']) ) \n",
    "    print(f'HII regions with overlap: {n_over} ({100*n_over/np.sum(nebulae_tmp[\"in_frame\"]):.2f}%)')\n",
    "    \n",
    "    print('\\n'+50*'-'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without rotation\n",
    "text = '''Nneb ovlp Nasc cont part 1to1\n",
    "777  295  393   45  215  154\n",
    "1006  299  487  280  137  182\n",
    "926  191  489  286   95  112\n",
    "1029  310  525  341  139  133\n",
    "1024  355  494  134  254  250\n",
    "2119  816 1578  707  579  373\n",
    "1039  365  641  232  244  223\n",
    "2696  973 1735  853  584  402\n",
    "1567  385  465  181  207  197\n",
    "743  163  264  185   54   96\n",
    "'''\n",
    "\n",
    "t = ascii.read(text)\n",
    "\n",
    "np.sum(t['cont']) / np.sum(t['Nasc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation\n",
    "text = '''Nneb ovlp Nasc cont part 1to1\n",
    "783  144  378    4  131   81\n",
    "1005  139  447   51  126   68\n",
    "908   66  493   25   61   46\n",
    "1029  193  521   98  173   82\n",
    "982   82  438    4   87   57\n",
    "2138  463 1540   86  507  203\n",
    "1046  208  658   39  213  129\n",
    "2691  625 1738  192  616  290\n",
    "1561   86  383    9   74   59\n",
    "722   37  123    6   35   29\n",
    "'''\n",
    "\n",
    "t = ascii.read(text)\n",
    "\n",
    "np.sum(t['cont']) / np.sum(t['Nasc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         Nneb ovlp Nasc cont part 1to1\n",
    "IC5332\n",
    "  0 deg:  777  295  393   45  215  154\n",
    " 90 deg:  783  144  378    4  131   81\n",
    "NGC1087\n",
    "  0 deg: 1006  299  487  280  137  182\n",
    " 90 deg: 1005  139  447   51  126   68\n",
    "NGC1365\n",
    "  0 deg:  926  191  489  286   95  112\n",
    " 90 deg:  908   66  493   25   61   46\n",
    "NGC1385\n",
    "  0 deg: 1029  310  525  341  139  133\n",
    " 90 deg: 1029  193  521   98  173   82\n",
    "NGC1433\n",
    "  0 deg: 1024  355  494  134  254  250\n",
    " 90 deg:  982   82  438    4   87   57\n",
    "NGC1566\n",
    "  0 deg: 2119  816 1578  707  579  373\n",
    " 90 deg: 2138  463 1540   86  507  203\n",
    "NGC2835\n",
    "  0 deg: 1039  365  641  232  244  223\n",
    " 90 deg: 1046  208  658   39  213  129\n",
    "NGC4303\n",
    "  0 deg: 2696  973 1735  853  584  402\n",
    " 90 deg: 2691  625 1738  192  616  290\n",
    "NGC4535\n",
    "  0 deg: 1567  385  465  181  207  197\n",
    " 90 deg: 1561   86  383    9   74   59\n",
    "NGC7496\n",
    "  0 deg:  743  163  264  185   54   96\n",
    " 90 deg:  722   37  123    6   35   29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JWST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import ReadHST\n",
    "\n",
    "hst_images = ReadHST('NGC7496',directory=data_ext/'HST'/'filterImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = data_ext / 'JWST' / 'NGC7496' / 'jw02107-c1018_t019_miri_f770w' / 'jw02107-c1018_t019_miri_f770w_i2d.fits'\n",
    "\n",
    "with fits.open(filename) as hdul:\n",
    "    f770w = NDData(hdul['SCI'].data,\n",
    "                  meta=hdul['SCI'].header,\n",
    "                  wcs=WCS(hdul['SCI'].header))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "f275w_cutout = Cutout2D(hst_images.f275w.data,(5000,5000),size=(5000,5000),wcs=hst_images.f275w.wcs)\n",
    "f770w_cutout = reproject_interp(f770w,output_projection=f275w_cutout.wcs,shape_out=f275w_cutout.shape,return_footprint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(figsize=(two_column,two_column/2),ncols=2)\n",
    "\n",
    "norm = simple_norm(f275w_cutout.data,clip=False,percent=99)\n",
    "ax1.imshow(f275w_cutout.data,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "ax1.set_title('HST f275w')\n",
    "\n",
    "norm = simple_norm(f770w_cutout.data,clip=False,percent=99)\n",
    "ax2.imshow(f770w_cutout.data,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "ax2.set_title('JWST f770w')\n",
    "\n",
    "for ax in [ax1,ax2]:\n",
    "    ax.axis('off')\n",
    "#ax.set(xlim=[300,1800],ylim=[0,1250])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "gal_name = 'NGC7496'\n",
    "\n",
    "# HST image for the background\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'hlsp_phangs-hst_hst_wfc3-uvis_{gal_name.lower()}_f275w_v1_exp-drc-sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "\n",
    "# nebulae mask\n",
    "filename = data_ext / 'Products' / 'Nebulae_catalogs' / 'Nebulae_catalogue_v2' /'spatial_masks'/f'{gal_name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "# association mask\n",
    "associations_mask16 = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                      target=gal_name.lower(),\n",
    "                                      scalepc=16,\n",
    "                                      data='mask')\n",
    "associations_mask32 = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                      target=gal_name.lower(),\n",
    "                                      scalepc=32,\n",
    "                                      data='mask')\n",
    "associations_mask64 = read_associations(folder=data_ext/'Products'/'stellar_associations',\n",
    "                                      target=gal_name.lower(),\n",
    "                                      scalepc=64,\n",
    "                                      data='mask')\n",
    "\n",
    "filename = data_ext/'Products'/'compact_clusters'/f'PHANGS_IR3_{gal_name.lower()}_phangs-hst_v1p1_ml_class12.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    compact_clusters = Table(hdul[1].data)\n",
    "compact_clusters['SkyCoord'] = SkyCoord(compact_clusters['PHANGS_RA']*u.deg,compact_clusters['PHANGS_DEC']*u.deg)\n",
    "compact_clusters.add_index('ID_PHANGS_CLUSTERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from skimage.measure import find_contours\n",
    "from reproject import reproject_interp\n",
    "from regions import PixCoord, RectangleSkyRegion\n",
    "\n",
    "sample = sub[:19]\n",
    "\n",
    "size=10*u.arcsec\n",
    "nrows=5\n",
    "ncols=4\n",
    "filename = basedir/'reports'/f'cutouts_JWST'\n",
    "    \n",
    "width = 8.27\n",
    "N = len(sample) \n",
    "Npage = nrows*ncols-1\n",
    "if N%Npage==0:\n",
    "    print('sample size % subplots = 0: no subplot for legend')\n",
    "Npages = int(np.ceil(N/Npage))\n",
    "gal_name = None\n",
    "\n",
    "with PdfPages(filename.with_suffix('.pdf')) as pdf:\n",
    "\n",
    "    for i in range(Npages):\n",
    "        print(f'working on page {i+1} of {Npages}')\n",
    "\n",
    "        sub_sample = sample[i*Npage:(i+1)*Npage]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=(width,width/ncols*nrows))\n",
    "        axes_iter = iter(axes.flatten())\n",
    "\n",
    "        for row in sub_sample:  \n",
    "                        \n",
    "            ax = next(axes_iter)\n",
    "             \n",
    "            position = row['SkyCoord_neb']\n",
    "            position = position.directional_offset_by(0*u.deg,0.3*u.arcsec)\n",
    "\n",
    "            cutout_image = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "            norm = simple_norm(cutout_image.data,clip=False,stretch='linear',percent=99.5)\n",
    "            ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "\n",
    "            # plot the nebulae catalogue\n",
    "            cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_image.wcs,shape_out=cutout_image.shape,order='nearest-neighbor')    \n",
    "            region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "            contours = []\n",
    "            for i in region_ID:\n",
    "                blank_mask = np.zeros_like(cutout_mask)\n",
    "                blank_mask[cutout_mask==i] = 1\n",
    "                contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "            for coords in contours:\n",
    "                ax.plot(coords[:,1],coords[:,0],color='tab:red',lw=1,label=r'H\\textsc{ii} region')\n",
    "\n",
    "\n",
    "            mask = np.zeros((*cutout_mask.shape,4))\n",
    "            mask[~np.isnan(cutout_mask.data),:] = (0.84, 0.15, 0.16,0.1)\n",
    "            ax.imshow(mask,origin='lower',cmap=plt.cm.gist_heat)\n",
    "\n",
    "            for mask2,scalepc,color in zip([associations_mask16,associations_mask32,associations_mask64],[16,32,64],['tab:green','tab:blue','tab:orange']):\n",
    "                cutout_mask, _  = reproject_interp(mask2,output_projection=cutout_image.wcs,shape_out=cutout_image.shape,order='nearest-neighbor')    \n",
    "                region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "                contours = []\n",
    "                for i in region_ID:\n",
    "                    blank_mask = np.zeros_like(cutout_mask)\n",
    "                    blank_mask[cutout_mask==i] = 1\n",
    "                    contours += find_contours(blank_mask, 0.5)\n",
    "\n",
    "                for coords in contours:\n",
    "                    ax.plot(coords[:,1],coords[:,0],color=color,lw=1,label=f'{scalepc}pc')\n",
    "\n",
    "                mask = np.zeros((*cutout_mask.shape,4))\n",
    "                mask[~np.isnan(cutout_mask.data),:] = (0.12,0.47,0.71,0.1)\n",
    "                ax.imshow(mask,origin='lower')\n",
    "\n",
    "  \n",
    "            region = RectangleSkyRegion(position,0.9*size,0.9*size)\n",
    "            in_frame = compact_clusters[region.contains(compact_clusters['SkyCoord'],cutout_image.wcs)]\n",
    "            for point in in_frame:\n",
    "                x,y = point['SkyCoord'].to_pixel(cutout_image.wcs)\n",
    "                if 5<x<cutout_image.data.shape[0]-5 and 5<y<cutout_image.data.shape[1]-5:\n",
    "                    ax.scatter(x,y,marker='o',facecolors='none',s=40,lw=0.8,color='green',label='DOLPHOT peaks')\n",
    "                    #if 'label' in points.columns:\n",
    "                    #    ax.annotate(row['label'], (x+4, y),color='green')\n",
    "\n",
    "            label = row['region_ID']\n",
    "            t = ax.text(0.0663,0.8666,label, transform=ax.transAxes,color='black',fontsize=7)\n",
    "            t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "        plt.subplots_adjust(wspace=-0.01, hspace=0.05)\n",
    "\n",
    "        # only the last page has subplots that need to be removed\n",
    "        h,l = fig.axes[0].get_legend_handles_labels()\n",
    "        ax = next(axes_iter)\n",
    "        ax.axis('off')\n",
    "        ax.legend(h[::len(h)-1],l[::(len(l)-1)],fontsize=7,loc='center',frameon=False)\n",
    "        t = ax.text(0.07,0.87,'name: region ID/assoc ID', transform=ax.transAxes,color='black',fontsize=8)\n",
    "\n",
    "        if i == int(np.ceil(N/Npage))-1:\n",
    "\n",
    "            for i in range(nrows*ncols-len(sub_sample)-1):\n",
    "                # remove the empty axes at the bottom\n",
    "                ax = next(axes_iter)\n",
    "                ax.axis('off')    \n",
    "\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = multiscale_nebulae[~multiscale_nebulae['overlap_any'] & (multiscale_nebulae['Ncluster']==0) & (multiscale_nebulae['gal_name']=='NGC7496') & (multiscale_nebulae['in_frame'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age','mass','HA/FUV','HA6562_FLUX','region_area']\n",
    "\n",
    "\n",
    "data = np.zeros((len(catalogue),len(columns)))\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    data[:,i] = catalogue[col].data\n",
    "data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nebulae = PCA(n_components=2)\n",
    "principalComponents = pca_nebulae.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyNeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O3 = pn.Atom('O',3, NLevels=5)\n",
    "O2 = pn.Atom('O',2, NLevels=5)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,7))\n",
    "O2.plotGrotrian(ax=ax1)\n",
    "O3.plotGrotrian(ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pn.Observation()\n",
    "\n",
    "for line,wave in zip(['OI6300','OII3726','OII7319','OII7330','OIII5006','NII5754',\n",
    "             'NII6583','SII6716','SII6730','SIII6312','SIII9068'],\n",
    "               [6300,3726,7319,7330,5007,5755,6584,6716,6731,6312,9069], \n",
    "               ):\n",
    "    Intens = subsample[f'{line}_FLUX_CORR']\n",
    "    Error  = subsample[f'{line}_FLUX_CORR_ERR']\n",
    "    line = pn.EmissionLine(line[0],len(line[1:-4]),wave,obsIntens=Intens,obsError=Error)\n",
    "    \n",
    "    obs.addLine(line)\n",
    "    \n",
    "diags = pn.Diagnostics()\n",
    "diags.addDiagsFromObs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "emisgrids = pn.getEmisGridDict(atomDict=diags.atomDict)\n",
    "diags.plot(emisgrids, obs, ax=ax,i_obs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te,ne = diags.getCrossTemDen(diag_tem='[NII] 5755/6584',diag_den='[SII] 6731/6716',obs=obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = [10000.]\n",
    "Ne = [1e3]\n",
    "# Define a dictionary to hold all the Atom objects needed\n",
    "all_atoms = pn.getAtomDict(atom_list=obs.getUniqueAtoms())\n",
    "# define a dictionary to store the abundances\n",
    "ab_dict = {}\n",
    "# we  use the following lines to determine the ionic abundances\n",
    "ab_labels = ['O3_5007A', 'H1r_4861A', 'O2_3726A', 'O2_7319A', 'O2_7330A',\n",
    "             'N2_5755A', 'N2_6584A', 'S2_6716A', 'S2_6731A', 'S3_6312A','S3_9069A']\n",
    "\n",
    "for line in obs.getSortedLines():\n",
    "    if line.label in ab_labels:\n",
    "        ab = all_atoms[line.atom].getIonAbundance(line.corrIntens, Te, Ne, \n",
    "                                                  to_eval=line.to_eval, Hbeta=100)\n",
    "        ab_dict[line.atom] = ab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "idx,sep,_=match_coordinates_sky(associations['SkyCoord'],nebulae['SkyCoord'])\n",
    "\n",
    "sep = sep.to(u.arcsec).value\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "bins = np.arange(0,10)\n",
    "\n",
    "print(f\"x<0.5: {np.mean(associations[sep<0.5]['age']):.2f}\")\n",
    "print(f\"0.5<x<1: {np.mean(associations[(sep>0.5) & (sep<1)]['age']):.2f}\")\n",
    "print(f\"1<x: {np.mean(associations[sep>1]['age']):.2f}\")\n",
    "\n",
    "ax.hist(associations[sep<0.5]['age'],bins=bins,alpha=0.5,label=r'$x<0.5\"$')\n",
    "ax.hist(associations[(sep>0.5) & (sep<1)]['age'],bins=bins,alpha=0.5,label=r'$0.5\"<x<1\"$')\n",
    "ax.hist(associations[sep>1]['age'],bins=bins,alpha=0.5,label=r'$1\"<x$')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure DIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "\n",
    "def measure_dig(data,mask,label,position,factor=1,max_iter=10,size=32,plot=True):\n",
    "    '''measure the diffuse ionized gas around an HII-region'''\n",
    "    \n",
    "    cutout_mask = Cutout2D(mask.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    cutout_data = Cutout2D(data.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    \n",
    "    area_mask  = np.sum(cutout_mask.data==label)\n",
    "    input_mask = cutout_mask.data==label\n",
    "    \n",
    "    n_iter = 0\n",
    "    while True:\n",
    "        n_iter+=1\n",
    "        boundaries = find_boundaries(input_mask,mode='outer')\n",
    "        input_mask |=boundaries\n",
    "        area_boundary = np.sum(input_mask & np.isnan(cutout_mask.data)) \n",
    "        if area_boundary > factor*area_mask or n_iter>max_iter: break\n",
    "            \n",
    "    if plot:\n",
    "        fig,ax=plt.subplots(figsize=(5,5))\n",
    "        ax.imshow(cutout_mask.data,origin='lower')\n",
    "        mask = np.zeros((*cutout_mask.shape,4))\n",
    "        mask[input_mask & np.isnan(cutout_mask.data),:] = (1,0,0,0.5)\n",
    "        ax.imshow(mask,origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "    #if np.sum(boundaries & np.isnan(cutout_mask.data))==0:\n",
    "    #    print(f'no boundaries for {label}')\n",
    "    dig = cutout_data.data[input_mask & np.isnan(cutout_mask.data)]\n",
    "\n",
    "    return np.median(dig),np.mean(dig),np.sum(dig)\n",
    "\n",
    "row = nebulae[4]\n",
    "measure_dig(Halpha,nebulae_mask,row['region_ID'],(row['x'],row['y']),factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['dig'] = np.nan\n",
    "for row in tmp:\n",
    "    row['dig'] = measure_dig(Halpha,nebulae_mask,row['region_ID'],(row['x'],row['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tmp['HA6562_FLUX'],tmp['dig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure density and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn\n",
    "diags = pn.Diagnostics()\n",
    "\n",
    "# [SII]6717/[SII]6731 this is a tracer for the density\n",
    "temp,density = diags.getCrossTemDen('[NII] 5755/6548', '[SII] 6731/6716', \n",
    "                     nebulae['NII5754_FLUX_CORR']/nebulae['NII6583_FLUX_CORR'],\n",
    "                     nebulae['SII6730_FLUX_CORR']/nebulae['SII6716_FLUX_CORR'])\n",
    "\n",
    "nebulae['density'] = density\n",
    "nebulae['temp'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.scatter(nebulae['density'],nebulae['SII6716_FLUX_CORR']/nebulae['SII6730_FLUX_CORR'])\n",
    "ax.set(xscale='log',xlim=[1,1e5],ylim=(0,1.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdu = fits.BinTableHDU(nebulae[['gal_name','region_ID','density','temperature']],name='density')\n",
    "hdu.writeto(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_density.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase matching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = nebulae['SkyCoord'][1094]\n",
    "size = 1.2*u.arcsec\n",
    "\n",
    "fig,(ax1) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "cutout_nebulae = Cutout2D(nebulae_mask.data,position,size=size,wcs=nebulae_mask.wcs)\n",
    "cutout_assoc = Cutout2D(associations_mask.data,position,size=size,wcs=associations_mask.wcs)\n",
    "reproject_nebula, _  = reproject_interp(nebulae_mask,output_projection=cutout_assoc.wcs,shape_out=cutout_assoc.shape,order='nearest-neighbor')    \n",
    "\n",
    "cutout_assoc.data[cutout_assoc.data==12] = 8\n",
    "cutout_nebulae.data[cutout_nebulae.data==1094] = 4\n",
    "reproject_nebula[reproject_nebula==1094] = 4\n",
    "\n",
    "ax1.imshow(cutout_nebulae.data,cmap=plt.cm.Reds,vmin=0,vmax=8)\n",
    "for (j,i),label in np.ndenumerate(cutout_nebulae.data):\n",
    "    ax1.text(i,j,f'{label:.0f}',ha='center',va='center')\n",
    "ax1.axis('off')\n",
    "#ax1.set_title('nebulae mask')\n",
    "#plt.savefig(basedir/'reports'/'mask_matching_nebulae.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax2) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax2.imshow(reproject_nebula,cmap=plt.cm.Reds,vmin=0,vmax=8)\n",
    "for (j,i),label in np.ndenumerate(reproject_nebula):\n",
    "    ax2.text(i,j,f'{label:.0f}',ha='center',va='center',fontsize=6)\n",
    "ax2.axis('off')\n",
    "#ax2.set_title('reprojected nebulae mask')\n",
    "plt.savefig(basedir/'reports'/'mask_matching_nebulae_reproj.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax3) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax3.imshow(cutout_assoc.data/10,cmap=plt.cm.Blues,vmin=0,vmax=1.6)\n",
    "for (j,i),label in np.ndenumerate(cutout_assoc.data):\n",
    "    ax3.text(i,j,f'{label/10:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax3.axis('off')\n",
    "#ax3.set_title('association mask')\n",
    "plt.savefig(basedir/'reports'/'mask_matching_assoc.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax4) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax4.imshow(cutout_assoc.data/10+reproject_nebula,cmap=plt.cm.Greens,vmin=0,vmax=9.6)\n",
    "for (j,i),label in np.ndenumerate(cutout_assoc.data/10+reproject_nebula):\n",
    "    ax4.text(i,j,f'{label:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax4.axis('off')\n",
    "#ax4.set_title('association+nebulae mask')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'mask_matching_sum.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.full ((5,5),np.nan)\n",
    "arr1[2:4,1:3] = 4\n",
    "\n",
    "arr2 = np.full ((10,10),np.nan)\n",
    "arr2[4:8,2:6] = 4\n",
    "\n",
    "arr3 = np.full ((10,10),np.nan)\n",
    "arr3[5:7,4:8] = 2\n",
    "arr3[2:5,0:3] = 9\n",
    "\n",
    "arr4 = arr2+arr3/10\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4,figsize=(12,4))\n",
    "\n",
    "extent = (0, arr1.shape[1], arr1.shape[0], 0)\n",
    "ax1.imshow(arr1,cmap=plt.cm.Reds,vmin=0,vmax=10,extent=extent)\n",
    "for (j,i),label in np.ndenumerate(arr1):\n",
    "    ax1.text(i+0.5,j+0.5,f'{label:.0f}',ha='center',va='center',fontsize=6)\n",
    "ax1.set_xticks(np.arange(0, arr1.shape[0], 1), minor=True)\n",
    "ax1.set_yticks(np.arange(0, arr1.shape[1], 1), minor=True)\n",
    "ax1.set_title('native nebula mask')\n",
    "\n",
    "extent = (0, arr2.shape[1], arr2.shape[0], 0)\n",
    "ax2.imshow(arr2,cmap=plt.cm.Reds,vmin=0,vmax=10,extent=extent)\n",
    "for (j,i),label in np.ndenumerate(arr2):\n",
    "    ax2.text(i+0.5,j+0.5,f'{label:.0f}',ha='center',va='center',fontsize=6)\n",
    "ax2.set_xticks(np.arange(0, arr2.shape[0], 1), minor=True)\n",
    "ax2.set_yticks(np.arange(0, arr2.shape[1], 1), minor=True)\n",
    "ax2.set_title('reprojected nebula mask')\n",
    "\n",
    "extent = (0, arr3.shape[1], arr3.shape[0], 0)\n",
    "ax3.imshow(arr3/10,cmap=plt.cm.Blues,vmin=-0.5,vmax=1.5,extent=extent)\n",
    "for (j,i),label in np.ndenumerate(arr3/10):\n",
    "    ax3.text(i+0.5,j+0.5,f'{label:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax3.set_xticks(np.arange(0, arr3.shape[0], 1), minor=True)\n",
    "ax3.set_yticks(np.arange(0, arr3.shape[1], 1), minor=True)\n",
    "ax3.set_title('association mask')\n",
    "\n",
    "extent = (0, arr4.shape[1], arr4.shape[0], 0)\n",
    "ax4.imshow(arr2,cmap=plt.cm.Reds,vmin=0,vmax=10,extent=extent)\n",
    "ax4.imshow(arr3/10,cmap=plt.cm.Blues,vmin=-0.5,vmax=1.5,extent=extent,alpha=1)\n",
    "ax4.imshow(arr4,cmap=plt.cm.Purples,vmin=0,vmax=9,extent=extent,alpha=1)\n",
    "for (j,i),label in np.ndenumerate(arr4):\n",
    "    ax4.text(i+0.5,j+0.5,f'{label:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax4.set_xticks(np.arange(0, arr4.shape[0], 1), minor=True)\n",
    "ax4.set_yticks(np.arange(0, arr4.shape[1], 1), minor=True)\n",
    "ax4.set_title('sum of both masks')\n",
    "\n",
    "for ax in (ax1,ax2,ax3,ax4):\n",
    "    ax.grid(which='both')\n",
    "    ax.tick_params(left=False,\n",
    "                bottom=False,\n",
    "                top=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'mask_matching.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deprojected radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centre       = SkyCoord(p['R.A.'],p['Dec.'])\n",
    "posang       = (p['posang'])*u.deg\n",
    "eccentricity = np.sin(p['Inclination']*u.deg).value\n",
    "r25   = (p['r25']/u.arcmin*300).value  # convert arcmin to pixel\n",
    "\n",
    "a     = r25\n",
    "b     = np.sqrt((r25)**2 * (1-eccentricity**2))\n",
    "\n",
    "tmp = nebulae['x','y','SkyCoord','deproj_dist','r_R25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deproject(coord,centre,a,b,theta):\n",
    "    '''\n",
    "    \n",
    "    https://math.stackexchange.com/questions/432902/how-to-get-the-radius-of-an-ellipse-at-a-specific-angle-by-knowing-its-semi-majo\n",
    "    '''\n",
    "    dist  = coord.separation(centre).to(u.arcsec)\n",
    "    theta = coord.position_angle(centre)-theta\n",
    "    \n",
    "    \n",
    "    r = a*b/(np.sqrt(a**2*np.sin(theta)**2+b**2*np.cos(theta)**2))\n",
    "\n",
    "    return dist/a*r\n",
    "    \n",
    "tmp['dist'] = deproject(tmp['SkyCoord'],centre,a,b,posang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import SkyEllipticalAperture\n",
    "from astropy.units import Quantity\n",
    "\n",
    "a     = 0.2*p['r25']*u.arcmin\n",
    "b     = np.sqrt((a)**2 * (1-eccentricity**2))\n",
    "theta = Quantity(p['posang'],unit=u.deg)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax = fig.add_subplot(projection=Halpha.wcs)\n",
    "\n",
    "norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "ax.imshow(Halpha.data,norm=norm,cmap=plt.cm.Reds)\n",
    "\n",
    "sky_aperture = SkyEllipticalAperture(centre,a,b,theta)\n",
    "pix_aperture = sky_aperture.to_pixel(Halpha.wcs)\n",
    "pix_aperture.plot(axes=ax)\n",
    "ax.scatter(*centre.to_pixel(Halpha.wcs),color='green')\n",
    "\n",
    "row = tmp[4]\n",
    "ax.scatter(row['x'],row['y'],color='blue')\n",
    "print(f\"posang = {row['SkyCoord'].position_angle(centre).to(u.degree):.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a     = p['r25']*u.arcmin\n",
    "b     = np.sqrt((a)**2 * (1-eccentricity**2))\n",
    "theta = Quantity(p['posang'],unit=u.deg)\n",
    "\n",
    "theta_diff = (tmp['SkyCoord'].position_angle(centre).to(u.degree)-180*u.deg)-theta\n",
    "r = a*b/(np.sqrt(a**2*np.sin(theta_diff)**2+b**2*np.cos(theta_diff)**2))\n",
    "\n",
    "\n",
    "(tmp['SkyCoord'].separation(centre).to(u.arcsec)*r/a).to(u.arcsec).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['dist'] = tmp['SkyCoord'].separation(centre).to(u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old and young populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "young = np.isin(associations_mask.data,associations[associations['age']<10]['assoc_ID'])\n",
    "old   = np.isin(associations_mask.data,associations[associations['age']>10]['assoc_ID'])\n",
    "\n",
    "young = young.astype(float)\n",
    "young[young==0] = np.nan\n",
    "\n",
    "old = old.astype(float)\n",
    "old[old==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(projection=F275.wcs)\n",
    "\n",
    "norm = simple_norm(F275.data,clip=False,percent=99)\n",
    "ax.imshow(F275.data,norm=norm,origin='lower',cmap=plt.cm.Greys,alpha=0.5)\n",
    "ax.imshow(old,vmin=0,vmax=1,cmap=plt.cm.Reds,alpha=0.8)\n",
    "ax.imshow(young,vmin=0,vmax=1,cmap=plt.cm.Blues,alpha=0.8)\n",
    "\n",
    "for row in associations:\n",
    "    ax.text(row['X'],row['Y'],row['assoc_ID'])\n",
    "\n",
    "ax.set(xlim=[2000,5000],ylim=[3000,5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure CO for associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import SkyCircularAperture, aperture_photometry\n",
    "\n",
    "\n",
    "apertures = SkyCircularAperture(associations['SkyCoord'],r=4*u.arcsec)\n",
    "co_flux = aperture_photometry(CO,apertures)\n",
    "associations['CO'] = co_flux['aperture_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.scatter(associations['age'],associations['CO'])\n",
    "ax.set(xlim=[0,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(1.2*single_column,single_column))\n",
    "\n",
    "tmp = associations\n",
    "sc = ax.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['CO'],vmin=0,vmax=200,\n",
    "           s=0.5)\n",
    "ax.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "fig.colorbar(sc,label='CO')\n",
    "ax.invert_yaxis()\n",
    "#plt.savefig('UBVI_matched_associations.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "gmc_directory  = data_ext/'Products'/'GMC'/'matched'\n",
    "gmc_resolution = '150pc'\n",
    "filename = [x for x in (gmc_directory/gmc_resolution).iterdir() if name.lower() in x.stem][0]\n",
    "with fits.open(filename) as hdul:\n",
    "    GMC= Table(hdul[1].data)\n",
    "GMC['SkyCoord'] = SkyCoord(GMC['XCTR_DEG']*u.deg,GMC['YCTR_DEG']*u.deg)\n",
    "idx,sep,_=match_coordinates_sky(associations['SkyCoord'],GMC['SkyCoord'])\n",
    "associations['GMC_sep'] = sep.to(u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([(-0.35,-1.67),(-0.18,-1.12),(0.3,-1.16),(0.75,-1.3),(0.6,-1),(0.55,-1),(0.55,-0.86),\n",
    "          (0.47,-0.86),(0.57,0.08),(1.1,0.1)])\n",
    "x,y=points.T\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(1.2*single_column,single_column))\n",
    "\n",
    "tmp = associations\n",
    "sc = ax.scatter(tmp['V_dolmag_vega']-tmp['I_dolmag_vega'],tmp['U_dolmag_vega']-tmp['B_dolmag_vega'],\n",
    "           c=tmp['GMC_sep'],vmin=0,vmax=4,\n",
    "           s=0.5)\n",
    "ax.plot(x,y,color='red')\n",
    "\n",
    "ax.set(xlabel=r'$V-I$',ylabel=r'$U-B$',xlim=[-1.5,2],ylim=[-2.5,1])\n",
    "fig.colorbar(sc,label='GMC sep / arcsec')\n",
    "ax.invert_yaxis()\n",
    "plt.savefig('UBVI_matched_associations.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.scatter(associations['GMC_sep'],associations['U_dolmag_vega']-associations['B_dolmag_vega'],s=0.5)\n",
    "ax.set(xlim=[0,20],ylim=[-2,0])\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMC_dic = data_ext/'Products'/'GMC'/'matched'\n",
    "\n",
    "for resolution in ['60pc','90pc','120pc','150pc']:\n",
    "    files = [x.stem.split('_')[0].upper() for x in (GMC_dic/resolution).iterdir()]\n",
    "    files = set(files) & set(sample_table['name'])\n",
    "    print(f'{resolution}: {len(files)} galaxes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'interim'/f'phangshst_associations_{HSTband}_ws{scalepc}pc_{version}.fits') as hdul:\n",
    "    associations = Table(hdul[1].data)\n",
    "    \n",
    "associations['SkyCoord'] = SkyCoord(associations['reg_ra']*u.degree,associations['reg_dec']*u.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gmc,y_gmc = GMC['SkyCoord'].to_pixel(Halpha.wcs)\n",
    "x_asc,y_asc = associations['SkyCoord'].to_pixel(Halpha.wcs)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(projection=Halpha.wcs)\n",
    "\n",
    "norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "ax.imshow(Halpha.data,norm=norm,cmap=plt.cm.Greys)\n",
    "\n",
    "ax.scatter(x_gmc,y_gmc,color='tab:red',s=0.1,lw=0.05)\n",
    "\n",
    "mask = associations['GMC_sep']<2\n",
    "ax.scatter(x_asc[mask],y_asc[mask],color='tab:blue',s=0.1,lw=0.05)\n",
    "ax.scatter(x_asc[~mask],y_asc[~mask],color='tab:green',s=0.1,lw=0.05)\n",
    "\n",
    "plt.savefig('gmc_asc.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "450.85px",
    "left": "257px",
    "top": "110.083px",
    "width": "298.367px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
