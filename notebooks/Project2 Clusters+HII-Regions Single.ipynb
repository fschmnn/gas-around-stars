{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and HII-regions Single <a class=\"tocSkip\">\n",
    "\n",
    "the aim of this notebook is to combine the HII-region and cluster catalogues. \n",
    "    \n",
    "In this notebook we do the matching on a per galaxie basis. For each resolution, a set of output files is produced that matches the nebulae to the association catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules after they have been modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pnlf.packages import *\n",
    "\n",
    "from pnlf.constants import tab10, single_column, two_column\n",
    "from cluster.plot import quick_plot, add_scale\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout,datefmt='%H:%M:%S',level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "basedir = Path('..')  # where we save stuff (and )\n",
    "data_ext = Path('a:') # raw data\n",
    "\n",
    "# we use the sample table for basic galaxy properties\n",
    "sample_table = ascii.read(basedir/'..'/'pnlf'/'data'/'interim'/'sample.txt')\n",
    "sample_table.add_index('name')\n",
    "sample_table['SkyCoord'] = SkyCoord(sample_table['R.A.'],sample_table['Dec.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "the galaxies listed in `hst_sample` have a cluster catalogue. The galaxies listed in `muse_sample` have astrosat observations to measure the FUV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_sample      = set(['NGC0628','NGC1087','NGC1365','NGC1385', 'NGC1433','NGC1512','NGC1566','NGC1672','NGC2835','NGC3351', 'NGC3627','NGC4254','NGC4321','NGC4535'])\n",
    "astrosat_sample = set([x.stem.split('_')[0] for x in (data_ext/'Astrosat').iterdir() if x.is_file() and x.suffix=='.fits'])\n",
    "muse_sample     = set(sample_table['name'])\n",
    "complete_sample = hst_sample & astrosat_sample & muse_sample\n",
    "\n",
    "name = 'NGC1365'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSE (DAP + nebulae catalogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pnlf.auxiliary import filter_table\n",
    "from pnlf.io import ReadLineMaps\n",
    "\n",
    "p = {x:sample_table.loc[name][x] for x in sample_table.columns}\n",
    "\n",
    "# DAP linemaps (Halpha and OIII)\n",
    "filename = data_ext / 'MUSE_DR2.1' / 'MUSEDAP' / f'{name}_MAPS.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    Halpha = NDData(data=hdul['HA6562_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['HA6562_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['HA6562_FLUX'].data),\n",
    "                    meta=hdul['HA6562_FLUX'].header,\n",
    "                    wcs=WCS(hdul['HA6562_FLUX'].header))\n",
    "    OIII = NDData(data=hdul['OIII5006_FLUX'].data,\n",
    "                    uncertainty=StdDevUncertainty(hdul['OIII5006_FLUX_ERR'].data),\n",
    "                    mask=np.isnan(hdul['OIII5006_FLUX'].data),\n",
    "                    meta=hdul['OIII5006_FLUX'].header,\n",
    "                    wcs=WCS(hdul['OIII5006_FLUX'].header)) \n",
    "\n",
    "path = data_ext / 'MUSE_DR2.1' / 'filterImages' \n",
    "sdss_g, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_g_WCS_Pall_mad.fits',header=True)\n",
    "sdss_r, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_r_WCS_Pall_mad.fits',header=True)\n",
    "sdss_i, h = fits.getdata(path / f'{name}_IMAGE_FOV_SDSS_i_WCS_Pall_mad.fits',header=True)\n",
    "    \n",
    "# the original catalogue from Francesco\n",
    "with fits.open(basedir / 'data' / 'interim' / 'Nebulae_Catalogue_v2p1.fits') as hdul:\n",
    "    nebulae = Table(hdul[1].data)\n",
    "nebulae['SkyCoord'] = SkyCoord(nebulae['cen_ra']*u.deg,nebulae['cen_dec']*u.deg,frame='icrs')\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_dig.fits') as hdul:\n",
    "    dig = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_fuv.fits') as hdul:\n",
    "    fuv = Table(hdul[1].data)\n",
    "    \n",
    "with fits.open(basedir/'data'/'interim'/f'Nebulae_Catalogue_v2p1_eq.fits') as hdul:\n",
    "    eq_width = Table(hdul[1].data)\n",
    "    \n",
    "nebulae = join(nebulae,fuv,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,eq_width,keys=['gal_name','region_ID'])\n",
    "nebulae = join(nebulae,dig,keys=['gal_name','region_ID'])\n",
    "nebulae.rename_columns(['cen_x','cen_y'],['x','y'])\n",
    "\n",
    "\n",
    "with np.errstate(divide='ignore',invalid='ignore'):\n",
    "    nebulae['[SIII]/[SII]'] = np.nan\n",
    "    SII = nebulae['SII6716_FLUX_CORR']+nebulae['SII6730_FLUX_CORR']\n",
    "    SIII = nebulae['SIII6312_FLUX_CORR']+nebulae['SIII9068_FLUX_CORR']\n",
    "    nebulae[SII>0]['[SIII]/[SII]'] = SIII[SII>0]/SII[SII>0]\n",
    "    nebulae['HA/FUV'] = nebulae['HA6562_FLUX_CORR']/nebulae['FUV_FLUX_CORR']\n",
    "    nebulae['HA/FUV_err'] = nebulae['HA/FUV']*np.sqrt((nebulae['HA6562_FLUX_CORR_ERR']/nebulae['HA6562_FLUX_CORR'])**2+(nebulae['FUV_FLUX_CORR_ERR']/nebulae['FUV_FLUX_CORR'])**2)\n",
    "\n",
    "\n",
    "nebulae['HIIregion'] = (nebulae['BPT_NII']==0) & (nebulae['BPT_SII']==0) & (nebulae['BPT_OI']==0)\n",
    "HII_regions = filter_table(nebulae,gal_name=name,BPT_NII=0,BPT_SII=0,BPT_OI=0)\n",
    "nebulae = filter_table(nebulae,gal_name=name)\n",
    "nebulae.add_index('region_ID')\n",
    "HII_regions.add_index('region_ID')\n",
    "\n",
    "filename = data_ext / 'MUSE_DR2.1' / 'Nebulae catalogue' /'spatial_masks'/f'{name}_nebulae_mask.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    nebulae_mask = NDData(hdul[0].data.astype(float),mask=Halpha.mask,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    nebulae_mask.data[nebulae_mask.data==-1] = np.nan\n",
    "\n",
    "# WFI image (larger FOV)\n",
    "filename = data_ext / 'WFI' / f'{name}_Rc_flux_nosky.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    WFI = NDData(data=hdul[0].data,\n",
    "                 meta=hdul[0].header,\n",
    "                 wcs=WCS(hdul[0].header))\n",
    "    \n",
    "# most of the time we do not need the datacubes\n",
    "if False:\n",
    "    #from spectral_cube import SpectralCube\n",
    "    filename = Path('g:') /'Archive'/'MUSE'/'DR2.1'/'datacubes'/f'{name}_DATACUBE_FINAL_WCS_Pall_mad.fits'\n",
    "    with fits.open(filename , memmap=True, mode='denywrite') as hdul:\n",
    "        data_cube   = hdul[1].data\n",
    "        cube_header = hdul[1].header   \n",
    "    \n",
    "print(f'{name}: {len(HII_regions)} HII-regions in final catalogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HST\n",
    "\n",
    "**white light + filter images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "target  = name.lower()\n",
    "scalepc = 64\n",
    "\n",
    "# whitelight image (we set 0s to nan)\n",
    "with fits.open(data_ext / 'HST' / 'white_light' / f'{name.lower()}_white_24rgb.fits') as hdul:\n",
    "    hst_whitelight = NDData(hdul[0].data,mask=hdul[0].data==0,meta=hdul[0].header,wcs=WCS(hdul[0].header))\n",
    "    hst_whitelight.data[hst_whitelight.data==0] = np.nan\n",
    "    \n",
    "# filter image with uncertainties\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'{name.lower()}_uvis_f275w_exp_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275 = NDData(hdul[0].data,\n",
    "                  mask=hdul[0].data==0,\n",
    "                  meta=hdul[0].header,\n",
    "                  wcs=WCS(hdul[0].header))\n",
    "filename = data_ext / 'HST' / 'filterImages' / f'{name.lower()}_uvis_f275w_err_drc_sci.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    F275.uncertainty = StdDevUncertainty(hdul[0].data)\n",
    "    \n",
    "associations, associations_mask = read_associations(folder=data_ext/'HST',target=target,scalepc=scalepc)\n",
    "\n",
    "\n",
    "print(f'{name}: {len(associations)} associations in catalogue')    \n",
    "# associations mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(data_ext/'ALMAv4p0'/f'{name.lower()}_12m+7m+tp_co21_broad_tpeak.fits') as hdul:\n",
    "    CO = NDData(data=hdul[0].data,\n",
    "                meta=hdul[0].header,\n",
    "                wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviormental Masks\n",
    "\n",
    "```\n",
    "1 = center (small bulge, nuclear ring, nuclear disc, etc.)\n",
    "2 = bar (excluding bar ends)\n",
    "3 = bar ends (overlap of bar and spiral)\n",
    "4 = interbar (R_gal < R_bar, but outside bar footprint)\n",
    "5 = spiral arms inside interbar (R_gal < R_bar)\n",
    "6 = spiral arms (R_gal > R_bar)\n",
    "7 = interarm (only the R_gal spanned by spiral arms, and R_gal > R_bar)\n",
    "8 = outer disc (R_gal > spiral arm ends, only for galaxies with identified spirals)\n",
    "9 = interbar (“disc” where R_gal < R_bar) where no strong spiral arms were identified\n",
    "10 = disc (R_gal > R_bar) where no spiral arms were identified (e.g. flocculent spirals)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "with fits.open(data_ext/'environment_masks'/f'{name}_simple.fits') as hdul:\n",
    "    mask = reproject_interp(hdul[0],Halpha.meta,order='nearest-neighbor',return_footprint=False)\n",
    "    env_masks_neb = NDData(data=mask,\n",
    "                           meta=hdul[0].header,\n",
    "                           wcs=Halpha.wcs)\n",
    "    \n",
    "    mask = reproject_interp(hdul[0],associations_mask.meta,order='nearest-neighbor',return_footprint=False)\n",
    "    env_masks_asc = NDData(data=mask,\n",
    "                           meta=hdul[0].header,\n",
    "                           wcs=associations_mask.wcs)\n",
    "    \n",
    "    \n",
    "environment_dict = {1 : 'center', 2 : 'bar', 3 : 'bar ends', 4 : 'interbar', \n",
    "                          5 : 'spiral arms',6 : 'spiral arms',7 : 'interarm' ,\n",
    "                          8 : 'outer disc', 9 : 'interbar', 10 : 'disc' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "from functools import reduce\n",
    "\n",
    "def region_from_mask(mask):\n",
    "    \n",
    "    # otherwiese we have problems with the edge of the image\n",
    "    mask[:,0] = False\n",
    "    mask[:,-1] = False\n",
    "    mask[0,:] = False\n",
    "    mask[-1,:] = False\n",
    "    \n",
    "    contours = find_contours(mask.astype(float),level=0.5)\n",
    "    \n",
    "    regs = []\n",
    "    for contour in contours:\n",
    "        regs.append(PolygonPixelRegion(PixCoord(*contour.T[::-1])))\n",
    "     \n",
    "    return regs\n",
    "    #return reduce(lambda x,y:x&y,regs)\n",
    "\n",
    "environment_regions = {}\n",
    "for i in np.unique(environment_masks.data):\n",
    "    reg = region_from_mask(np.isin(env_masks_neb.data,i))\n",
    "    environment_regions[environment_dict[i]] = reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = nebulae[nebulae_tmp['env_neb']=='spiral arms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(8,8))\n",
    "\n",
    "norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "ax.imshow(Halpha.data,norm=norm,cmap=plt.cm.Greys,origin='lower')\n",
    "\n",
    "mask = (env_masks_neb.data==6).astype(float)\n",
    "mask[mask==0.] = np.nan\n",
    "ax.imshow(mask,alpha=0.5,origin='lower')\n",
    "\n",
    "for reg in environment_regions['spiral arms']:\n",
    "    patch = reg.as_artist()\n",
    "    ax.add_patch(patch)\n",
    "    \n",
    "plt.scatter(sub['x'],sub['y'])    \n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiral_arms_pix = reduce(lambda x,y:x|y,regions)\n",
    "spiral_arms_sky = spiral_arms_pix.to_sky(environment_masks.wcs)\n",
    "\n",
    "nebulae['spiral_arms'] = spiral_arms_sky.contains(nebulae['SkyCoord'],wcs=Halpha.wcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Astrosat\n",
    "\n",
    "https://uvit.iiap.res.in/Instrument/Filters\n",
    "\n",
    "the resolution is 0.4\" per pixel. With a PSF resolution of 1.8\" this leads to fwhm ~ 4.5 px. This corresponds to a std = 1.91 px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# whitelight image\n",
    "astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F148W_flux_reproj.fits'\n",
    "\n",
    "if not astro_file.is_file():\n",
    "    astro_file = data_ext / 'Astrosat' / f'{name}_FUV_F154W_flux_reproj.fits'\n",
    "    if not astro_file.is_file():\n",
    "        print(f'no astrosat file for {name}')\n",
    "    \n",
    "with fits.open(astro_file) as hdul:\n",
    "    astrosat = NDData(hdul[0].data,meta=hdul[0].header,wcs=WCS(hdul[0].header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Equivalent Width\n",
    "\n",
    "the first step is to extract the spectra of each HII-region.\n",
    "\n",
    "Are the spectra continuum subtracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "\n",
    "filename = data_ext / 'MUSE_DR2.1' / 'Nebulae catalogue' /'spectra'/f'{name}_VorSpectra.fits'\n",
    "with fits.open(filename) as hdul:\n",
    "    spectra = Table(hdul[1].data)\n",
    "    spectral_axis = np.exp(Table(hdul[2].data)['LOGLAM'])*u.Angstrom\n",
    "    \n",
    "spectra['region_ID'] = np.arange(len(spectra))\n",
    "spectra.add_index('region_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H0 = 67 * u.km / u.s / u.Mpc\n",
    "z = (H0*Distance(distmod=p['(m-M)'])/c.c).decompose()\n",
    "lam_HA0 = 6562.8*u.Angstrom\n",
    "lam_HA = (1+z)*lam_HA0\n",
    "\n",
    "lam_HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.spectrum import fit_emission_line\n",
    "\n",
    "region_ID = 10\n",
    "#lam_HA = 6595*u.Angstrom\n",
    "filename = basedir/'reports'/name/f'{name}_eqwidth.png'\n",
    "flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "fit = fit_emission_line(spectral_axis,flux,lam_HA,filename=filename)\n",
    "integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "\n",
    "print(f\"f_cat/f_fit = {nebulae.loc[region_ID]['HA6562_FLUX']/integrated_flux.value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#lam_HA = 6595*u.Angstrom\n",
    "\n",
    "HA = []\n",
    "HII_regions['eq_width'] = np.nan\n",
    "for region_ID in tqdm(HII_regions['region_ID']):\n",
    "    \n",
    "    flux = spectra.loc[region_ID]['SPEC']*u.erg/u.s/u.cm**2/u.A\n",
    "    fit = fit_emission_line(spectral_axis,flux,lam_HA,plot=False)\n",
    "    integrated_flux = fit.amplitude_0*np.sqrt(np.pi)*np.exp(-1/(2*fit.stddev_0**2)) * u.erg/u.s/u.cm**2\n",
    "    continuum = fit.c0_1 * u.erg/u.s/u.cm**2/u.Angstrom\n",
    "    eq_width = integrated_flux/continuum\n",
    "    eq_width = HII_regions.loc[region_ID]['HA6562_FLUX']/continuum\n",
    "    HII_regions.loc[region_ID]['eq_width'] = eq_width.value\n",
    "    \n",
    "    HA.append(integrated_flux)\n",
    "    #HA_cat = nebulae.loc[region_ID]['HA6562_FLUX']\n",
    "    #print(f'{integrated_flux/HA_cat:.2f}')\n",
    "    #print(f'HA = {fit.mean_0.value:.2f}')\n",
    "HA = np.array([x.value for x in HA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.scatter(HA,HII_regions['HA6562_FLUX']/2.5)\n",
    "x = np.linspace(0,2e6)\n",
    "ax.plot(x,x,color='black')\n",
    "ax.set(ylim=[0,2e6],xlim=[0,2e6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HST and MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### compare footprints of different observations\n",
    "\n",
    "here we compare the footprints of the observations and check which objects overlap. The FOV of astrosat is a circle with a diameter of 28'. This is much larger than HST and MUSE and both will always be covered by the astrosat observations\n",
    "\n",
    "NGC0628\n",
    "987 (of 1077) associations in MUSE FOV\n",
    "2563 (of 2869) nebulae in HST FOV\n",
    "\n",
    "NGC1365\n",
    "499 (of 518) associations in MUSE FOV\n",
    "907 (of 1455) nebulae in HST FOV\n",
    "\n",
    "NGC1433\n",
    "495 (of 496) associations in MUSE FOV\n",
    "1011 (of 1736) nebulae in HST FOV\n",
    "\n",
    "NGC1566\n",
    "1584 (of 1653) associations in MUSE FOV\n",
    "2112 (of 2404) nebulae in HST FOV\n",
    "\n",
    "NGC3351\n",
    "710 (of 836) associations in MUSE FOV\n",
    "1179 (of 1284) nebulae in HST FOV\n",
    "\n",
    "\n",
    "NGC3627\n",
    "1330 (of 1415) associations in MUSE FOV\n",
    "1635 (of 1635) nebulae in HST FOV\n",
    "\n",
    "NGC4535\n",
    "475 (of 647) associations in MUSE FOV\n",
    "1555 (of 1938) nebulae in HST FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from cluster.regions import find_sky_region\n",
    "\n",
    "reg_muse_pix, reg_muse_sky = find_sky_region(nebulae_mask.mask.astype(int),wcs=nebulae_mask.wcs)\n",
    "reg_hst_pix, reg_hst_sky = find_sky_region(hst_whitelight.mask.astype(int),wcs=hst_whitelight.wcs)\n",
    "\n",
    "# check which nebulae/clusters are within the HST/MUSE FOV\n",
    "associations['in_frame'] = reg_muse_sky.contains(associations['SkyCoord'],nebulae_mask.wcs)\n",
    "#clusters['in_frame'] = reg_muse_sky.contains(clusters['SkyCoord'],nebulae_mask.wcs)\n",
    "nebulae['in_frame']  = reg_hst_sky.contains(nebulae['SkyCoord'],nebulae_mask.wcs)\n",
    "\n",
    "print(f'{np.sum(associations[\"in_frame\"])} (of {len(associations)}) associations in MUSE FOV')\n",
    "#print(f'{np.sum(clusters[\"in_frame\"])} (of {len(clusters)}) clusters in MUSE FOV')\n",
    "print(f'{np.sum(nebulae[\"in_frame\"])} (of {len(nebulae)}) nebulae in HST FOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=6*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "ax = quick_plot(WFI_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/'footpring.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association and nebulae\n",
    "\n",
    "the association catalogue differs from the clusters in that its entries are extended. Because we match two catalogues with extended objects, we must proceed differently.\n",
    "\n",
    "In a first step we take a look at a single association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "from regions import PixCoord, PolygonPixelRegion\n",
    "\n",
    "assoc_ID = 10\n",
    "pos = associations['SkyCoord'][associations['assoc_ID']==assoc_ID]\n",
    "\n",
    "contours = find_contours(associations_mask.data==assoc_ID,0.5,)\n",
    "coords = max(contours,key=len)\n",
    "\n",
    "# the coordinates from find_counters are switched compared to astropy\n",
    "reg_pix  = PolygonPixelRegion(vertices = PixCoord(*coords.T[::-1])) \n",
    "reg_sky  = reg_pix.to_sky(associations_mask.wcs)\n",
    "\n",
    "mask_cutout = Cutout2D(associations_mask.data,pos,size=1*u.arcsecond,wcs=associations_mask.wcs)\n",
    "F275_cutout = Cutout2D(F275.data,pos,size=2*u.arcsecond,wcs=F275.wcs)\n",
    "\n",
    "reg_pix_cut  = reg_sky.to_pixel(mask_cutout.wcs)\n",
    "\n",
    "ax = quick_plot(F275_cutout,figsize=(single_column,single_column),cmap=plt.cm.gray)\n",
    "ax.imshow(mask_cutout.data,alpha=0.5)\n",
    "\n",
    "reg_pix_cut.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to match the nebulae to the associations we first reproject the mask of the nebulae to the HST image. We then scale the association mask by the number of associations (assume we have 1432 objects, then 615 becomes 0.0615). This way we can add the two masks together and infer from the resulting unique values which clusters overlap with which associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match catalogues\n",
    "\n",
    "and make some plots that showcase the position/overlap\n",
    "\n",
    "```\n",
    "# NGC3627 is too large to reproject\n",
    "#center = SkyCoord(sample_table.loc[name]['R.A.'],sample_table.loc[name]['Dec.'])\n",
    "#cutout = Cutout2D(associations_mask.data,center,size=(5.5*u.arcmin,3*u.arcmin),wcs=associations_mask.wcs)\n",
    "cutout = Cutout2D(associations_mask.data,(3000,4800),size=(9000,5500),wcs=associations_mask.wcs)\n",
    "\n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=cutout.wcs,\n",
    "                                   shape_out=cutout.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "scale = 10**np.ceil(np.log10(max(cutout.data[~np.isnan(cutout.data)])))\n",
    "s_arr = cutout.data/scale+nebulae_hst\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "# reproject nebulae mask to hst \n",
    "nebulae_hst, _  = reproject_interp(nebulae_mask,\n",
    "                                   output_projection=associations_mask.wcs,\n",
    "                                   shape_out=associations_mask.data.shape,\n",
    "                                   order='nearest-neighbor')    \n",
    "\n",
    "# we scale the associations such that the the id is in the decimal\n",
    "scale = 10**np.ceil(np.log10(max(associations_mask.data[~np.isnan(associations_mask.data)])))\n",
    "s_arr = associations_mask.data/scale+nebulae_hst\n",
    "\n",
    "header = associations_mask.wcs.to_header()\n",
    "header['scale'] = scale\n",
    "#hdu = fits.PrimaryHDU(s_arr,header=header)\n",
    "#hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_map.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids of associations, nebulae and combination (sum) of both\n",
    "a_id = np.unique(associations_mask.data[~np.isnan(associations_mask.data)]).astype(int)\n",
    "n_id = np.unique(nebulae_mask.data[~np.isnan(nebulae_mask.data)]).astype(int)\n",
    "s_id = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "# this splits the sum into two parts (nebulae and associations)\n",
    "a_modf,n_modf = np.modf(s_id)\n",
    "n_modf = n_modf.astype(int)\n",
    "a_modf = np.round(a_modf*scale).astype(int)\n",
    "\n",
    "unique_a, count_a = np.unique(a_modf,return_counts=True)\n",
    "unique_n, count_n = np.unique(n_modf,return_counts=True)\n",
    "\n",
    "nebulae_dict = {int(n) : a_modf[n_modf==n].tolist() for n in n_id}     \n",
    "associations_dict = {int(a) : n_modf[a_modf==a].tolist() for a in a_id}     \n",
    "\n",
    "\n",
    "# so far we ensured that the nebulae in unique_n have only one association,\n",
    "# but it is possible that this association goes beyond the nebulae and into\n",
    "# a second nebulae. Those objects are excluded here\n",
    "isolated_nebulae = set()\n",
    "isolated_assoc   = set()\n",
    "for n,v in nebulae_dict.items():\n",
    "    if len(v)==1:\n",
    "        if len(associations_dict[v[0]])==1:\n",
    "            isolated_nebulae.add(n)\n",
    "            isolated_assoc.add(v[0])\n",
    "            \n",
    "print(f'n_associations = {len(associations_dict)}')\n",
    "print(f'n_nebulae      = {len(nebulae_dict)}')\n",
    "print(f'1to1 match     = {len(isolated_nebulae)}')\n",
    "\n",
    "\n",
    "# we save those two dicts so we do not have to redo this everytime\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_nebulae.yml','w+') as f:\n",
    "    yaml.dump(nebulae_dict,f)\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_associations.yml','w+') as f:\n",
    "    yaml.dump(associations_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all assoc that have at least one pixel outside of the nebulae masks\n",
    "mask = associations_mask.data.copy()\n",
    "mask[~np.isnan(nebulae_hst)] = np.nan\n",
    "outside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "# find all assoc that have at least one pixel inside of the nebulea masks\n",
    "mask = associations_mask.data.copy()\n",
    "mask[np.isnan(nebulae_hst)] = np.nan\n",
    "inside = np.unique(mask[~np.isnan(mask)].astype(int))\n",
    "\n",
    "contained = np.setdiff1d(inside,outside)\n",
    "partial   = np.intersect1d(inside,outside)\n",
    "isolated  = np.setdiff1d(outside,inside)\n",
    "\n",
    "print(f'contained: {len(contained)}\\npartial: {len(partial)}\\nisolated: {len(isolated)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_tmp = associations[['assoc_ID','X','Y']].copy()\n",
    "assoc_tmp.add_index('assoc_ID')\n",
    "\n",
    "assoc_tmp['env_asc'] = [environment_dict[env_masks_asc.data[y,x]] for \n",
    "                          x,y in zip(assoc_tmp['X'].astype(int),assoc_tmp['Y'].astype(int))]\n",
    "del assoc_tmp[['X','Y']]\n",
    "\n",
    "assoc_tmp['overlap'] = np.empty(len(associations),dtype='U9')\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],contained)] = 'contained'\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],partial)]   = 'partial'\n",
    "assoc_tmp['overlap'][np.isin(assoc_tmp['assoc_ID'],isolated)]  = 'isolated'\n",
    "assoc_tmp['1to1'] = False\n",
    "assoc_tmp['1to1'][np.isin(assoc_tmp['assoc_ID'],list(isolated_assoc))] = True\n",
    "assoc_tmp['Nnebulae'] = [len(associations_dict[k]) for k in assoc_tmp['assoc_ID']]\n",
    "\n",
    "assoc_tmp['region_ID'] = np.nan\n",
    "assoc_tmp['region_ID'][assoc_tmp['1to1']] = [associations_dict[k][0] for k in assoc_tmp[assoc_tmp['1to1']]['assoc_ID']]\n",
    "\n",
    "overlap = join(\n",
    "    Table(np.unique(associations_mask.data[~np.isnan(associations_mask.data)],return_counts=True),names=['assoc_ID','size']),\n",
    "    Table(np.unique(associations_mask.data[~np.isnan(nebulae_hst) & ~np.isnan(associations_mask.data)],return_counts=True),names=['assoc_ID','overlap_size']),\n",
    "    keys=['assoc_ID'],join_type='outer')\n",
    "overlap = overlap.filled(0)\n",
    "overlap['overlap_asc'] = overlap['overlap_size']/overlap['size']\n",
    "overlap['overlap_asc'].info.format = '%.2f'\n",
    "assoc_tmp = join(assoc_tmp,overlap[['assoc_ID','overlap_asc']],keys='assoc_ID')\n",
    "\n",
    "print('write to file')\n",
    "hdu = fits.BinTableHDU(assoc_tmp,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_associations.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.regions import find_neighbors\n",
    "from tqdm import tqdm \n",
    "\n",
    "nebulae_tmp = nebulae[['region_ID','x','y']].copy()\n",
    "nebulae_tmp.add_index('region_ID')\n",
    "\n",
    "nebulae_tmp['env_neb'] = [environment_dict[env_masks_neb.data[y,x]] for \n",
    "                          x,y in zip(nebulae_tmp['x'].astype(int),nebulae_tmp['y'].astype(int))]\n",
    "\n",
    "nebulae_tmp['neighbors'] = np.nan\n",
    "for row in tqdm(nebulae_tmp):\n",
    "    row['neighbors'] = len(find_neighbors(nebulae_mask.data,tuple(row[['x','y']]),row['region_ID'],plot=False))\n",
    "del nebulae_tmp[['x','y']]\n",
    "\n",
    "nebulae_tmp['1to1'] = False\n",
    "nebulae_tmp['1to1'][np.isin(nebulae_tmp['region_ID'],list(isolated_nebulae))] = True\n",
    "nebulae_tmp['Nassoc'] = [len(nebulae_dict[k]) for k in nebulae_tmp['region_ID']]\n",
    "nebulae_tmp['assoc_ID'] = np.nan\n",
    "nebulae_tmp['assoc_ID'][nebulae_tmp['1to1']] = [nebulae_dict[k][0] for k in nebulae_tmp[nebulae_tmp['1to1']]['region_ID']]\n",
    "\n",
    "\n",
    "overlap = join(\n",
    "    Table(np.unique(nebulae_hst[~np.isnan(nebulae_hst)],return_counts=True),names=['region_ID','size']),\n",
    "    Table(np.unique(nebulae_hst[~np.isnan(nebulae_hst) & ~np.isnan(associations_mask.data)],return_counts=True),names=['region_ID','overlap_size']),\n",
    "    keys=['region_ID'],join_type='outer')\n",
    "overlap = neb_tbl.filled(0)\n",
    "overlap['overlap_neb'] = overlap['overlap_size']/overlap['size']\n",
    "overlap['overlap_neb'].info.format = '%.2f'\n",
    "nebulae_tmp = join(nebulae_tmp,overlap[['region_ID','overlap_neb']],keys='region_ID')\n",
    "\n",
    "\n",
    "hdu = fits.BinTableHDU(nebulae_tmp,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_nebulae.fits',overwrite=True)\n",
    "#del nebulae_tmp['1to1']\n",
    "\n",
    "print(f'{np.sum(nebulae_tmp[\"neighbors\"]==0)} nebulae have no neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "catalogue = join(assoc_tmp,nebulae_tmp,keys=['assoc_ID','region_ID'])\n",
    "catalogue = join(catalogue,nebulae,keys='region_ID')\n",
    "catalogue = join(catalogue,associations,keys='assoc_ID')\n",
    "\n",
    "# pay attention to the order of assoc, neb\n",
    "catalogue.rename_columns(['X','Y','x','y','RA','DEC','cen_ra','cen_dec',\n",
    "                          'reg_area','region_area',\n",
    "                          'EBV_1','EBV_2','EBV_err','EBV_ERR',\n",
    "                          'SkyCoord_1','SkyCoord_2'],\n",
    "                         ['x_asc','y_asc','x_neb','y_neb','ra_asc','dec_asc','ra_neb','dec_neb',\n",
    "                          'area_asc','area_neb',\n",
    "                          'EBV_balmer','EBV_stars','EBV_balmer_err','EBV_stars_err',\n",
    "                          'SkyCoord_asc','SkyCoord_neb'])\n",
    "\n",
    "# separation to other associations and nebulae\n",
    "idx,sep_asc,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_neb,_= match_coordinates_sky(catalogue['SkyCoord_neb'],nebulae['SkyCoord'],nthneighbor=2)\n",
    "catalogue['sep_asc'] = sep_asc.to(u.arcsec)\n",
    "catalogue['sep_neb'] = sep_neb.to(u.arcsec)\n",
    "\n",
    "# select the columns of the joined catalogue\n",
    "columns = ['assoc_ID','region_ID','x_asc','y_asc','x_neb','y_neb',\n",
    "           'ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_asc','SkyCoord_neb',\n",
    "           'env_asc','env_neb','area_asc','area_neb',\n",
    "           'sep_asc','sep_neb','neighbors','Nassoc','overlap','overlap_asc','overlap_neb',\n",
    "           'age','age_err','mass','mass_err','EBV_stars','EBV_stars_err','EBV_balmer','EBV_balmer_err',\n",
    "           'met_scal','met_scal_err','logq_D91','logq_D91_err',] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR')] + \\\n",
    "            [x for x in HII_regions.columns if x.endswith('_FLUX_CORR_ERR')] + \\\n",
    "            ['NUV_FLUX','NUV_FLUX_ERR','U_FLUX','U_FLUX_ERR','B_FLUX','B_FLUX_ERR',\n",
    "             'V_FLUX','V_FLUX_ERR','I_FLUX','I_FLUX_ERR'] + \\\n",
    "            ['HA/FUV','eq_width']\n",
    "catalogue = catalogue[columns]\n",
    "        \n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR')],\n",
    "                      [col.replace('FLUX_CORR','flux') for col in catalogue.columns if col.endswith('FLUX_CORR')])\n",
    "catalogue.rename_columns([col for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')],\n",
    "                      [col.replace('FLUX_CORR_ERR','flux_err') for col in catalogue.columns if col.endswith('FLUX_CORR_ERR')])\n",
    "catalogue['assoc_ID'] = catalogue['assoc_ID'].astype('int')\n",
    "catalogue['region_ID'] = catalogue['region_ID'].astype('int')\n",
    "\n",
    "catalogue.info.description = 'Joined catalogue between associations and nebulae'\n",
    "mean_sep = np.mean(catalogue['SkyCoord_asc'].separation(catalogue['SkyCoord_neb']))\n",
    "print(f'{len(catalogue)} objects in catalogue')\n",
    "print(f'the mean separation between cluster and association center is {mean_sep.to(u.arcsecond):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write\n",
    "export parts of the joined catalogue (right now only the fully contained objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = catalogue.copy() #[catalogue['contained']]\n",
    "#export.add_column(export['SkyCoord_asc'].to_string(style='hmsdms',precision=2),index=6,name='RaDec_asc')\n",
    "#export.add_column(export['SkyCoord_neb'].to_string(style='hmsdms',precision=2),index=8,name='RaDec_neb')\n",
    "\n",
    "RA_asc ,DEC_asc = zip(*[x.split(' ') for x in export['SkyCoord_asc'].to_string(style='hmsdms',precision=2)])\n",
    "RA_neb ,DEC_neb = zip(*[x.split(' ') for x in export['SkyCoord_neb'].to_string(style='hmsdms',precision=2)])\n",
    "\n",
    "export.add_column(RA_asc,index=6,name='Ra_asc')\n",
    "export.add_column(DEC_asc,index=8,name='Dec_asc')\n",
    "export.add_column(RA_neb,index=10,name='Ra_neb')\n",
    "export.add_column(DEC_neb,index=12,name='Dec_neb')\n",
    "\n",
    "for col in export.columns:\n",
    "    if col not in ['Ra_asc','Dec_asc','Ra_neb','Dec_neb','region_ID','cluster_ID','overlap']:\n",
    "        export[col].info.format = '%.2f'\n",
    "\n",
    "del export[['ra_asc','dec_asc','ra_neb','dec_neb','SkyCoord_neb','SkyCoord_asc','HA/FUV']]\n",
    "\n",
    "hdu = fits.BinTableHDU(export,name='joined catalogue')\n",
    "hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_associations_and_nebulae_joined.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_nebulae.yml') as f:\n",
    "    nebulae_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "with open(basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_associations.yml') as f:\n",
    "    associations_dict = yaml.load(f,Loader=yaml.SafeLoader)\n",
    "    \n",
    "filename = basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_associations.fits'\n",
    "assoc_tmp = Table(fits.getdata(filename,ext=1))\n",
    "associations=join(associations,assoc_tmp,keys='assoc_ID')\n",
    "associations.add_index('assoc_ID')\n",
    "\n",
    "filename = basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_nebulae.fits'\n",
    "nebulae_tmp = Table(fits.getdata(filename,ext=1))\n",
    "nebulae=join(nebulae,nebulae_tmp,keys='region_ID')\n",
    "nebulae.add_index('region_ID')\n",
    "# read in existing catalogues\n",
    "filename = basedir/'data'/'map_nebulae_association'/f'{name}_{scalepc}pc_associations_and_nebulae_joined.fits'\n",
    "catalogue = Table(fits.getdata(filename,ext=1))\n",
    "\n",
    "catalogue['SkyCoord_asc'] = SkyCoord(catalogue['Ra_asc'],catalogue['Dec_asc'])\n",
    "catalogue['SkyCoord_neb'] = SkyCoord(catalogue['Ra_neb'],catalogue['Dec_neb'])\n",
    "catalogue['HA/FUV'] = catalogue['HA6562_flux']/catalogue['FUV_flux']\n",
    "catalogue['HA/FUV_err'] = catalogue['HA/FUV']*np.sqrt((catalogue['HA6562_flux_err']/catalogue['HA6562_flux'])**2+(catalogue['FUV_flux_err']/catalogue['FUV_flux'])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### save cutout for each region in seperate fits file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from reproject import reproject_interp \n",
    "\n",
    "def save_cutouts(position,size=4*u.arcsec):\n",
    "    \n",
    "    header = fits.Header()\n",
    "    header['gal_name'] = name\n",
    "    header['RA'] = row['SkyCoord_asc'].ra.to(u.degree).value\n",
    "    header['DEC'] = row['SkyCoord_asc'].dec.to(u.degree).value\n",
    "    header['RADESYS'] = 'ICRS'\n",
    "    header['regionID'] = row['region_ID']\n",
    "    header['assocID'] = row['cluster_ID']\n",
    "\n",
    "    hdul = fits.HDUList([fits.PrimaryHDU(header=header)])\n",
    "\n",
    "    # save HST image\n",
    "    cutout = Cutout2D(F275.data,position=position,size=size,wcs=F275.wcs)\n",
    "    hdul.append(fits.ImageHDU(cutout.data,header=cutout.wcs.to_header(),name='F275'))\n",
    "\n",
    "    # save Halpha\n",
    "    Halpha_cutout, _  = reproject_interp(Halpha,output_projection=cutout.wcs,shape_out=cutout.shape,order='bilinear')    \n",
    "    hdul.append(fits.ImageHDU(Halpha_cutout,header=cutout.wcs.to_header(),name='Halpha'))\n",
    "\n",
    "\n",
    "    # save OIII\n",
    "    OIII_cutout, _  = reproject_interp(OIII,output_projection=cutout.wcs,shape_out=cutout.shape,order='bilinear')    \n",
    "    hdul.append(fits.ImageHDU(OIII_cutout,header=cutout.wcs.to_header(),name='OIII'))\n",
    "\n",
    "\n",
    "    # save nebulae mask\n",
    "    nebulae_cutout, _  = reproject_interp(nebulae_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')    \n",
    "    hdul.append(fits.ImageHDU(nebulae_cutout,header=cutout.wcs.to_header(),name='nebulae'))\n",
    "\n",
    "\n",
    "    # save association mask\n",
    "    assoc_cutout, _  = reproject_interp(associations_mask,output_projection=cutout.wcs,shape_out=cutout.shape,order='nearest-neighbor')    \n",
    "    hdul.append(fits.ImageHDU(assoc_cutout,header=cutout.wcs.to_header(),name='assoc'))\n",
    "\n",
    "    hdul.writeto(basedir/'data'/'cutouts'/f'{name}_region{row[\"region_ID\"]}.fits',overwrite=True,checksum=True)\n",
    "    \n",
    "    \n",
    "row = catalogue[0]\n",
    "position = row['SkyCoord_neb']\n",
    "\n",
    "save_cutouts(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cutouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the joined catalogue (containing only nebulae and clusters with a 1 to 1 relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import single_cutout\n",
    "\n",
    "region_ID = 95\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,8))\n",
    "single_cutout(ax=ax,position = position,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             #label= f'{region_ID}/{nebulae_dict[region_ID][0]}',\n",
    "             size = 2*u.arcsecond)\n",
    "ax.axis('off')\n",
    "plt.savefig(basedir/'reports'/name/'single_region.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_cutout\n",
    "\n",
    "sample = catalogue[:12]\n",
    "# this is for NGC1365 to illustrate the different flags\n",
    "#sample = associations[np.searchsorted(associations['assoc_ID'],[63,26,40,17,4,490,343])]\n",
    "#labels = ['partial','partial 1to1','contained','contained 1to1','isolated','neighbors=0','neighbors=1','Nassoc=0',]\n",
    "#positions = list(sample['SkyCoord'])\n",
    "#positions.append(SkyCoord(53.3960053*u.deg, -36.13620865*u.deg))\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_isolated_associations_F275'\n",
    "positions = sample['SkyCoord']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in sample[['region_ID','assoc_ID']]]\n",
    "\n",
    "multi_cutout(positions = positions,\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a multi page pdf with all isolated objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import multi_page_cutout\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_{scalepc}pc_isolated_associations_F275'\n",
    "positions = catalogue['SkyCoord_neb'][catalogue['overlap']=='contained']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in catalogue[['region_ID','assoc_ID']][catalogue['overlap']=='contained']]\n",
    "\n",
    "filename = basedir/'reports'/'low_age_low_HaFUV_cutouts'\n",
    "sub = catalogue[(catalogue['age']<=1) & (catalogue['HA/FUV']<10)]\n",
    "positions = sub['SkyCoord_neb']\n",
    "labels = [f'{ri}/{ci}' for ri, ci in sub[['region_ID','assoc_ID']]]\n",
    "\n",
    "multi_page_cutout(positions = positions[:59],\n",
    "             image = F275,\n",
    "             mask1 = nebulae_mask,\n",
    "             mask2 = associations_mask,\n",
    "             #points= clusters,\n",
    "             labels= labels,\n",
    "             size = 4*u.arcsecond,\n",
    "             filename=filename,\n",
    "             ncols=5,nrows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WFI_cutout = Cutout2D(WFI.data,p['SkyCoord'],size=4*u.arcmin,wcs=WFI.wcs)\n",
    "\n",
    "# project from muse to hst coordinates\n",
    "reg_muse_wfi = reg_muse_sky.to_pixel(WFI_cutout.wcs)\n",
    "reg_hst_wfi  = reg_hst_sky.to_pixel(WFI_cutout.wcs)\n",
    "\n",
    "# plot image\n",
    "fig,ax=plt.subplots(figsize=(two_column,two_column),subplot_kw={'projection': WFI_cutout.wcs})\n",
    "norm = simple_norm(WFI_cutout.data,clip=False,percent=96)\n",
    "ax.imshow(WFI_cutout.data,norm=norm,origin='lower',cmap=plt.cm.Greys)\n",
    "add_scale(ax,u.arcmin,label=\"1'\",color='white',fontsize=10)\n",
    "\n",
    "reg_muse_wfi.plot(ax=ax,ec='tab:red',label='MUSE')\n",
    "reg_hst_wfi.plot(ax=ax,ec='tab:orange',label='HST')\n",
    "\n",
    "x,y = catalogue['SkyCoord_neb'][catalogue['contained']].to_pixel(WFI_cutout.wcs)\n",
    "ax.scatter(x,y,marker='s',facecolors='none',s=30,lw=1,color='tab:red')\n",
    "\n",
    "ax.coords[0].set_ticks_visible(False)\n",
    "ax.coords[1].set_ticks_visible(False)\n",
    "ax.coords[0].set_ticklabel_visible(False)\n",
    "ax.coords[1].set_ticklabel_visible(False)\n",
    "#ax.set(xlim=[3000,11000],ylim=[3000,11000])\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_location_in_galaxy.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations in joined catalogue\n",
    "\n",
    "first we create a mask to select a subset of objects (e.g. based on mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "# separation to other associations\n",
    "idx,sep_others,_= match_coordinates_sky(catalogue['SkyCoord_asc'],associations['SkyCoord'],nthneighbor=2)\n",
    "idx,sep_int,_= match_coordinates_sky(catalogue['SkyCoord_asc'],catalogue['SkyCoord_neb'])\n",
    "\n",
    "# size of the association compared to the HII-region\n",
    "small_HII = (catalogue['area_neb']/0.039) / (catalogue['area_asc']/11.95) > 2\n",
    "\n",
    "# distance to centre of galaxy\n",
    "galactic_center = SkyCoord(ra=p['R.A.'],dec=p['Dec.'])\n",
    "catalogue['galactic_radius'] = catalogue['SkyCoord_neb'].separation(galactic_center).to(u.arcmin)\n",
    "\n",
    "# define the criteria which objects we use in the plot\n",
    "criteria = (catalogue['mass']>1e3) & (catalogue['overlap']=='contained') & (catalogue['age']<10) \n",
    "\n",
    "#& catalogue['contained'] #& (catalogue['galactic_radius']>1*u.arcmin)\n",
    "\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HA/FUV vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.errorbar(tmp['age'],tmp['HA/FUV'],fmt='o')\n",
    "            #xerr=tmp['age_err'],yerr=tmp['HA/FUV_err'])\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['HA/FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equivalent width vs cluster age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0.5,10.5]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(tmp['age'],tmp['eq_width']) #,c=tmp['mass'],vmin=1e5,vmax=3e5)\n",
    "#fig.colorbar(sc,label='mass / Msun')\n",
    "\n",
    "x,mean,std = bin_stat(tmp['age'],tmp['eq_width'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black',label='mass / Msun')\n",
    "ax.set(xlabel='Age/Myr',ylabel='equivalent width / Angstrom',xlim=xlim)\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_eq_width_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,100]\n",
    "criteria = HII_regions['HA/FUV']<150\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "sc= ax.scatter(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria])\n",
    "\n",
    "x,mean,std = bin_stat(HII_regions['eq_width'][criteria],HII_regions['HA/FUV'][criteria],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='equivalent width / Angstrom',ylabel='Halpha / FUV',xlim=xlim,ylim=(0,70))\n",
    "#ax.set_title(r'only clusters with $M>10^{5}M_\\odot$')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_HaFUV_over_eq_width.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extinction from stars and from nebulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# we expect EBV_balmer = 2 EBV_stars\n",
    "\n",
    "fig = plt.figure(figsize=(single_column,single_column/1.1))\n",
    "ax = fig.add_subplot()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size=\"10%\", pad=0.2,)\n",
    "\n",
    "xlim = [0,10]\n",
    "sc=ax.scatter(tmp['EBV_stars'],tmp['EBV_balmer'],c=tmp['age'],s=3,vmin=0,vmax=10,cmap=plt.cm.viridis_r)\n",
    "ax.plot([0,1],[0,2],color='black')\n",
    "ax.plot([0,2],[0,2],color='black')\n",
    "fig.colorbar(sc,label='age / Myr',cax=cax)\n",
    "ax.set(xlim=[0,0.7],ylim=[0,0.7],xlabel='E(B-V) stars',ylabel='E(B-V) Balmer')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/f'{name}_EBV_Balmer_vs_Stars.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corner Plot\n",
    "\n",
    "are there any obvious differences between eg the old, high EW regions versus young, low EW regions?\n",
    "\n",
    "Likewise for the HA/FUV vs HA EQW plot, how much of this correlation is driven by Halpha, rather than a general trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.plot import corner\n",
    "\n",
    "catalogue['HA/NUV'] = catalogue['HA6562_flux'] / catalogue['NUV_FLUX'] / 1e12\n",
    "\n",
    "filename = basedir/'reports'/name/f'{name}_corner'\n",
    "columns  = ['age','HA/FUV','met_scal','logq_D91']\n",
    "limits   = {'age':(0,10),'eq_width':(0,100),'HA/FUV':(0,50),'HA/NUV':(0,20),'met_scal':(8.4,8.7),'logq_D91':(6,8)}\n",
    "\n",
    "tmp = catalogue[(catalogue['overlap']=='contained') & (catalogue['mass']>1e3)]\n",
    "print(f'sample contains {len(tmp)} objects')\n",
    "\n",
    "corner(tmp,columns,limits,nbins=5,filename=filename,vmin=1000,vmax=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "\n",
    "lines = [col for col in catalogue.columns if col.endswith('_flux')]\n",
    "print(f'{len(lines)} different lines with {comb(len(lines),2)} possible combinations')\n",
    "\n",
    "correlation = []\n",
    "for pair in itertools.combinations(lines,2):\n",
    "    not_nan = ~np.isnan(catalogue[pair[0]]) & ~np.isnan(catalogue[pair[1]])\n",
    "    r,p = spearmanr(catalogue['age'],catalogue[pair[0]][not_nan]/catalogue[pair[1]][not_nan])\n",
    "    correlation.append((r,pair))\n",
    "a = [x for x in correlation if np.abs(x[0])>0.15]\n",
    "a.sort(key=lambda x: np.abs(x[0]),reverse=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "rho,(line1,line2) = a[-1]\n",
    "ax.scatter(catalogue['age'],catalogue[line1]/catalogue[line2])\n",
    "ax.set(xlabel='age',ylabel=f\"{line2.split('_')[0]}/{line1.split('_')[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['HA/SII'] = catalogue['HA6562_flux'] / catalogue['SII6716_flux']\n",
    "catalogue['HA/OI'] = catalogue['HA6562_flux'] / catalogue['OI6300_flux']\n",
    "catalogue['HA/SII'][~np.isfinite(catalogue['HA/SII'])] = np.nan\n",
    "catalogue['HA/OI'][~np.isfinite(catalogue['HA/OI'])] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halpha luminosity vs mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1e4,5e5]\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] \n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "ax.scatter(tmp['mass'],tmp['HA6562_flux'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['HA6562_flux'],xlim)\n",
    "#ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='Halpha',\n",
    "       xlim=xlim,ylim=[1e4,1e6],xscale='log',yscale='log')\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "xlim = [1,2e5]\n",
    "criteria = (catalogue['age']<20) #& (sep>Angle('3\"'))\n",
    "print(f'{np.sum(criteria)} objects match the criteria')\n",
    "tmp = catalogue[criteria] #catalogue[criteria]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(tmp['mass'],tmp['region_area'])\n",
    "x,mean,std = bin_stat(tmp['mass'],tmp['region_area'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='mass / Msun',ylabel='HII-region area',xlim=xlim)\n",
    "\n",
    "#plt.savefig(basedir/'reports'/name/'HaFUV_over_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associated vs isolated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "tmp = associations[(associations['mass']>1e3) & (associations['age']<10)]\n",
    "\n",
    "ages_con = tmp[tmp['overlap']=='contained']['age']\n",
    "ages_par = tmp[tmp['overlap']=='partial']['age']\n",
    "ages_iso = tmp[tmp['overlap']=='isolated']['age']\n",
    "\n",
    "print(f'ages: con={np.mean(ages_con):.2f}, par={np.mean(ages_par):.2f}, iso={np.mean(ages_iso):.2f}')\n",
    "\n",
    "ax1.hist(ages_con,bins=bins,histtype='step',label='contained')\n",
    "ax2.hist(ages_par,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages_iso,bins=bins,histtype='step',label='isolated')\n",
    "\n",
    "ax1.set_title(f'contained ({np.mean(ages_con):.2f} Myr)')\n",
    "ax2.set_title(f'partially ({np.mean(ages_par):.2f} Myr)')\n",
    "ax3.set_title(f'isolated ({np.mean(ages_iso):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,120],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_age_hist_contained.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "\n",
    "tmp = associations[(associations['mass']>1e3) & (associations['age']<10)]\n",
    "idx,sep,_=match_coordinates_sky(tmp['SkyCoord'],nebulae['SkyCoord'])\n",
    "\n",
    "ages1 = tmp[(sep<0.4*u.arcsec)]['age']\n",
    "ages2 = tmp[(sep>0.4*u.arcsec) & (sep<0.8*u.arcsec)]['age']\n",
    "ages3 = tmp[(sep>0.8*u.arcsec)]['age']\n",
    "\n",
    "print(f'mean age: 1={np.mean(ages1):.2f}, 2={np.mean(ages2):.2f}, 3={np.mean(ages3):.2f} Myr')\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "bins = np.arange(0,10,1)\n",
    "\n",
    "ax1.hist(ages1,bins=bins,histtype='step',label='isolated')\n",
    "ax2.hist(ages2,bins=bins,histtype='step',label='partially')\n",
    "ax3.hist(ages3,bins=bins,histtype='step',label='contained')\n",
    "ax1.set_title(r'$s<0.4\"$'+f' ({np.mean(ages1):.2f} Myr)')\n",
    "ax2.set_title(r'$0.4\"<s<0.8\"$' +f' ({np.mean(ages2):.2f} Myr)')\n",
    "ax3.set_title(r'$0.8\"<s$'+f' ({np.mean(ages3):.2f} Myr)')\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set(ylim=[0,100],xlabel='age / Myr')\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_age_hist_sep.pdf',dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different resolutions\n",
    "\n",
    "the associations are for 16pc, 32pc and 64pc resolution. Here we compare them to each other to see how accurate the age dating is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.io import read_associations\n",
    "\n",
    "name   = 'NGC1566'\n",
    "target = name\n",
    "associations16, associations_mask16 = read_associations(folder=data_ext/'HST',target=target,scalepc=16)\n",
    "associations32, associations_mask32 = read_associations(folder=data_ext/'HST',target=target,scalepc=32)\n",
    "associations64, associations_mask64 = read_associations(folder=data_ext/'HST',target=target,scalepc=64)\n",
    "\n",
    "associations16.rename_columns(list(associations16.columns),[x+'_16' for x in associations16.columns])\n",
    "associations32.rename_columns(list(associations32.columns),[x+'_32' for x in associations32.columns])\n",
    "associations64.rename_columns(list(associations64.columns),[x+'_64' for x in associations64.columns])\n",
    "\n",
    "\n",
    "associations16.add_index('assoc_ID_16')\n",
    "associations32.add_index('assoc_ID_32')\n",
    "associations64.add_index('assoc_ID_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_catalogues_with_masks(mask1,mask2):\n",
    "    '''find the overlap between two masks\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    scale = 10**np.ceil(np.log10(max(mask1[~np.isnan(mask1)])))\n",
    "    s_arr = mask1.data/scale+mask2\n",
    "\n",
    "    # ids of associations, nebulae and combination (sum) of both\n",
    "    id1 = np.unique(mask1[~np.isnan(mask1)]).astype(int)\n",
    "    id2 = np.unique(mask2[~np.isnan(mask2)]).astype(int)\n",
    "    # the labels of the added masks\n",
    "    id3 = np.unique(s_arr[~np.isnan(s_arr)])\n",
    "\n",
    "    # this splits the sum into two parts (nebulae and associations)\n",
    "    modf1,modf2 = np.modf(id3)\n",
    "    modf2 = modf2.astype(int)\n",
    "    modf1 = np.round(modf1*scale).astype(int)\n",
    "\n",
    "    unique1, count1 = np.unique(modf1,return_counts=True)\n",
    "    unique1, count2 = np.unique(modf2,return_counts=True)\n",
    "\n",
    "    dict1 = {int(n) : modf2[modf1==n].tolist() for n in id1}     \n",
    "    dict2 = {int(n) : modf1[modf2==n].tolist() for n in id2}     \n",
    "                \n",
    "    return dict1, dict2\n",
    "\n",
    "dict_16_32, dict_32_16 = match_catalogues_with_masks(associations_mask16.data,\n",
    "                                                     associations_mask32.data)\n",
    "dict_32_64, dict_64_32 = match_catalogues_with_masks(associations_mask32.data,\n",
    "                                                     associations_mask64.data)\n",
    "dict_64_16, dict_16_64 = match_catalogues_with_masks(associations_mask64.data,\n",
    "                                                     associations_mask16.data)\n",
    "\n",
    "isolated_16_32 = set()\n",
    "isolated_32_16 = set()\n",
    "for n,v in dict_16_32.items():\n",
    "    if len(v)==1:\n",
    "        if len(dict_32_16[v[0]])==1:\n",
    "            isolated_16_32.add(n)\n",
    "            isolated_32_16.add(v[0])\n",
    "\n",
    "isolated_32_64 = set()\n",
    "isolated_64_32 = set()\n",
    "for n,v in dict_32_64.items():\n",
    "    if len(v)==1:\n",
    "        if len(dict_64_32[v[0]])==1:\n",
    "            isolated_32_64.add(n)\n",
    "            isolated_64_32.add(v[0])\n",
    "            \n",
    "isolated_64_16 = set()\n",
    "isolated_16_64 = set()\n",
    "for n,v in dict_64_16.items():\n",
    "    if len(v)==1:\n",
    "        if len(dict_16_64[v[0]])==1:\n",
    "            isolated_64_16.add(n)\n",
    "            isolated_16_64.add(v[0])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the subsample from the association catalogues\n",
    "sub32 = associations32.copy() #[np.isin(associations32['assoc_ID_32'],list(isolated_32_16 | isolated_32_64))].copy()\n",
    "sub16 = associations16[np.isin(associations16['assoc_ID_16'],list(isolated_16_32))].copy()\n",
    "sub64 = associations64[np.isin(associations64['assoc_ID_64'],list(isolated_64_32))].copy()\n",
    "\n",
    "#del sub32['SkyCoord_32']\n",
    "del sub16['SkyCoord_16']\n",
    "del sub64['SkyCoord_64']\n",
    "\n",
    "sub16['assoc_ID_32'] = [dict_16_32[x][0] for x in sub16['assoc_ID_16']]\n",
    "sub64['assoc_ID_32'] = [dict_64_32[x][0] for x in sub64['assoc_ID_64']]\n",
    "\n",
    "sub32 = join(sub32,sub16,keys=['assoc_ID_32'],join_type='outer')\n",
    "sub32 = join(sub32,sub64,keys=['assoc_ID_32'],join_type='outer')\n",
    "sub32 = sub32.filled(np.nan)\n",
    "sub32.add_index('assoc_ID_32')\n",
    "\n",
    "# for associations that overlap with multiple associations we take the mean\n",
    "for assoc_ID_32 in sub32[np.isnan(sub32['assoc_ID_16'])]['assoc_ID_32']:\n",
    "    if len(dict_32_16[assoc_ID_32])>1:\n",
    "        ages = associations16[np.isin(associations16['assoc_ID_16'],dict_32_16[assoc_ID_32])]['age_16']\n",
    "        sub32.loc[assoc_ID_32]['age_16'] = np.mean(ages)\n",
    "        \n",
    "for assoc_ID_32 in sub32[np.isnan(sub32['assoc_ID_64'])]['assoc_ID_32']:\n",
    "    if len(dict_32_64[assoc_ID_32])>1:\n",
    "        ages = associations64[np.isin(associations64['assoc_ID_64'],dict_32_64[assoc_ID_32])]['age_64']\n",
    "        sub32.loc[assoc_ID_32]['age_64'] = np.mean(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(basedir/'data'/'map_nebulae_association'/f'{name}_32pc_associations.fits') as hdul:\n",
    "    assoc_tmp = Table(hdul[1].data)\n",
    "print(f'original length: {len(assoc_tmp)}')\n",
    "#del assoc_tmp['uniform_age']\n",
    "\n",
    "sub32.rename_column('assoc_ID_32','assoc_ID')\n",
    "assoc_tmp = join(assoc_tmp,sub32['assoc_ID','assoc_ID_16','assoc_ID_64','age_16','age_64'],keys=['assoc_ID'])\n",
    "print(f'final length: {len(assoc_tmp)}')\n",
    "\n",
    "#hdu = fits.BinTableHDU(assoc_tmp,name='joined catalogue')\n",
    "#hdu.writeto(basedir/'data'/'map_nebulae_association'/f'{name}_32pc_associations.fits',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = np.abs(sub32['age_32']-sub32['age_16'])>sub32['age_err_32']\n",
    "criteria |= np.abs(sub32['age_32']-sub32['age_64'])>sub32['age_err_32']\n",
    "#sub32[~criteria][['assoc_ID_16','assoc_ID_32','assoc_ID_64','age_16','age_32','age_64','age_err_32']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove associations with different ages \n",
    "uniform_age32 = []\n",
    "for k in dict_32_16.keys():\n",
    "    age     = associations32.loc[k]['age_32']\n",
    "    age_err = associations32.loc[k]['age_err_32']\n",
    "    for a16 in dict_32_16[k]:\n",
    "        if np.abs(associations16.loc[a16]['age_16']-age)>age_err:\n",
    "            break\n",
    "    else:\n",
    "        for a64 in dict_32_64[k]:\n",
    "            if np.abs(associations64.loc[a64]['age_64']-age)>age_err:\n",
    "                break\n",
    "        else:\n",
    "            uniform_age32.append(k)\n",
    "print(f'32pc: {name}: {len(uniform_age32)} of {len(dict_32_16)} associations have uniform ages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot the ages from the different resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0,10]\n",
    "ylim = xlim\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.scatter(sub32['age_16'],sub32['age_32'],alpha=0.2,edgecolor=None)\n",
    "ax1.set(xlim=xlim,ylim=ylim,xlabel='age / Myr (16pc resolution)',ylabel='age / Myr (32pc resolution)')\n",
    "\n",
    "ax2.scatter(sub32['age_32'],sub32['age_64'],alpha=0.2,edgecolor=None)\n",
    "ax2.set(xlim=xlim,ylim=ylim,xlabel='age / Myr (32pc resolution)',ylabel='age / Myr (64pc resolution)')\n",
    "\n",
    "ax3.scatter(sub32['age_64'],sub32['age_16'],alpha=0.2,edgecolor=None)\n",
    "ax3.set(xlim=xlim,ylim=ylim,xlabel='age / Myr (64pc resolution)',ylabel='age / Myr (16pc resolution)')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/target/f'{name}_ages_by_resolution.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-9.5,10,1)\n",
    "\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.hist(sub32['age_16']-sub32['age_32'],bins=bins)\n",
    "#ax1.set_title(f\"median={np.nanmedian(sub32['age_16']-sub32['age_32']):.3f}\")\n",
    "ax1.set(xlabel=r'age$_{16\\mathrm{pc}}-$age$_{32\\mathrm{pc}}$ / Myr')\n",
    "\n",
    "ax2.hist(sub32['age_32']-sub32['age_64'],bins=bins)\n",
    "#ax2.set_title(f\"median={np.nanmedian(sub32['age_32']-sub32['age_64']):.3f}\")\n",
    "ax2.set(xlabel=r'age$_{32\\mathrm{pc}}-$age$_{64\\mathrm{pc}}$ / Myr')\n",
    "\n",
    "ax3.hist(sub32['age_64']-sub32['age_16'],bins=bins)\n",
    "#ax3.set_title(f\"median={np.nanmedian(sub32['age_64']-sub32['age_16']):.3f}\")\n",
    "ax3.set(xlabel=r'age$_{64\\mathrm{pc}}-$age$_{16\\mathrm{pc}}$ / Myr')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/target/f'{name}_ages_by_resolution_hist.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age difference vs U-band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "ax1.scatter(sub32['age_16']-sub32['age_32'],sub32['U_dolmag_vega_16']-sub32['U_dolmag_vega_32'])\n",
    "ax1.set(ylabel=r'U$_{16\\mathrm{pc}}-$U$_{32\\mathrm{pc}}$',\n",
    "        xlabel=r'age$_{16\\mathrm{pc}}-$age$_{32\\mathrm{pc}}$ / Myr',\n",
    "        xlim=[-100,100])\n",
    "\n",
    "ax2.scatter(sub32['age_32']-sub32['age_64'],sub32['U_dolmag_vega_32']-sub32['U_dolmag_vega_64'])\n",
    "ax2.set(ylabel=r'U$_{32\\mathrm{pc}}-$U$_{64\\mathrm{pc}}$',\n",
    "        xlabel=r'age$_{32\\mathrm{pc}}-$age$_{64\\mathrm{pc}}$ / Myr',\n",
    "        xlim=[-100,100])\n",
    "\n",
    "ax3.scatter(sub32['age_64']-sub32['age_16'],sub32['U_dolmag_vega_64']-sub32['U_dolmag_vega_16'])\n",
    "ax3.set(ylabel=r'U$_{64\\mathrm{pc}}-$U$_{16\\mathrm{pc}}$',\n",
    "        xlabel=r'age$_{64\\mathrm{pc}}-$age$_{16\\mathrm{pc}}$ / Myr',\n",
    "        xlim=[-100,100])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(basedir/'reports'/target/f'{name}_ages_by_resolution_vs_U_mag.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a cutout with all three masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "sub = catalogue[(catalogue['overlap']=='contained') & (catalogue['neighbors']==0)]\n",
    "position = sub['SkyCoord_neb'][4]\n",
    "#position = associations16.loc[5]['SkyCoord']\n",
    "size = 4*u.arcsec\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "cutout_F275 = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "norm = simple_norm(cutout_F275.data,stretch='linear',clip=False,percent=99.9)\n",
    "\n",
    "cutout_CO, _  = reproject_interp(CO,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "cutout_Halpha, _  = reproject_interp(Halpha,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape)    \n",
    "\n",
    "rgb = create_RGB(cutout_CO,cutout_Halpha,cutout_F275.data,\n",
    "                 percentile=[98,98,99.8],weights=[0.7,0.6,1])\n",
    "#ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "ax.imshow(rgb,origin='lower')\n",
    "\n",
    "# plot the nebulae catalogue\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_F275.wcs,shape_out=cutout_F275.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=1,label='HII-region')\n",
    "\n",
    "mask = np.zeros((*cutout_mask.shape,4))\n",
    "mask[~np.isnan(cutout_mask.data),:] = (0.17, 0.60, 0.17,0.05)\n",
    "#ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 32 pc\n",
    "cutout_32 = Cutout2D(associations_mask32.data,position,size=size,wcs=associations_mask32.wcs)\n",
    "region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_32.data)\n",
    "    blank_mask[cutout_32.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='blue',lw=1,label='32pc')\n",
    "\n",
    "mask = np.zeros((*cutout_32.shape,4))\n",
    "mask[~np.isnan(cutout_32.data),:] = (0.12,0.47,0.71,0.05)\n",
    "#ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "    \n",
    "#legend_without_duplicate_labels(ax)\n",
    "\n",
    "#t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "#t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "subsample = sub32[~np.isnan(sub32['assoc_ID_16']) & ~np.isnan(sub32['assoc_ID_64'])]\n",
    "position = subsample['SkyCoord_32'][0]\n",
    "#position = associations16.loc[5]['SkyCoord']\n",
    "size = 2*u.arcsec\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "\n",
    "cutout_image = Cutout2D(F275.data,position,size=size,wcs=F275.wcs)\n",
    "norm = simple_norm(cutout_image.data,stretch='linear',clip=False,percent=99.9)\n",
    "ax.imshow(cutout_image.data,origin='lower',norm=norm,cmap=plt.cm.gray_r)\n",
    "\n",
    "# plot the nebulae catalogue\n",
    "cutout_mask, _  = reproject_interp(nebulae_mask,output_projection=cutout_image.wcs,shape_out=cutout_image.shape,order='nearest-neighbor')    \n",
    "region_ID = np.unique(cutout_mask[~np.isnan(cutout_mask)])\n",
    "\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_mask)\n",
    "    blank_mask[cutout_mask==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.5,label='HII-region')\n",
    "\n",
    "mask = np.zeros((*cutout_mask.shape,4))\n",
    "mask[~np.isnan(cutout_mask.data),:] = (0.84, 0.15, 0.16,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 16 pc\n",
    "cutout_16 = Cutout2D(associations_mask16.data,position,size=size,wcs=associations_mask16.wcs)\n",
    "region_ID = np.unique(cutout_16.data[~np.isnan(cutout_16.data)])\n",
    "print('16pc: ' + str(region_ID))\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_16.data)\n",
    "    blank_mask[cutout_16.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.5,label='16pc')\n",
    "\n",
    "mask = np.zeros((*cutout_16.shape,4))\n",
    "mask[~np.isnan(cutout_16.data),:] = (0.12,0.47,0.71,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 32 pc\n",
    "cutout_32 = Cutout2D(associations_mask32.data,position,size=size,wcs=associations_mask32.wcs)\n",
    "region_ID = np.unique(cutout_32.data[~np.isnan(cutout_32.data)])\n",
    "print('32pc: ' + str(region_ID))\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_32.data)\n",
    "    blank_mask[cutout_32.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:green',lw=0.5,label='32pc')\n",
    "\n",
    "mask = np.zeros((*cutout_32.shape,4))\n",
    "mask[~np.isnan(cutout_32.data),:] = (0.17, 0.60, 0.17,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "\n",
    "# 64 pc\n",
    "cutout_64 = Cutout2D(associations_mask64.data,position,size=size,wcs=associations_mask64.wcs)\n",
    "region_ID = np.unique(cutout_64.data[~np.isnan(cutout_64.data)])\n",
    "print('64pc: ' + str(region_ID))\n",
    "contours = []\n",
    "for i in region_ID:\n",
    "    blank_mask = np.zeros_like(cutout_64.data)\n",
    "    blank_mask[cutout_64.data==i] = 1\n",
    "    contours += find_contours(blank_mask, 0.5)\n",
    "for coords in contours:\n",
    "    ax.plot(coords[:,1],coords[:,0],color='tab:orange',lw=0.5,label='64pc')\n",
    "\n",
    "mask = np.zeros((*cutout_64.shape,4))\n",
    "mask[~np.isnan(cutout_64.data),:] = (0.96, 0.48, 0.053,0.05)\n",
    "ax.imshow(mask,origin='lower')\n",
    "\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "    \n",
    "legend_without_duplicate_labels(ax)\n",
    "\n",
    "#t = ax.text(0.06,0.87,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "#t.set_bbox(dict(facecolor='white', alpha=1, ec='white'))\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure FUV and Halpha at association position\n",
    "\n",
    "### From point\n",
    "\n",
    "If I select an aperture smaller than a pixel, the measured flux is directly proportional to the apertuer size. Therefore it doesn't matter that the astrosat resolution is much worse than HST or MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust_extinction.parameter_averages import O94, CCM89\n",
    "\n",
    "extinction_model = CCM89(Rv=3.1)\n",
    "\n",
    "def extinction(EBV,EBV_err,wavelength,plot=False):\n",
    "    '''Calculate the extinction for a given EBV and wavelength with errors'''\n",
    "    \n",
    "    EBV = np.atleast_1d(EBV)\n",
    "    sample_size = 100000\n",
    "\n",
    "    ext = extinction_model.extinguish(wavelength,Ebv=EBV)\n",
    "    \n",
    "    EBV_rand = np.random.normal(loc=EBV,scale=EBV_err,size=(sample_size,len(EBV)))\n",
    "    ext_arr  = extinction_model.extinguish(wavelength,Ebv=EBV_rand)\n",
    "        \n",
    "    ext_err  = np.std(ext_arr,axis=0)\n",
    "    ext_mean = np.mean(ext_arr,axis=0)\n",
    "    \n",
    "    if plot:\n",
    "        fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "        ax1.hist(EBV_rand[:,0],bins=100)\n",
    "        ax1.axvline(EBV[0],color='black')\n",
    "        ax1.set(xlabel='E(B-V)')\n",
    "        ax2.hist(ext_arr[:,0],bins=100)\n",
    "        ax2.axvline(ext[0],color='black')\n",
    "        ax2.set(xlabel='extinction')\n",
    "        plt.show()\n",
    " \n",
    "    return ext,ext_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import SkyCircularAperture,SkyCircularAnnulus,aperture_photometry\n",
    "\n",
    "criteria = np.isin(associations['cluster_ID'],isolated_assoc)\n",
    "\n",
    "aperture_size = 1*u.arcsecond\n",
    "positions = associations['SkyCoord'][criteria]\n",
    "\n",
    "aperture = SkyCircularAperture(positions,aperture_size)\n",
    "\n",
    "fluxes = associations[['cluster_ID','SkyCoord','age','age_err','mass','mass_err','EBV','EBV_err']][criteria]\n",
    "fluxes['FUV'] = 1e20*aperture_photometry(astrosat,aperture)['aperture_sum']\n",
    "fluxes['HA'] = aperture_photometry(Halpha,aperture)['aperture_sum']\n",
    "\n",
    "\n",
    "\n",
    "# because the HII-regions are sometimes extended and not circular, this is probably not sufficient\n",
    "'''\n",
    "r_in,r_out = 5*u.arcsec,8*u.arcsec\n",
    "A_circle  = np.pi*aperture_size**2\n",
    "A_annulus = np.pi*(r_out**2-r_in**2)\n",
    "annulus_aperture = SkyCircularAnnulus(positions,r_in=r_in, r_out=r_out)\n",
    "\n",
    "\n",
    "fluxes['FUV_bkg'] = 1e20*aperture_photometry(astrosat,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['HA_bkg'] = aperture_photometry(Halpha,annulus_aperture)['aperture_sum']/A_annulus*A_circle\n",
    "fluxes['FUV'] = fluxes['FUV']-fluxes['FUV_bkg']\n",
    "fluxes['HA']  = fluxes['HA']-fluxes['HA_bkg']\n",
    "'''\n",
    "\n",
    "# E(B-V) is estimated from nebulae. E(B-V)_star = 0.5 E(B-V)_nebulae. FUV comes directly from stars\n",
    "extinction_mw  = extinction_model.extinguish(1481*u.angstrom,Ebv=0.5*p['E(B-V)'])\n",
    "ext_int,ext_int_err = extinction(associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=1481*u.angstrom)\n",
    "fluxes['FUV'] = fluxes['FUV'] / extinction_mw \n",
    "fluxes['FUV_CORR'] = fluxes['FUV'] / ext_int \n",
    "\n",
    "# the Halpha line maps are already MW extinction corrected\n",
    "ext_int,ext_int_err = extinction(2*associations['EBV'][criteria],associations['EBV_err'][criteria],wavelength=6562*u.angstrom)\n",
    "fluxes['HA_CORR'] = fluxes['HA'] / ext_int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA']/fluxes['FUV'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA']/fluxes['FUV'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,125])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster.auxiliary import bin_stat\n",
    "\n",
    "bins = 10\n",
    "xlim=[0.5,10.5]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'])\n",
    "\n",
    "x,mean,std = bin_stat(fluxes['age'],fluxes['HA_CORR']/fluxes['FUV_CORR'],xlim)\n",
    "ax.errorbar(x,mean,yerr=std,fmt='-',color='black')\n",
    "ax.set(xlabel='Age/Myr',ylabel='Halpha / FUV',xlim=xlim,ylim=[-10,70])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From mask\n",
    "\n",
    "because the resolution of MUSE and astrosat is so much worse than HST, many associations won't be resolved and hence we can not measure the fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp\n",
    "\n",
    "associations_muse, _  = reproject_interp(associations_mask,output_projection=Halpha.wcs,shape_out=Halpha.data.shape,order='nearest-neighbor') \n",
    "associations_astro, _ = reproject_interp(associations_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(set(np.unique(associations_muse[~np.isnan(associations_muse)])) & set(np.unique(associations_astro[~np.isnan(associations_astro)])))\n",
    "sample.sort()\n",
    "HA_flux = [np.sum(Halpha.data[associations_muse==cluster_ID]) for cluster_ID in sample]\n",
    "FUV_flux = [np.sum(astrosat.data[associations_astro==cluster_ID]) for cluster_ID in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import join\n",
    "\n",
    "fluxes = Table([sample,HA_flux,FUV_flux],names=['cluster_ID','HA','FUV'])\n",
    "catalogue = join(associations,fluxes[(~np.isnan(HA_flux)) & (~np.isnan(FUV_flux))],keys='cluster_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "xlim=[0,100]\n",
    "fig,ax=plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.scatter(catalogue['age'],catalogue['HA']/catalogue['FUV'])\n",
    "\n",
    "ax.set(xlim=xlim,xlabel='age/Myr',ylabel='Ha/FUV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem with the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_interp, reproject_exact\n",
    "from astropy.nddata import block_replicate\n",
    "\n",
    "nebulae_astrosat, _ = reproject_interp(nebulae_mask,output_projection=astrosat.wcs,shape_out=astrosat.data.shape,order='nearest-neighbor') \n",
    "astro_MUSE, _ = reproject_exact(astrosat,output_projection=Halpha.wcs,shape_out=Halpha.data.shape)    \n",
    "asttro_fine = block_replicate(astrosat,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_ID = 13\n",
    "position = nebulae['SkyCoord'][nebulae['region_ID']==region_ID]\n",
    "size = 4*u.arcsecond\n",
    "\n",
    "nebulae_cutout       = Cutout2D(nebulae_mask.data,position,size=size,wcs=nebulae_mask.wcs)\n",
    "astrosat_cutout_MUSE = Cutout2D(astro_MUSE,position,size=size,wcs=Halpha.wcs)\n",
    "\n",
    "nebulae_cutout_astrosat = Cutout2D(nebulae_astrosat,position,size=size,wcs=astrosat.wcs)\n",
    "astrosat_cutout = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)\n",
    "\n",
    "astrosat_up = block_replicate(astrosat_cutout.data,4)\n",
    "nebulae_fine, _ = reproject_interp(nebulae_mask,output_projection=astrosat_cutout.wcs,shape_out=astrosat_up.shape,order='nearest-neighbor') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "neb_contours = []\n",
    "for i in np.unique(nebulae_cutout.data[~np.isnan(nebulae_cutout.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout.data)\n",
    "    blank_mask[nebulae_cutout.data==i] = 1\n",
    "    neb_contours += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "neb_contours_astrosat = []\n",
    "for i in np.unique(nebulae_cutout_astrosat.data[~np.isnan(nebulae_cutout_astrosat.data)]):\n",
    "    blank_mask = np.zeros_like(nebulae_cutout_astrosat.data)\n",
    "    blank_mask[nebulae_cutout_astrosat.data==i] = 1\n",
    "    neb_contours_astrosat += find_contours(blank_mask, 0.5)\n",
    "\n",
    "neb_contours_fine = []\n",
    "for i in np.unique(nebulae_fine[~np.isnan(nebulae_fine)]):\n",
    "    blank_mask = np.zeros_like(nebulae_fine)\n",
    "    blank_mask[nebulae_fine==i] = 1\n",
    "    neb_contours_fine += find_contours(blank_mask, 0.5)\n",
    "    \n",
    "norm1 = simple_norm(astrosat_cutout.data,clip=False,percent=99)\n",
    "ax1.imshow(astrosat_cutout.data,norm=norm1)\n",
    "\n",
    "norm2 = simple_norm(astrosat_cutout_MUSE.data,clip=False,percent=99)\n",
    "ax2.imshow(astrosat_cutout_MUSE.data,norm=norm2)\n",
    "\n",
    "norm3 = simple_norm(astrosat_up,clip=False,percent=99)\n",
    "ax3.imshow(astrosat_up,norm=norm3)\n",
    "\n",
    "for coords in neb_contours:\n",
    "    ax2.plot(coords[:,1],coords[:,0],color='tab:blue',lw=0.8)\n",
    "for coords in neb_contours_astrosat:\n",
    "    ax1.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)   \n",
    "for coords in neb_contours_fine:\n",
    "    ax3.plot(coords[:,1],coords[:,0],color='tab:red',lw=0.8)  \n",
    "    \n",
    "ax1.set_title('original astrosat resolution')\n",
    "ax2.set_title('interpolated to MUSE resolution')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'interpolate regions: {np.sum(astrosat_cutout.data[nebulae_cutout_astrosat.data==region_ID]):.2g}')\n",
    "print(f'interpolate astrosat: {np.sum(np.sum(astrosat_cutout_MUSE.data[nebulae_cutout.data==region_ID])):.2g}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproject import reproject_exact\n",
    "\n",
    "ast_small , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(101,101)) \n",
    "ast_large , _  = reproject_exact(astrosat,output_projection=neb_cutout.wcs,shape_out=(202,202)) \n",
    "ast_org = Cutout2D(astrosat.data,position,size=size,wcs=astrosat.wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(10,5))\n",
    "\n",
    "ax1.imshow(ast_org.data)\n",
    "ax2.imshow(ast_small)\n",
    "ax3.imshow(ast_large)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "\n",
    "\n",
    "astrosat_upsampled = block_replicate(astrosat,4,conserve_sum=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starburst99\n",
    "\n",
    "compare our observations with simulated data\n",
    "\n",
    "**Note**: the GENEVAHIGH 23 (Z=0.008) model used a metallicity of 0.02 for the high resolution models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "cluster = Cluster(stellar_model='GENEVAv40',metallicity=0.014)\n",
    "cluster.measure_FUV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multiple Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test what multiple stellar populations would look like\n",
    "\n",
    "dT = 0.5*np.array([1,2,3,4,5]) * u.Myr\n",
    "\n",
    "time = cluster.ewidth['Time']\n",
    "Ha  = cluster.ewidth['Luminosity_H_A'].copy()\n",
    "FUV = cluster.FUV['FUV'].copy()\n",
    "\n",
    "fig,(ax1,ax2,ax3) =plt.subplots(ncols=3,figsize=(1.5*two_column,two_column/2))\n",
    "\n",
    "if False:\n",
    "    ax1.plot(time.value/1e6,Ha,label=f'{0*u.Myr}')\n",
    "    ax2.plot(time.value/1e6,FUV,label=f'{0*u.Myr}')   \n",
    "    ax3.plot(time.value/1e6,Ha/FUV,label=f'{0*u.Myr}')   \n",
    "\n",
    "for t in dT:\n",
    "    c0 = cluster.time_shift(t)\n",
    "    Ha_new  = np.interp(time,c0.ewidth['Time'],c0.ewidth['Luminosity_H_A'],left=0,right=0)\n",
    "    FUV_new = np.interp(time,c0.FUV['Time'],c0.FUV['FUV'],left=0,right=0)   \n",
    "     \n",
    "    ax1.axvline(t.value,color='black',ls='--')\n",
    "    ax2.axvline(t.value,color='black',ls='--')\n",
    "    #ax3.axvline(t.value,color='black',ls='--')\n",
    "    \n",
    "    if False:\n",
    "        ax1.plot(c0.ewidth['Time'].value/1e6,c0.ewidth['Luminosity_H_A'],label=f'{t}')\n",
    "        ax2.plot(c0.FUV['Time'].value/1e6,c0.FUV['FUV'],label=f'{t}')   \n",
    "        ax3.plot(c0.FUV['Time'].value/1e6,c0.ewidth['Luminosity_H_A']/c0.FUV['FUV'],label=f'{t}')    \n",
    "\n",
    "    Ha += Ha_new\n",
    "    FUV += FUV_new\n",
    "    \n",
    "ax1.plot(time.value/1e6,Ha,label='sum')\n",
    "ax2.plot(time.value/1e6,FUV,label='sum')   \n",
    "ax3.plot(time.value/1e6,Ha/FUV,label='sum')   \n",
    "    \n",
    "ax1.set(xlabel='time / Myr',ylabel='Halpha',xlim=[0,20])\n",
    "ax2.set(xlabel='time / Myr',ylabel='FUV',xlim=[0,20])\n",
    "ax3.set(xlabel='time / Myr',ylabel='Halpha / FUV',xlim=[0,20])\n",
    "\n",
    "ax1.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def __add__(table1,table2):\n",
    "    '''add two clusters with different ages\n",
    "    \n",
    "    this function takes the starburst\n",
    "    '''\n",
    "    \n",
    "    time1 = table1['Time']\n",
    "    \n",
    "    time2 = table2['Time']\n",
    "    \n",
    "    HI_rate = np.interp(time1,time2,table2['HI_rate'],left=0,right=0)\n",
    "    \n",
    "    plt.plot(time1,table1['HI_rate'])\n",
    "    plt.plot(time2,table2['HI_rate'])\n",
    "    plt.plot(time1,table1['HI_rate']+HI_rate)\n",
    "    plt.plot(time1,HI_rate)\n",
    "    plt.show()\n",
    "                                           \n",
    "    return 0\n",
    "    \n",
    "__add__(cluster.quanta,c2.quanta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) =plt.subplots(ncols=3,figsize=(two_column,two_column/3))\n",
    "\n",
    "\n",
    "ax1.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "ax1.set(ylabel='Halpha/ FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "\n",
    "ax2.plot(cluster.ewidth['Time']/1e6,cluster.ewidth['Equ_width_H_A'],color='black')\n",
    "ax2.set(ylabel='eq width / Angstrom',xlabel='time / Myr',xlim=[0,10])\n",
    "\n",
    "ax3.plot(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],color='black')\n",
    "sc = ax3.scatter(cluster.ewidth['Equ_width_H_A'],cluster.ewidth['Luminosity_H_A']/cluster.FUV['FUV'],\n",
    "                 c=cluster.ewidth['Time']/1e6,vmin=0,vmax=10)\n",
    "fig.colorbar(sc,ax=ax3,label='age / Myr')\n",
    "ax3.set(ylabel='Halpha/ FUV',xlabel='eq width')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'SB99_age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of ionizing photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue.add_index('region_ID')\n",
    "catalogue['Qpredicted'] = np.nan\n",
    "for region_ID in catalogue['region_ID']:\n",
    "    # the masses in the catalogue are off by a factor of 10\n",
    "    mass = 0.1*catalogue['mass'][catalogue['region_ID']==region_ID]\n",
    "    age  = catalogue['age'][catalogue['region_ID']==region_ID]*u.Myr\n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    idx = np.argmin(np.abs(scaled_cluster.quanta['Time']-age))\n",
    "    catalogue.loc[region_ID]['Qpredicted'] = scaled_cluster.quanta['HI_rate'][idx].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the conversion factor is from Niederhofer+2016\n",
    "\n",
    "$$\n",
    "Q(\\mathrm{H}\\alpha) = 7.31\\cdot 10^{11} L(\\mathrm{H}\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed\n",
    "catalogue['L(Ha)'] = (catalogue['HA6562_flux']*1e-20*u.erg/u.s/u.cm**2 *4*np.pi*Distance(distmod=p['(m-M)'])**2).to(u.erg/u.s)\n",
    "catalogue['Qobserved'] = 7.31e11*catalogue['L(Ha)']/u.erg\n",
    "tmp = catalogue[(catalogue['mass']>1e3) & (catalogue['age']<6) & (catalogue['age']>0)]\n",
    "# fesc = (Qpredicted-Qobserved) / Qpredicted \n",
    "fesc = (tmp['Qpredicted']-tmp['Qobserved'])/tmp['Qpredicted']\n",
    "\n",
    "print(f\"{np.sum(fesc<0)} of {len(tmp)} regions have negative fesc\")\n",
    "#Ha_from_q = (catalogue['Q']*1.37e-12*u.erg/u.s / (4*np.pi*Distance(distmod=p['(m-M)'])**2)).to(u.erg/u.s/u.cm**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "Qpredicted = np.logspace(-2,2)\n",
    "\n",
    "for f in [0.0,0.5,0.9,0.99]:\n",
    "    Qobserved = Qpredicted*(1-f)\n",
    "    ax.plot(Qpredicted,Qobserved,label=f'fesc={f}',zorder=1)\n",
    "\n",
    "sc=ax.scatter(tmp['Qpredicted']/1e50,tmp['Qobserved']/1e50,\n",
    "           c=tmp['age'],cmap=plt.cm.copper,vmin=0,vmax=6,s=2,zorder=2)\n",
    "ax.legend()\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "#ax.plot([0,100],np.array([10,110]),color='gray',ls='--')\n",
    "#ax.plot([0,100],np.array([-10,90]),color='gray',ls='--')\n",
    "\n",
    "ax.set(xlabel=r'$Q_{\\mathrm{H}\\alpha}$ / $10^{50} \\mathrm{s}^{-1}$ predicted',\n",
    "       ylabel='$Q$ / $10^{50} \\mathrm{s}^{-1}$ observed',\n",
    "       xscale='log',yscale='log',xlim=[1e-2,1e2],ylim=[1e-2,1e2])\n",
    "plt.savefig(basedir/'reports'/name/f'{name}_fesc.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.hist(fesc,bins=np.arange(0,1.1,0.05))\n",
    "ax.set(xlim=[0,1.1],xlabel='fesc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax.scatter(tmp['galactic_radius'],fesc)\n",
    "ax.set(ylim=[0,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Filter response curve\n",
    "\n",
    "to get the FUV flux by integrating the spectrum. The curves are from the [astrosat website](https://uvit.iiap.res.in/Instrument/Filters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from speclite.filters import FilterResponse, load_filters, plot_filters\n",
    "\n",
    "response_curve = ascii.read(basedir/'data'/'external'/'astrosat_response_curve.txt',\n",
    "                                     names=['wavelength','EA','Filter'])\n",
    "\n",
    "F148W_mask = response_curve['Filter']=='F148W'\n",
    "F148W_lam = response_curve['wavelength'][F148W_mask]*u.angstrom\n",
    "F148W_res = response_curve['EA'][F148W_mask] / max(response_curve['EA'][F148W_mask])\n",
    "F148W = FilterResponse(F148W_lam,F148W_res,meta=dict(group_name='Astrosat',band_name='F148W'))\n",
    "\n",
    "F154W_mask = response_curve['Filter']=='F154W'\n",
    "F154W_lam  = response_curve['wavelength'][F154W_mask]*u.angstrom\n",
    "F154W_res  = response_curve['EA'][F154W_mask] / max(response_curve['EA'][F154W_mask])\n",
    "F154W = FilterResponse(F154W_lam,F154W_res,meta=dict(group_name='Astrosat',band_name='F154W'))\n",
    "\n",
    "astrosat_filter = load_filters('Astrosat-F148W', 'Astrosat-F154W')\n",
    "plot_filters(astrosat_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Halpha and FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "Ha  = cluster.ewidth['Luminosity_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "FUV  = cluster.FUV['FUV']\n",
    "time_FUV = cluster.FUV['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha,color='tab:red')\n",
    "ax1.set_ylabel('Halpha/ (erg/s)',color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr')\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "FUV_int = np.interp(time_HA,time_FUV,FUV)\n",
    "\n",
    "fig,(ax1,ax2)=plt.subplots(ncols=2,figsize=(two_column,two_column/2))\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='tab:blue')\n",
    "ax1.set_ylabel('Halpha / FUV',color='tab:blue')\n",
    "ax1.set(xlim=[0,3],ylim=[1e-5,0.0017],xlabel='Time/Myr')\n",
    "\n",
    "axt = ax1.twinx()\n",
    "quanta = cluster.quanta\n",
    "axt.plot(quanta['Time']/1e6,quanta['HI_rate'],color='tab:orange')\n",
    "axt.set_ylabel('ionizing photons / 1/s',color='tab:orange')\n",
    "axt.set(xlim=[0,10])\n",
    "\n",
    "HI_rate_int = np.interp(time_HA,quanta['Time'],quanta['HI_rate'])\n",
    "\n",
    "ax2.plot(HI_rate_int,Ha/FUV_int,color='black')\n",
    "sc = ax2.scatter(HI_rate_int,Ha/FUV_int,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(sc,cax=cax,label='age / Myr',pad=-1)\n",
    "\n",
    "ax2.set(xlabel='ionizing photons / 1/s',ylabel='Halpha / FUV')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "ax1.plot(time_HA/1e6,Ha/FUV_int,color='black')\n",
    "#sc = ax1.scatter(FUV_int,Halpha,c=time_HA/1e6,vmin=0,vmax=3)\n",
    "#fig.colorbar(sc,ax=ax1,label='age / Myr')\n",
    "ax1.set(ylabel=r'H$\\alpha$ / FUV',xlabel='time / Myr',xlim=[0,10])\n",
    "plt.savefig(basedir/'reports'/'age_vs_Ha_over_FUV.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [5e2,1e3,2e3,5e3,1e4,2e4,5e4]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    clusters[m] = Cluster(stellar_model=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "for m in [23,53,63]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax1.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax1.legend()\n",
    "\n",
    "for m in [24,54,64]:\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "\n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax2.plot(np.log10(FUV.value),np.log10(Halpha.value),label=label)\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    #sc = ax.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "ax2.set(ylabel=r'H$\\alpha$ / (erg/s)',xlabel='FUV / (erg/s)')\n",
    "ax2.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes =plt.subplots(nrows=2,ncols=3,figsize=(two_column,two_column/1.618))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for m in [23,53,63,24,54,64]:\n",
    "    \n",
    "    ax = next(axes_iter)\n",
    "    cl = clusters[m]\n",
    "    \n",
    "    Halpha  = cl.ewidth['Luminosity_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    FUV = cl.FUV['FUV']\n",
    "    ionizing = cl.quanta['HI_rate']\n",
    "  \n",
    "    label = f'{cl.stellar_model}, Z={cl.metallicity}'\n",
    "    ax.plot(Halpha/FUV,ionizing,color='black')\n",
    "    #t = ax.text(0.05,0.9,label, transform=ax.transAxes,color='black',fontsize=8)\n",
    "    ax.set_title(label,fontsize=8)\n",
    "    sc = ax.scatter(Halpha/FUV,ionizing,c=time_HA/1e6,vmin=0,vmax=10)\n",
    "\n",
    "    ax.set(ylabel=r'H$\\alpha$ / FUV',xlabel='ionization')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.85)\n",
    "cbar_ax = fig.add_axes([0.9, 0.1, 0.05, 0.7])\n",
    "fig.colorbar(sc, cax=cbar_ax,label='age / Myr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare observations to simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue['distance'] = np.nan\n",
    "for name in np.unique(catalogue['gal_name']):\n",
    "    catalogue['distance'][catalogue['gal_name']==name] = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(two_column,two_column/1.618))\n",
    "\n",
    "for mass in [2e3,5e3,1e4,2e4,5e4,1e5,2e5,5e5,1e6,2e6,5e6]:\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    Halpha  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    time_HA = scaled_cluster.ewidth['Time']\n",
    "    FUV = scaled_cluster.FUV['FUV']\n",
    "    \n",
    "    ax1.plot(np.log10(FUV.value),np.log10(Halpha.value),color='black')\n",
    "    ax1.text(np.log10(FUV.value)[0],np.log10(Halpha.value)[0],f'{mass:.0g}  ',\n",
    "            horizontalalignment='right',verticalalignment='bottom')\n",
    "    sc = ax1.scatter(np.log10(FUV.value),np.log10(Halpha.value),c=time_HA/1e6,vmin=0,vmax=30)\n",
    "\n",
    "Halpha_FLUX = ((catalogue['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "FUV_FLUX = 5e5*((catalogue['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*catalogue['distance']**2).value\n",
    "ax1.scatter(np.log10(FUV_FLUX),np.log10(Halpha_FLUX))\n",
    "\n",
    "ax1.set(ylabel=r'log10 H$\\alpha$ / (erg/s)',xlabel='log10 FUV / (erg/s)')\n",
    "fig.colorbar(sc,label='age / Myr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the grid to compare the observations to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time = len(cluster.ewidth['Time'])\n",
    "n_mass = 1000\n",
    "\n",
    "mass_min = 5e3\n",
    "mass_max = 1e6\n",
    "\n",
    "mass_grid = np.linspace(mass_min,mass_max,n_mass)\n",
    "\n",
    "HA_grid = np.zeros((n_time,n_mass))\n",
    "FUV_grid = np.zeros((n_time,n_mass))\n",
    "\n",
    "for i,mass in enumerate(mass_grid):\n",
    "    \n",
    "    scaled_cluster = cluster.scale(mass)\n",
    "    \n",
    "    HA_grid[:,i]  = scaled_cluster.ewidth['Luminosity_H_A']\n",
    "    FUV_grid[:,i] = scaled_cluster.FUV['FUV']\n",
    "\n",
    "time = scaled_cluster.FUV['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance = (sample_table.loc[name]['Distance']*u.Mpc).to(u.cm)\n",
    "\n",
    "mass, age, chi2 = [],[],[]\n",
    "for row in catalogue:\n",
    "    \n",
    "    Halpha_FLUX = ((row['HA6562_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    Halpha_ERR  = ((row['HA6562_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    FUV_FLUX = 1e6*((row['FUV_FLUX_CORR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    FUV_ERR  = ((row['FUV_FLUX_CORR_ERR']*1e-20*u.erg/u.s/u.cm**2) * 4*np.pi*row['distance']**2).value\n",
    "    \n",
    "    chi2_grid = (Halpha_FLUX-HA_grid)**2/Halpha_ERR**2 + (FUV_FLUX-FUV_grid)**2/FUV_ERR**2\n",
    "    \n",
    "    row,col = np.unravel_index(chi2_grid.argmin(), chi2_grid.shape)\n",
    "    mass.append(mass_grid[col])\n",
    "    age.append(time[row].value)\n",
    "    chi2.append(np.min(chi2_grid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) =plt.subplots(nrows=1,ncols=2,figsize=(two_column,two_column/2))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,catalogue['AGE_MINCHISQ'])\n",
    "ax1.set(ylim=[0,30],xlim=[0,30],xlabel='age from Nebulae / Myr',ylabel='age from Cluster / Myr')\n",
    "\n",
    "ax2.scatter(np.array(mass),catalogue['MASS_MINCHISQ'])\n",
    "ax2.set(xlabel='mass from Nebulae / Msun',ylabel='mass from Cluster / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(nrows=1,ncols=1,figsize=(single_column,single_column))\n",
    "\n",
    "ax1.scatter(np.array(age)/1e6,np.array(mass))\n",
    "ax1.set(xlim=[0,30],ylim=[0,5e5],xlabel='age from Nebulae / Myr',ylabel='mass / Msun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "eqwHA  = cluster.ewidth['Equ_width_H_A']\n",
    "time_HA = cluster.ewidth['Time']\n",
    "\n",
    "ax1.plot(time_HA/1e6,eqwHA,color='tab:red')\n",
    "ax1.set(xlabel='Time/Myr',ylabel='eq / AA',xlim=[0,10])\n",
    "\n",
    "#ax2 = ax1.twinx() \n",
    "#ax2.plot(time_FUV/1e6,FUV,color='tab:green')\n",
    "#ax2.set_ylabel('FUV / (erg/s)',color='tab:green')\n",
    "#ax2.set(xlabel='Time/Myr',xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    eqwHA  = cl.ewidth['Equ_width_H_A']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    #sl = cl.scale(1e5)\n",
    "    #eqwHA  = sl.ewidth['Equ_width_H_A']\n",
    "    #time_HA = sl.ewidth['Time']\n",
    "    #ax1.plot(np.log10(time_HA/u.yr),np.log10(eqwHA/u.angstrom),ls='--',color=c)\n",
    "    \n",
    "    \n",
    "ax1.set(xlabel='log (Time / Myr)',ylabel=r'log (W(H$\\alpha$) / $\\AA$)',xlim=[6,7.5])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "colors = ['tab:blue','tab:cyan','tab:red','tab:orange','tab:green','tab:olive']\n",
    "\n",
    "for m,c in zip([23,24,53,54,63,64],colors):\n",
    "    \n",
    "    cl = clusters[m]\n",
    "    \n",
    "    HAFUV  = cl.ewidth['Luminosity_H_A'] / cl.FUV['FUV']\n",
    "    time_HA = cl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,label=f'{cl.stellar_model }',color=c)\n",
    "    \n",
    "    sl = cl.scale(1e5)\n",
    "    HAFUV  = sl.ewidth['Luminosity_H_A'] / sl.FUV['FUV']\n",
    "    time_HA = sl.ewidth['Time']\n",
    "    ax1.plot(time_HA/1e6,HAFUV,ls='--',color=c)    \n",
    "    \n",
    "ax1.set(xlabel='Time / Myr',ylabel=r'HA/FUV',xlim=[0,10])\n",
    "ax1.legend()\n",
    "#plt.savefig(basedir/'reports'/'equivalent_width_vs_age.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "fig = plt.figure(figsize=(two_column,two_column*2/3))\n",
    "\n",
    "for i,stellar_model in enumerate(['GENEVASTD','GENEVAHIGH','PADOVASTD','PADOVAAGB','GENEVAv00','GENEVAv40']):\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    \n",
    "    cluster = Cluster(stellar_model=stellar_model,metallicity=0.008)\n",
    "    ax.plot(cluster.quanta['Time']/1e6,cluster.quanta['HI_rate'])\n",
    "    ax.set(xlim=[0,10],ylim=[0,7e52],xlabel='age/Myr',ylabel='HI rate')\n",
    "\n",
    "    ax.set_title(stellar_model)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test mass scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import Cluster\n",
    "\n",
    "c1e5 = Cluster(basedir/'..'/'starburst'/'data'/'model54_1e5Msun'/'cluster')\n",
    "c1e6 = Cluster(basedir/'..'/'starburst'/'data'/'model54_1e6Msun'/'cluster')\n",
    "s1e6 = c1e5.scale(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(ncols=3,figsize=(10,3))\n",
    "\n",
    "tname = 'ewidth'\n",
    "xname = 'Time'\n",
    "yname = 'eq_width_H_A'\n",
    "\n",
    "# get the table tname\n",
    "t_c1e5 = getattr(c1e5,tname)\n",
    "t_c1e6 = getattr(c1e6,tname)\n",
    "t_s1e6 = getattr(s1e6,tname)\n",
    "\n",
    "ax1.plot(t_c1e5[xname],t_c1e5[yname],label='c1e5')\n",
    "ax2.plot(t_c1e6[xname],t_c1e6[yname],label='c1e6')\n",
    "ax3.plot(t_s1e6[xname],t_s1e6[yname],label='s1e6')\n",
    "\n",
    "for ax,l in zip((ax1,ax2,ax3),('c1e5','c1e6','s1e6')):\n",
    "    ax.set(xlabel=xname.replace('_',''))\n",
    "    ax.set_title(l)\n",
    "ax1.set(ylabel=yname.replace('_',''))\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling of Ha/FUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "Ha1e5  = c1e5.ewidth['Luminosity_H_A']\n",
    "time_HA1e5 = c1e5.ewidth['Time']\n",
    "FUV1e5  = c1e5.FUV['FUV']\n",
    "time_FUV1e5 = c1e5.FUV['Time']\n",
    "FUV_int1e5 = np.interp(time_HA1e5,time_FUV1e5,FUV1e5)\n",
    "\n",
    "Ha1e6  = c1e6.ewidth['Luminosity_H_A']\n",
    "time_HA1e6 = c1e6.ewidth['Time']\n",
    "FUV1e6  = c1e6.FUV['FUV']\n",
    "time_FUV1e6 = c1e6.FUV['Time']\n",
    "FUV_int1e6 = np.interp(time_HA1e5,time_FUV1e6,FUV1e6)\n",
    "\n",
    "\n",
    "ax1.plot(time_HA1e5/1e6,Ha1e5/FUV_int1e5,color='tab:red',label='1e5Msun')\n",
    "ax1.plot(time_HA1e6/1e6,Ha1e6/FUV_int1e6,color='tab:green',label='1e6Msun')\n",
    "ax1.set(ylabel='Halpha/FUV',xlabel='Time/Myr',xlim=[0,10])\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling of eq width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 =plt.subplots(figsize=(single_column,single_column))\n",
    "\n",
    "eqw1e5  = c1e5.ewidth['eq_width_H_A']\n",
    "time_1e5 = c1e5.ewidth['Time']\n",
    "\n",
    "eqw1e6  = c1e6.ewidth['eq_width_H_A']\n",
    "time_1e6 = c1e6.ewidth['Time']\n",
    "\n",
    "\n",
    "ax1.plot(time_1e5/1e6,eqw1e5,color='tab:red',label='1e5Msun')\n",
    "ax1.plot(time_1e6/1e6,eqw1e6,color='tab:green',label='1e6Msun')\n",
    "ax1.set(ylabel='eq width',xlabel='Time/Myr',xlim=[0,10])\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starburst import create_new_starburst99_folder, stellar_models\n",
    "\n",
    "\n",
    "create_new_starburst99_folder(basedir/'..'/'starburst'/'data'/'PADOVASTD'/'33',\n",
    "                              overwrite=True,\n",
    "                              model=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age','mass','HA/FUV','HA6562_FLUX','region_area']\n",
    "\n",
    "\n",
    "data = np.zeros((len(catalogue),len(columns)))\n",
    "\n",
    "for i,col in enumerate(columns):\n",
    "    data[:,i] = catalogue[col].data\n",
    "data = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nebulae = PCA(n_components=2)\n",
    "principalComponents = pca_nebulae.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyNeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyneb as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O3 = pn.Atom('O',3, NLevels=5)\n",
    "O2 = pn.Atom('O',2, NLevels=5)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(13,7))\n",
    "O2.plotGrotrian(ax=ax1)\n",
    "O3.plotGrotrian(ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pn.Observation()\n",
    "\n",
    "for line,wave in zip(['OI6300','OII3726','OII7319','OII7330','OIII5006','NII5754',\n",
    "             'NII6583','SII6716','SII6730','SIII6312','SIII9068'],\n",
    "               [6300,3726,7319,7330,5007,5755,6584,6716,6731,6312,9069], \n",
    "               ):\n",
    "    Intens = subsample[f'{line}_FLUX_CORR']\n",
    "    Error  = subsample[f'{line}_FLUX_CORR_ERR']\n",
    "    line = pn.EmissionLine(line[0],len(line[1:-4]),wave,obsIntens=Intens,obsError=Error)\n",
    "    \n",
    "    obs.addLine(line)\n",
    "    \n",
    "diags = pn.Diagnostics()\n",
    "diags.addDiagsFromObs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "emisgrids = pn.getEmisGridDict(atomDict=diags.atomDict)\n",
    "diags.plot(emisgrids, obs, ax=ax,i_obs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te,ne = diags.getCrossTemDen(diag_tem='[NII] 5755/6584',diag_den='[SII] 6731/6716',obs=obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = [10000.]\n",
    "Ne = [1e3]\n",
    "# Define a dictionary to hold all the Atom objects needed\n",
    "all_atoms = pn.getAtomDict(atom_list=obs.getUniqueAtoms())\n",
    "# define a dictionary to store the abundances\n",
    "ab_dict = {}\n",
    "# we  use the following lines to determine the ionic abundances\n",
    "ab_labels = ['O3_5007A', 'H1r_4861A', 'O2_3726A', 'O2_7319A', 'O2_7330A',\n",
    "             'N2_5755A', 'N2_6584A', 'S2_6716A', 'S2_6731A', 'S3_6312A','S3_9069A']\n",
    "\n",
    "for line in obs.getSortedLines():\n",
    "    if line.label in ab_labels:\n",
    "        ab = all_atoms[line.atom].getIonAbundance(line.corrIntens, Te, Ne, \n",
    "                                                  to_eval=line.to_eval, Hbeta=100)\n",
    "        ab_dict[line.atom] = ab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import match_coordinates_sky\n",
    "idx,sep,_=match_coordinates_sky(associations['SkyCoord'],nebulae['SkyCoord'])\n",
    "\n",
    "sep = sep.to(u.arcsec).value\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "bins = np.arange(0,10)\n",
    "\n",
    "print(f\"x<0.5: {np.mean(associations[sep<0.5]['age']):.2f}\")\n",
    "print(f\"0.5<x<1: {np.mean(associations[(sep>0.5) & (sep<1)]['age']):.2f}\")\n",
    "print(f\"1<x: {np.mean(associations[sep>1]['age']):.2f}\")\n",
    "\n",
    "ax.hist(associations[sep<0.5]['age'],bins=bins,alpha=0.5,label=r'$x<0.5\"$')\n",
    "ax.hist(associations[(sep>0.5) & (sep<1)]['age'],bins=bins,alpha=0.5,label=r'$0.5\"<x<1\"$')\n",
    "ax.hist(associations[sep>1]['age'],bins=bins,alpha=0.5,label=r'$1\"<x$')\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure DIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "\n",
    "def measure_dig(data,mask,label,position,factor=1,max_iter=10,size=32,plot=True):\n",
    "    '''measure the diffuse ionized gas around an HII-region'''\n",
    "    \n",
    "    cutout_mask = Cutout2D(mask.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    cutout_data = Cutout2D(data.data,position,size=(size,size),mode='partial',fill_value=np.nan)\n",
    "    \n",
    "    area_mask  = np.sum(cutout_mask.data==label)\n",
    "    input_mask = cutout_mask.data==label\n",
    "    \n",
    "    n_iter = 0\n",
    "    while True:\n",
    "        n_iter+=1\n",
    "        boundaries = find_boundaries(input_mask,mode='outer')\n",
    "        input_mask |=boundaries\n",
    "        area_boundary = np.sum(input_mask & np.isnan(cutout_mask.data)) \n",
    "        if area_boundary > factor*area_mask or n_iter>max_iter: break\n",
    "            \n",
    "    if plot:\n",
    "        fig,ax=plt.subplots(figsize=(5,5))\n",
    "        ax.imshow(cutout_mask.data,origin='lower')\n",
    "        mask = np.zeros((*cutout_mask.shape,4))\n",
    "        mask[input_mask & np.isnan(cutout_mask.data),:] = (1,0,0,0.5)\n",
    "        ax.imshow(mask,origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "    #if np.sum(boundaries & np.isnan(cutout_mask.data))==0:\n",
    "    #    print(f'no boundaries for {label}')\n",
    "    dig = cutout_data.data[input_mask & np.isnan(cutout_mask.data)]\n",
    "\n",
    "    return np.median(dig),np.mean(dig),np.sum(dig)\n",
    "\n",
    "row = nebulae[4]\n",
    "measure_dig(Halpha,nebulae_mask,row['region_ID'],(row['x'],row['y']),factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['dig'] = np.nan\n",
    "for row in tmp:\n",
    "    row['dig'] = measure_dig(Halpha,nebulae_mask,row['region_ID'],(row['x'],row['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tmp['HA6562_FLUX'],tmp['dig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showcase matching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = nebulae['SkyCoord'][1094]\n",
    "size = 1.2*u.arcsec\n",
    "\n",
    "fig,(ax1) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "cutout_nebulae = Cutout2D(nebulae_mask.data,position,size=size,wcs=nebulae_mask.wcs)\n",
    "cutout_assoc = Cutout2D(associations_mask.data,position,size=size,wcs=associations_mask.wcs)\n",
    "reproject_nebula, _  = reproject_interp(nebulae_mask,output_projection=cutout_assoc.wcs,shape_out=cutout_assoc.shape,order='nearest-neighbor')    \n",
    "\n",
    "cutout_assoc.data[cutout_assoc.data==12] = 8\n",
    "cutout_nebulae.data[cutout_nebulae.data==1094] = 4\n",
    "reproject_nebula[reproject_nebula==1094] = 4\n",
    "\n",
    "ax1.imshow(cutout_nebulae.data,cmap=plt.cm.Reds,vmin=0,vmax=8)\n",
    "for (j,i),label in np.ndenumerate(cutout_nebulae.data):\n",
    "    ax1.text(i,j,f'{label:.0f}',ha='center',va='center')\n",
    "ax1.axis('off')\n",
    "#ax1.set_title('nebulae mask')\n",
    "plt.savefig(basedir/'reports'/'mask_matching_nebulae.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax2) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax2.imshow(reproject_nebula,cmap=plt.cm.Reds,vmin=0,vmax=8)\n",
    "for (j,i),label in np.ndenumerate(reproject_nebula):\n",
    "    ax2.text(i,j,f'{label:.0f}',ha='center',va='center',fontsize=6)\n",
    "ax2.axis('off')\n",
    "#ax2.set_title('reprojected nebulae mask')\n",
    "plt.savefig(basedir/'reports'/'mask_matching_nebulae_reproj.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax3) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax3.imshow(cutout_assoc.data/10,cmap=plt.cm.Blues,vmin=0,vmax=1.6)\n",
    "for (j,i),label in np.ndenumerate(cutout_assoc.data):\n",
    "    ax3.text(i,j,f'{label/10:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax3.axis('off')\n",
    "#ax3.set_title('association mask')\n",
    "plt.savefig(basedir/'reports'/'mask_matching_assoc.png',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax4) = plt.subplots(figsize=(12,12))\n",
    "\n",
    "ax4.imshow(cutout_assoc.data/10+reproject_nebula,cmap=plt.cm.Greens,vmin=0,vmax=9.6)\n",
    "for (j,i),label in np.ndenumerate(cutout_assoc.data/10+reproject_nebula):\n",
    "    ax4.text(i,j,f'{label:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax4.axis('off')\n",
    "#ax4.set_title('association+nebulae mask')\n",
    "\n",
    "plt.savefig(basedir/'reports'/'mask_matching_sum.png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.full ((5,5),np.nan)\n",
    "arr1[2:4,1:3] = 4\n",
    "\n",
    "arr2 = np.full ((10,10),np.nan)\n",
    "arr2[4:8,2:6] = 4\n",
    "\n",
    "arr3 = np.full ((10,10),np.nan)\n",
    "arr3[5:7,4:8] = 2\n",
    "arr3[2:5,0:3] = 9\n",
    "\n",
    "arr4 = arr2+arr3/10\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(ncols=4,figsize=(12,4))\n",
    "\n",
    "extent = (0, arr1.shape[1], arr1.shape[0], 0)\n",
    "ax1.imshow(arr1,cmap=plt.cm.Reds,vmin=0,vmax=10,extent=extent)\n",
    "for (j,i),label in np.ndenumerate(arr1):\n",
    "    ax1.text(i+0.5,j+0.5,f'{label:.0f}',ha='center',va='center',fontsize=6)\n",
    "ax1.set_xticks(np.arange(0, arr1.shape[0], 1), minor=True)\n",
    "ax1.set_yticks(np.arange(0, arr1.shape[1], 1), minor=True)\n",
    "ax1.set_title('native nebula mask')\n",
    "\n",
    "extent = (0, arr2.shape[1], arr2.shape[0], 0)\n",
    "ax2.imshow(arr2,cmap=plt.cm.Reds,vmin=0,vmax=10,extent=extent)\n",
    "for (j,i),label in np.ndenumerate(arr2):\n",
    "    ax2.text(i+0.5,j+0.5,f'{label:.0f}',ha='center',va='center',fontsize=6)\n",
    "ax2.set_xticks(np.arange(0, arr2.shape[0], 1), minor=True)\n",
    "ax2.set_yticks(np.arange(0, arr2.shape[1], 1), minor=True)\n",
    "ax2.set_title('reprojected nebula mask')\n",
    "\n",
    "extent = (0, arr3.shape[1], arr3.shape[0], 0)\n",
    "ax3.imshow(arr3/10,cmap=plt.cm.Blues,vmin=-0.5,vmax=1.5,extent=extent)\n",
    "for (j,i),label in np.ndenumerate(arr3/10):\n",
    "    ax3.text(i+0.5,j+0.5,f'{label:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax3.set_xticks(np.arange(0, arr3.shape[0], 1), minor=True)\n",
    "ax3.set_yticks(np.arange(0, arr3.shape[1], 1), minor=True)\n",
    "ax3.set_title('association mask')\n",
    "\n",
    "extent = (0, arr4.shape[1], arr4.shape[0], 0)\n",
    "ax4.imshow(arr2,cmap=plt.cm.Reds,vmin=0,vmax=10,extent=extent)\n",
    "ax4.imshow(arr3/10,cmap=plt.cm.Blues,vmin=-0.5,vmax=1.5,extent=extent,alpha=1)\n",
    "ax4.imshow(arr4,cmap=plt.cm.Purples,vmin=0,vmax=9,extent=extent,alpha=1)\n",
    "for (j,i),label in np.ndenumerate(arr4):\n",
    "    ax4.text(i+0.5,j+0.5,f'{label:.1f}',ha='center',va='center',fontsize=6)\n",
    "ax4.set_xticks(np.arange(0, arr4.shape[0], 1), minor=True)\n",
    "ax4.set_yticks(np.arange(0, arr4.shape[1], 1), minor=True)\n",
    "ax4.set_title('sum of both masks')\n",
    "\n",
    "for ax in (ax1,ax2,ax3,ax4):\n",
    "    ax.grid(which='both')\n",
    "    ax.tick_params(left=False,\n",
    "                bottom=False,\n",
    "                top=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(basedir/'reports'/'mask_matching.pdf',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deprojected radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centre       = SkyCoord(p['R.A.'],p['Dec.'])\n",
    "posang       = (p['posang'])*u.deg\n",
    "eccentricity = np.sin(p['Inclination']*u.deg).value\n",
    "r25   = (p['r25']/u.arcmin*300).value  # convert arcmin to pixel\n",
    "\n",
    "a     = r25\n",
    "b     = np.sqrt((r25)**2 * (1-eccentricity**2))\n",
    "\n",
    "tmp = nebulae['x','y','SkyCoord','deproj_dist','r_R25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deproject(coord,centre,a,b,theta):\n",
    "    '''\n",
    "    \n",
    "    https://math.stackexchange.com/questions/432902/how-to-get-the-radius-of-an-ellipse-at-a-specific-angle-by-knowing-its-semi-majo\n",
    "    '''\n",
    "    dist  = coord.separation(centre).to(u.arcsec)\n",
    "    theta = coord.position_angle(centre)-theta\n",
    "    \n",
    "    \n",
    "    r = a*b/(np.sqrt(a**2*np.sin(theta)**2+b**2*np.cos(theta)**2))\n",
    "\n",
    "    return dist/a*r\n",
    "    \n",
    "tmp['dist'] = deproject(tmp['SkyCoord'],centre,a,b,posang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import SkyEllipticalAperture\n",
    "from astropy.units import Quantity\n",
    "\n",
    "a     = 0.2*p['r25']*u.arcmin\n",
    "b     = np.sqrt((a)**2 * (1-eccentricity**2))\n",
    "theta = Quantity(p['posang'],unit=u.deg)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax = fig.add_subplot(projection=Halpha.wcs)\n",
    "\n",
    "norm = simple_norm(Halpha.data,clip=False,percent=99)\n",
    "ax.imshow(Halpha.data,norm=norm,cmap=plt.cm.Reds)\n",
    "\n",
    "sky_aperture = SkyEllipticalAperture(centre,a,b,theta)\n",
    "pix_aperture = sky_aperture.to_pixel(Halpha.wcs)\n",
    "pix_aperture.plot(axes=ax)\n",
    "ax.scatter(*centre.to_pixel(Halpha.wcs),color='green')\n",
    "\n",
    "row = tmp[4]\n",
    "ax.scatter(row['x'],row['y'],color='blue')\n",
    "print(f\"posang = {row['SkyCoord'].position_angle(centre).to(u.degree):.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a     = p['r25']*u.arcmin\n",
    "b     = np.sqrt((a)**2 * (1-eccentricity**2))\n",
    "theta = Quantity(p['posang'],unit=u.deg)\n",
    "\n",
    "theta_diff = (tmp['SkyCoord'].position_angle(centre).to(u.degree)-180*u.deg)-theta\n",
    "r = a*b/(np.sqrt(a**2*np.sin(theta_diff)**2+b**2*np.cos(theta_diff)**2))\n",
    "\n",
    "\n",
    "(tmp['SkyCoord'].separation(centre).to(u.arcsec)*r/a).to(u.arcsec).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['dist'] = tmp['SkyCoord'].separation(centre).to(u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nebulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old and young populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "young = np.isin(associations_mask.data,associations[associations['age']<10]['assoc_ID'])\n",
    "old   = np.isin(associations_mask.data,associations[associations['age']>10]['assoc_ID'])\n",
    "\n",
    "young = young.astype(float)\n",
    "young[young==0] = np.nan\n",
    "\n",
    "old = old.astype(float)\n",
    "old[old==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(projection=F275.wcs)\n",
    "\n",
    "norm = simple_norm(F275.data,clip=False,percent=99)\n",
    "ax.imshow(F275.data,norm=norm,origin='lower',cmap=plt.cm.Greys,alpha=0.5)\n",
    "ax.imshow(old,vmin=0,vmax=1,cmap=plt.cm.Reds,alpha=0.8)\n",
    "ax.imshow(young,vmin=0,vmax=1,cmap=plt.cm.Blues,alpha=0.8)\n",
    "\n",
    "for row in associations:\n",
    "    ax.text(row['X'],row['Y'],row['assoc_ID'])\n",
    "\n",
    "ax.set(xlim=[2000,5000],ylim=[3000,5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "450.85px",
    "left": "257px",
    "top": "110.083px",
    "width": "310.9px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
